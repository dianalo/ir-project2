<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-615-416  </DOCNO><DOCID>08 615 416.andM;</DOCID><JOURNAL>Datamation  July 1 1990 v36 n13 p53(4)* Full Text COPYRIGHT Cahners Publishing Co. 1990.andM;</JOURNAL><TITLE>High performance without compromise. (the use of normalization inthe design of sound database structures)</TITLE><AUTHOR>Edwards, Joe B.andM;</AUTHOR><SUMMARY>Database normalization refers to the step-by-step refinement ofdatabase structures in an attempt to simplify complex structures,typically by generating smaller rows and more tables in arelational system.andP;  This minimizes data redundancy and creates amore application-independent database design, eliminating inquiry,update, delete and insert anomalies.andP;  The process generates morefiles or tables, however, and performance is slowed furtherbecause structures are not designed for specific applications.andO;Many analysts say the emphasis on performance in the real worldcalls for denormalization through combination of files and tables;they claim normalization should be left as an academic exercise.andO;Most see merit in normalization, although users must consider whenit is best to denormalize and the methods available to avoiddenormalization when it is best to do so.andP;  The first three levelsof normalization theory are discussed: first normal form, secondnormal form and third normal form.andM;</SUMMARY><DESCRIPT>Topic:     NormalizationData StructuresData Base DesignNew TechniqueRelational DBMSRelational data bases.andO;Feature:   illustrationtable.andO;Caption:   The first steps in normalizing database structures. (table)After second normal form. (table)After third normal form. (table)andM;</DESCRIPT><TEXT>High Performance Without CompromiseandM;Much has been written about various approaches to the design of sounddatabase structures.andP;  One of the most frequently discussed topics, and insome ways one of the most controversial, is the concept of databasenormalization.andP;  Normalizaton, in simple terms, is the step-by-step refinementof database structures in order to transform complex structures into simplerones.andP;  Although normalization may be explained in simple terms, the argumentsfor and against its use get considerably more complex.andM;First, take a look at the basics.andP;  Normalization typically results in smallerrows and more tables in an relational system.andP;  These normalized databasestructures minimize data redundancy and provide a moreapplication-independent database design.andP;  Such a structure also encouragesthe sharring of data structures between applications.andP;  Normalizationeliminates inquiry, update, delete and insert anomalies.andM;So why would anyone argue against it?andP;  Because, as good as it sounds, thereis a penalty to pay.andP;  Since normalization results in more files ortables--and because the structures are not tailored for specificapplications--performance of individual applications may be slower than ifthe data structures were not normalized.andP;  As a result, the industry has yetto fully embrace the concept.andM;Some say that, in order to provide acceptable performance for a specificapplication, one must reverse the normalization process--that is,&quot;denormalize&quot; by combining files and tables.andP;  Denormalizaton, they continue,makes perfect sense in the real world where performance is at a premium.andO;These skeptics see normalizaton as an academic exercise best left tostudents.andM;Yet experienced analysts agree that normalization is valuable and should notbe skipped or reversed to meet the needs of a particular application.andP;  Thequestions are, then: When is it appropriate to denormalize? and What methodsexist to avoid denormalization when there may be practical reasons to do so?andM;Normalization theory is the result of research by E. F. Codd, the originatorof the relational model.andP;  Based on mathematical set theory, normalizationdefines a sequence of rules to which normalized data structures must adhere.andO;As a result of this origin, normalization is often associated with relationaldatabases, even though it applies equally well to sequential and indexedfiles that adhere to the discipline imposed on relational tables--forexample, the rule that only one record type can be allowed per file.andM;Normalization OriginationsandM;Although the academic community has taken the theory much further, only thefirst three levels of normalization will be examined here:andM;* First Normal Form: all colums in the table are nonrepeating.andM;* Second Normal Form: every nonkey column is identified by the entire primarykey.andM;* Third Normal Form: every nonkey column is dependent on only the primarykey.andM;The first three figures accompanying this article illustrate the process ofdatabase normalization using a university database example.andP;  The universitydatabase starts out as one large and complex table (as illustrated in the&quot;Before Normalization&quot; figure).andP;  Perhaps this table is well suited to theapplications of registering students for a particular course and looking upwhich students are registered for a given course.andP;  But, if someone wants tolook up a course schedule for a given student, the programming would not betrivial since student data repeat within the row.andP;  Furthermore, the recordstructure woul have to allow for a variable length record or limit the numbrof students allowed to register for a course.andM;By taking the database to first normal form--in other words, by eliminatingthe repeating groups--one can create a simpler data structure that allows forboth retrievals of students taking a particular course and retrievals ofcourses taken by a particular student.andM;Even in first normal form, the structure contains redundancy and hasinsertion/deletion anomalies.andP;  For instance, there's no way to record whichprofessor teaches a given course until at least one student registers forthat course.andP;  This is because the structure is not in second normal form,where the professor column value depends only on the course column value.andO;Transforming this first normal form structure into the second normal formstructure eliminates the problem and reduces some redundancy.andP;  For example,the professor's name and office location are no longer repeated for everystudent that is registered for courses taught by that professor (see &quot;AfterSecond Normal Form&quot;).andM;In second normal form, some redundancy still exists in that theprofessor-office column is repeated for every course the professor teaches.andO;Not only that, a professor must teach a course in order for the professor'soffice location to be recoreded at all.andP;  The prof-office column in theTeaching-Assignments table depends on the professor name, which is not theprimary key.andP;  Taking the university database structures to third normal formeliminates the remaining anomalies and rduces unnecesary redundancy bysplitting the PRofessor-Assignments table into two tables (see &quot;After ThirdNormal Form&quot;).andM;These normalized structures are simple, contain minimal redundancy andsupport a variety of access strategies.andP;  However, for the originalapplication, requiring a list of students registered for a given course,physical database I/O will almost certainly increase.andP;  The reason: requireddata are now stored in four tables rather than just one.andM;Inherent in normalization is a structured set of analytical questions thatmust be asked during a development project.andP;  These questions result when thedatabase administrator (DBA) validates the database design using the threenormalization rules described earlier.andP;  For example, without the benefit ofnormalization rules, the DBA might not think to ask whether more than oneprofessor can teach a particular course or whether all professors arerequired to teach at least one course.andM;Why Denormalize?andM;Even though normalization prompts the DBA to ask the right questions duringanalysis and provides a logical data structure that reflects business rules,not all final implementations should be completely normalized.andP;  That is,logical data structures may be normalized during early phases of a projectand denormalized later for performance reasons.andM;The most often-cited reason for denormalization is to provide acceptableperformance for specific applications.andP;  In doing this, the DBA is essentiallyconcluding that performance and application efficiency are more important tothe organization than flexibility, intergrity and accessibility of data.andO;Denormalization, in essence, provides improved performance for one type ofdatabase access at the expense of others, both known and unknown.andM;In those cases where access to data is conducted primarily on a read-onlybasis and is failry predictable, static and voluminous, denormalizationshould be seriously considered--but only after the logical structure has beendeveloped to third normal form and the trade-offs have been evaluated.andM;The effect of denormalization on the performance and hardware requirements ofan application can be dramatic, as illustrated in a 1988 study conducted bymainframe manufacturer Amdahl Corp. of Sunnyvale, Calif.andP;  In a benchmark,Amdahl found that use of normalized data structures resulted in an averageresponse time of 1.5 seconds, consumed 16% of the CPU and processed 186transactions per minute.andP;  Once the data structure was denormalized, this sameapplication resulted in an average response time of 0.4 seconds, occupiedonly 11% of the CPU and processed 273 transactions per minute.andP;  Thisbenchmark was performed using an inquiry-intensive work load with a staticset of access paths.andM;Even if an application's access paths do not result in excessive amounts ofdatabase I/O, as in the Amdahl study, denormalization can still provide amore simplified access path for programmers and end users.andP;  For example, itmay not be desirable to require the end user to access two database tables inorder to display the course-no., professor and prof-office columns togetheron screen, even though the third normal form dictates the use of two separatestructures.andM;Denormalization doesn't always lead to the promised land of greatperformance.andP;  For tables that are frequently updated, for example,denormalization can result in extremely poor performance because, by its verynature, denormalization results in data redundancy.andP;  Updates to redundantdata lead to unnecessary I/O operations, or worse, inconsistent data.andM;Databases that are designed for unique applications and that will not beshared by other current or planned applications also present an obviousopportunity for denormalization.andP;  However, if databases are so tailored toparticular applications that no other applications can effectively use theirdata, then application segregation can become a self-fulfilling prophecy.andO;This, unfortunately, is a situation that many data-processing installationsare in today.andP;  Their many application databases are filled with redundantdata in varying formats that rarely agree or are properly synchronized.andM;Avoiding DenormalizationandM;Therefore, although denormalization promises improved performance for manyapplications, the trade-offs are such that in some instances it should beavoided or at least deferred.andP;  The creative designer has available at leastfour approaches to avoid denormalization.andM;1.andP;  Physical Placement of Data.andP;  Some database management systems providefacilities to mix logically separate tables into one physical file.andP;  In thecase of IBM's DB2, multiple logical tables can be stored in one physicaltablespace.andP;  Because DB2 manages all access to the database, the user is notconcerned with accidentally retrieving rows from other tables in a combinedtablespace.andP;  If such a database is carefully managed, the DBA can influencewhere DB2 physically stores the rows of the various tables within thetablespace.andP;  Consequently, the DBA can improve performance withoutinterfering with the user's logical access to the data.andM;Consider an on-line application that frequently requires a professor's officelocation to be displayed with the professor's name and course number (see &quot;AUniversity Screen&quot;).andP;  If the database structure is in third normal form, theapplication must retrieve data from at least two tables: one that determineswhich professor teaches a given course and one that identifies a professor'soffice location.andM;Ordinarily, accessing two DB2 tables will result in at least one or twoadditional physical I/O operations.andP;  However, DB2 allows the DBA to store twoor more tables in the same tablespace and to order rows within the tablespaceaccording to a clustering index.andP;  If the clustering index is defined on theprofessor name in both tables, it's likely that most rows concerning aparticular professor from both logical tables will be stored on one physicalDB2 page.andP;  Once that page is in the DB2 buffer pool, DB2 avoids theadditional I/O normally required when two tables are stored in separate,simple tablespaces.andM;A clustering feature also imporoves performance when accessing multipletables in the case of the Oracle DBMS from Belmont, Calif.-based Oracle Corp.andO;Oracle's clustering feature is similar to DB2 clustering in that it istransparent to the user.andP;  However, Oracle tries to store related rows fromdifferent tables on the same physical page and also physically stores thecommon column values only once.andM;In the university example, the DBA could tell Oracle that the prof-namecolumn in the two tables establishes a relationship between the two tables.andO;This achieves two objectives.andP;  First, Oracle will try to physically storerelated rows from the two tables on the same physical page.andP;  Second, Oraclewill recognize that related rows from these two tables have the same valuesfor the professor name, and it will, therefore, store the value of theprofessor name only once physically.andM;There are two obvious limitations to mixing related tables in the sametablespace, at least in the DB2 environment.andP;  First, the DBA does not havecontrol over the physical placement of the rows.andP;  However, physical placementcan be influenced significantly by careful design and selection of theclustering indexes and by changing the load and reorganization processes.andO;Second, tablespaces with more than one table have drawbacks.andP;  Tablespacespans, for example, result in unnecessary I/Os when data from only one of thelogical tables are needed.andP;  However, this approach offers the advantage ofminimizing I/O associated with on-line access and avoids the problems ofdenormalized data structures.andM;2.andP;  Logical View Facilities.andP;  It's sometimes difficult during design todetrmine precisely which tables will require denormalization.andP;  Worse, onceapplication programmers and users develop their Structured Query Language(SQLe statements around a particular data structure, denormalization orfurther normalization becomes impractical.andP;  There are cases where the DBA hassome idea as to which databases are potential performance bottlenecks earlyin a project.andP;  Still, most DBAs try to avoid being locked into certainphysical designs before actual production use begins.andM;To combat this problem, another method for bypassing denormalization can beused, employing the logical view facility provided by most relational DBMSproducts.andP;  In any relational database, access to data is done by specifyingthe correct table in which those data are stored.andP;  Therefore, if a DBA movesa column of data from one table to the next, there may be serious programmingramifications.andP;  However, if the DBA develops logical views of tables toproduce denormalized virtual tables--in other words, views--then the impactof later denormalization can be minimized.andM;This logical view alternative can be illustrated using the example shown in&quot;A University Screen.&quot;andP;  In this situation, the DBA is not sure how often thisscreen and others like it will be used, but thinks it will be usedfrequently.andP;  So, initially, the DBA may be tempted to denormalize thedatabase structure to second normal form to avoid performance problems withthese screens.andP;  Just suppose, however, that the DBA is told that professorschange office locations frequently and, when they do change locations, userswant the database to reflect the changes immediately.andP;  So the DBA may alsowant to maintain the office location in the database physically only once toavoid lengthy on-line update transactions.andM;In such a case, the DBA can develop a logical view (see &quot;Creating the LogicalView&quot;) of the two tables in a second normal form, which screens such as theone in &quot;A University Screen&quot; use, but maintain two separate physical tablesfor other transactions.andP;  If the frequent use of these screens employing thislogical view does not cause a performance problem, then the logical views canremain, and they provide the added benefit of simplifying the database accessstatements.andM;If the DBA determines that the database structure is causing performanceproblems, he or she can materialize the logical view into a physical table.andO;By dropping the professor-assignments logical view and creating a newdenormalized physical table (called Professor-Assignments), applicationsusing the view can continue to execute without any procedural changes.andM;Now the DBA must determine how to maintain and synchronize the duplicatedata.andP;  The existing update programs access only the Professor-Location table.andO;These update programs must now be changed so that they also update theProfessor-Assignments table that now includes office locations forprofessors.andP;  Under this approach, the DBA chooses to sacrifice efficienciesin disk space in order to gain efficiencies in applications performance.andM;3.andP;  CASE Tools to Make Denormalization Transparent.andP;  The increasedavailability of sophisticated computer-aided software engineering (CASE)tools offers a new and intriguing alternative to denormalization.andP;  TexasInstruments Inc.'s Information Engineering Facility (IEF) tool, for example,allows denormalization of columns during the system generation phase withoutrequiring any procedural logic changes--in other words, the logical databasestructure remains unchanged from the programmers' point of view.andP;  IEFautomatically generates the COBOL code to update the denormalized copies ofdata every time the application updates what it thinks is a single copy ofthe data.andM;In the university example, one might decide to combine (denormalize) theprofessor office data from the Professor-Location table into theTeaching-Assignments table to provide adequate performance for screens likethe one in &quot;A University Screen.&quot;andP;  IEF allows the DBA to move a copy of theprof-office column to the Teaching-Assignments table.andP;  IEF automaticallygenerates COBOL code to update the prof-office column in both tables.andO;Presumably, the DBA would decide to denormalize in such a fashion once it'sdiscovered that updates to the prof-office column are infrequent, but readsto it in combination with the data in the Teaching-Assignments table are veryfrequent.andM;The only disadvantage of using CASE tools to denormalize is that it resultsin some data redundancy, albeit a controlled redundancy.andP;  It does, however,allow the DBA to store information about where a professor's office is, evenif the professor is not currently teaching a course.andM;4.andP;  Selective Hardware Upgrades.andP;  The last option is the one hardware vendorslove the most and management likes the least: throw hardware at theperformance problem.andP;  Expanded storage, solid state devices, cache storageand additional main memory can all contribute to improved databaseperformance in a cost-effective way.andM;If one weighs the cost of data redundancy in terms of additional disk spacerequired and additional CPU cycles needed to update redundant values,discriminating use of hardware does make sense.andP;  Hardware is a particularlyattractive solution for managing small tables that are accessed heavily byseveral applications.andP;  If a table is small enough, buffer pools can beallocated to ensure that the data will always be in memory.andP;  For tables thatare large but still present system hot spots, cache storage devices are aviable alternative.andM;Continuing the university example, the DBA might decide to locate theProfessor-Location table on a disk drive attached to a cache storage device.andO;By doing so, table read accesses can be reduced from, say, current rates of25 or 35 milli-seconds to 1.5 or 2msec.andP;  This may sound like a smallreduction.andP;  But in a high-volume situation where read requests can wait inqueue for disk service, this reduction in service times can substantiallyimprove the performance of applications needing access to data.andM;Until that distant day when hardware is nearly free and DBAs become used tothe concept of third normal form tables, denormalization will have merit.andO;Still, for today's business problems, there are numerous ways to bypassdenormalization in favor of a sound applications strategy--and withoutsacrificing acceptable performance levels.andP;  So while there are legitimatereasons to consider denormalization, there are also proven techniques toavoid it, or at least lessen its negative impact.andM;Based in Dallas, Joe B. Edwards is a senior manager in the InformationTechnology practice of Deloitte andamp; Touche, a national consulting andaccounting firm.andO;</TEXT></DOC>