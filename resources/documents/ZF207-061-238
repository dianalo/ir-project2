<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-061-238  </DOCNO><DOCID>07 061 238.andM;</DOCID><JOURNAL>Communications of the ACM  Feb 1989 v32 n2 p246(10)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Caching and other disk access avoidance techniques on personalcomputers. (technical)</TITLE><AUTHOR>Jalics, Paul J.; McIntyre, David R.andM;</AUTHOR><SUMMARY>As CPU speed increases on PCs, so does the gap between disk accessand CPU speed.andP;  Three common disk caching systems are examined inan effort to determine the best technique for cutting thedifference in speeds: hardware cache, software cache and RAM disk.andO;DOS buffers are also examined as a primitive disk cache system.andO;Various PC applications are tested with each system.andP;  Hardwarecache systems can be used in all circumstances with substantialperformance improvements.andP;  Software caches can actually degradeperformance of sequential writes but provide major advantages whenreading files that file entirely into the cache.andP;  The use ofadditional DOS buffers do not increase performance dramatically.andO;RAM disk methods are the fastest possible way of avoiding diskaccesses under any circumstance.andM;</SUMMARY><DESCRIPT>Topic:     MicrocomputersCache MemoryInformation Storage and RetrievalDisk Access SchedulingMS-DOS.andO;Feature:   illustrationtablegraph.andO;Caption:   Execution times in seconds for some applications programs. (table)Record blocking on sequential write--elapsed time to write versusblocking. (graph)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>CACHING AND OTHER DISK ACCESS AVOIDANCE TECHNIQUES ON PERSONAL COMPUTERS Diskcaching is a technique for keeping the most recently used disk blocks in anassociative memory in main storage, with the expectation that those blockswill be used again in the near future.andP;  This technique, although somewhatenew to personal computers, is more common on mainframes.andP;  A number of studieshave reported on its performance implications for mainframes.andP;  Unlikemainframe computers which do multiprogramming and overlap CPU processing withI/O, most current PCs that run DOS are single user systems with no concurrentI/O.andP;  As memory costs go down and several megabytes of extended memory is notuncommon, some of this memory is actually underutilized, hence the potentialfor using some of this memory for disk caching.andP;  Also, IBM has included asoftware cache in its PS/2 system which has brought additional attention tothis area.andP;  Finally, there are a number of speed-up hardware cards for the PCAT generation of computers, and a number of these include disk caches.andM;DISK ACCESS AVOIDANCE TECHNIQUES ANDandM;THE ENVIRONMENT OF THE EXPERIMENTandM;Hardware CacheandM;Part of our interest in disk caching started when Distributed ProcessTechnology lent us a hardware caching hard disk controller.andP;  This cardreplaces the AT hard disk controller, is based on a M68000 processor whichdoes the I/O and caching, has 512 kilobytes of on-board cache, and can beconnected to another card with 3 megabytes of cache memory.andP;  One veryimportant feature of this caching mechanism is that it assumes sequentialprocessing and tries to prefetch blocks into the cache before they arerequested by the user program.andM;Software CacheandM;Initially only the IBMCACHE program provided with the PS/2 Model 80 wasconsidered for a software cache.andP;  Initial experiments, however, indicatedthat disk writes were bypassing the cache altogether, and only reads wouldcause cache loading, with subsequent reads benefiting from cache access.andO;this might bypass half the potential of caching such as temporary file usagewhere a file is written and read back only once, so we added another diskcaching software package, which cached both disk reads and writes.andP;  Thispackage named Flash (release 5.5, [9]) by Software Masters had been singledout as the best in a review by Rosch.andP;  Later experiments indicated that about80 percent of disk I/O involves reading from the disk, sot he impact ofnon-cached writes may not be as great as we had assumed.andM;RAM DiskandM;DOS 3.3 provides a device driver named VDISK that allows some of the memoryto be used as a simulated disk drive.andP;  While this kind of RAM disk hasseveral operational disadvantages, it is a useful comparison tool since VDISKimplements a zero-wait disk with no seek time, no latency time, and minimaltransfer time.andP;  Since accessing the RAM disk involves much of the samesoftware as a hard disk except for the actual I/O initiation and wait, it canbe used to estimate the CPU over-head involved in a hard disk access.andM;DOS BuffersandM;The DOS operating system has a system parameter called BUFFERS that can beset from 1 to 99 to specify the number of 512 byte memory buffers to beallocated to DOS.andP;  The system uses this group of buffers in circular pool asscratch pad working areas during I/O.andP;  Before DOS reads a record, it checkswhether that block is already in one of the buffers.andP;  Thus, DOS buffers serveas a primitive disk cache.andP;  The number of DOS buffers was kept constant at 20for all but one of the experiments.andP;  That one had to do with the impact ofchanging the number of buffers.andM;Logical Record BlockingandM;COBOL and most programming languages support logical record blocking indedicated internal buffers.andP;  Most mainframe systems would have two suchblocks for each file so that while the one was being used, the other could beread or written to the disk.andP;  These buffers can serve to do part of what diskcaches do, namely, prefetch some logical records into memory before they arerequested.andP;  Interestingly, on a PC it makes little sense to have more thanone set of buffers for a file because there is no concurrent CPU and I/Opossible under DOS.andP;  This aspect of disk access avoidance is mentionedbecause Smith found that file blocking loses some of its performanceadvantages when caching is in effect.andM;EXPERIMENTAL SETUPandM;The experiments were run on a standard 6 megahertz IBM PC AT under DOS 3.3with the number of I/O buffers set to 20, using the standard 20 megabyte harddisk.andP;  Interestingly, the experiments were run on the AT because the hardwarecache was designed specifically to extend the life of ATs with additionalperformance and would not be available on PS/2s for some time.andM;Several of the test programs for sequential, ISAM, and relative files werewritten in COBOL and compiled using the Realia 3.0 COBOL Compiler.andP;  thehardware cache is a Distributed Processing Technology PX3011A Caching HardDisk Controller with 512 kilobytes of on-board cache.andP;  The software cachesused were the IBMCACHE program released on the reference diskette of IBM PS/2Model 80 computers, and Flash 5.5 from Software Masters.andM;The Effect of OptionsandM;Both the hardware and software cache programs tested had options to tunetheir performance for specific situations.andP;  We, however, used the programswith their default settings.andP;  While a specialist may be willing and able tofine tune the parameters to optimize the performance for a given application,the average PC user does not have the required skills nor does he or she wantto be bother with such technical questions.andM;Cache Size versue File SizeandM;Initial experiments were run with a hardware cache of 3.5 megabytes, and asoftware cache of up to 4 megabytes.andP;  It was decided to reduce the cache sizeto 512 kilobytes for two reasons: few people would be likely to have such alarge cache, and one major interest was in the cache performance when thedata files were considerably larger than the cache.andP;  With a maximum of 20megabytes of hard disk storage, it was not practical to deal with files ofmore than 14 megabytes (less than four times the cache size).andP;  Furthermore,some of the experiments run with these large file sizes took more than twodays of elapsed time.andP;  The experiments were repeated with data file sizesthat ranged as follows: File_Size File_Size/Cache_Size 256 Kilobytes 0.5 409Kilobytes 0.8 512 Kilobytes 1.0 1024 Kilobytes 2.0 2048 Kilobytes 4.0 3072Kilobytes 6.0andM;Thus, the experiments were parameterized on file sized relative to cache sizewith factors of 0.5 through 6.0.andP;  The sample point at 0.5 and 0.8 areadequate for all files that can fit entirely into the cache, while the otherpoints show cache performance as the file size exceeds cache size.andP;  Theresults can be made relevant to other cache sizes by using the relative filesize factor shown above.andM;It is probably the case that a majority of files processed on PCs arerelatively small (i.e., less than 50,000 bytes).andP;  Thus, our choice of 256kilobytes as the smallest datafile needs some clarification.andP;  All the testswere carried out one at a time since multiprogramming is not possible underthe DOS operating system.andP;  Thus, in a typical test there is only one file inuse; the entire file fits into the cache, and processing time is directlyproportional to file size.andP;  Therefore, the results in percent savings aretypically applicable to files that are smaller than 256 kilobytes.andP;  The samecan be said for other tests that involve the use of two files simultaneously.andO;One reason for choosing 25 kilobytes as the smallest file is that operationson it can be measured accurately, which would not be the case for smallerfiles.andP;  For example, measuring a seven percent savings in a spreadsheet orword processor load which takes only 20 seconds total and which has to bemeasured by hand is difficult enough; to do it for a file much smaller wouldyield inaccurate results.andM;Is I/O the Limiting Factor on PCs?andM;Some mainframe comparison experiment were run on an IBM3081K running MVS/SPVerson 1, with OS/VS COBOL 2.4, using IBM3350 disks which do not have diskcaching capability.andP;  The machine was in a single thread or standaloneevinronment with no other system load present.andP;  While one can drawconclusions from the PC experiments about the PCs capacity to do useful work,one cannot do the same with the mainframe experiments since it involves twoprocessors, multiprogramming, and 32 I/O channels.andP;  Nevertheless, comparingthe execution of the same program on the mainframe in a standlone environmentdoes yield some interesting insights into the relative capabilities of PCsand mainframes.andM;MEASUREMENT EXPERIMENTSandM;The results of a number of experiments are presented here.andP;  Some yieldimpressive gains under caching and others do not.andP;  Results are presented forall the areas studied, since our goal is to show the behavior of cachingunder a wide range of activities rather than highlight areas wheresignificant improvements are found.andM;Many of the results are presented as graphs that compare the performance ofthe varius caching techniques.andP;  Typically, the graphs show Percent Savingsvs.andP;  File size for each of the caching techniques.andP;  Other graphs show theactual elapsed time to perform the operations.andP;  Even though the latterresults are quite specific to the hardware used, they provide anotherperspective on caching behavior and allow the interested reader to delve intoadditional detail.andP;  For example, having the actual sequential write times inFigure 2 allows us to compare it directly to the sequential read times inFigure 3, the ISAM read times in Figure 7, the relative read times in Figure8, and the influence of buffers on random reads in Figure 9.andM;How I/O Limited are Personal Computers?andM;It is commonly assumed that PC performance is limited to a great extent bydisk I/O performance.andP;  Is this really true?andP;  Looking at the following resultsfor sequential writes of 80 character records, one can get an idea of howmuch of the time is spent on I/O by comparing the RAM disk to the hard diskwith no cache.andP;  The actual experiment that generated the following dataconsists of writing 13,107 80-byte records with a block-size of 1 kilobyte,and then reading them sequentially.andP;  RAM_Disk Hard_Disk %I/O Time to Write an1.52 2.51 40 80-byte Record Time to Read an 0.89 1.80 51 80-byte RecordandM;Assuming that the RAM Disk CPU overhead is similar to the Hard Disk overheadfor similar operations, one can see that the actual I/O wait time is only 40percent for writes and 51 percent for reads.andP;  This is a far cry from amainframe like the IBM3081K where the I/O wait times amounted to 96.9 percentfor writes, and 96.8 percent for reads.andP;  Thus the ratio of CPU power vs.andP;  I/Opower on a PC is radically different from the mainframe: the mainframe CPU ismuch more powerful in comparison to the I/O channel on 3081.andP;  Alternatively,the PC CPU is much slower when compared to the speed of I/O on a hard disk.andO;Thus the potential gains from disk caching ar not as dramatic on the PC asthey might be on the mainframe.andP;  This is simply so since the maximum gain wecould hope for is 40 percent for writes and 51 percent for reads.andP;  Incontrast, if all the data were in a disk cache on the 3081, one could hope toreduce the runtime by 96.9 percent for writes and 96.9 percent for reads.andM;Figure 1 shows the relative speeds of the AT, an IBM PS/2 model 80, and theIBM3081K for a sequential write, and also shows the proportion of time spentexecuting CPU instructions to accomplish the COBOL write which is done on asequential file with a block size of one kilobyte.andP;  The times were deducedfrom an experiment that wrote 10,000 such records, and the CPU time on thetwo micros is calculated from the write times to a RAM disk.andP;  While theresults give substantial insights into the relative performance of CPU andI/O on all three systems, one should also realize that the relationships maybe quite different for other I/O operations.andP;  Also, there might be otherfactors involved on the mainframe including process scheduling, even on anotherwise idle system.andM;The experiment just shown was also run on the hardware cache where the readtime of a file that is entirely in the cache was found to be identical to theRAM disk read time.andP;  This reinforces the idea that the RAM disk is a goodbound on I/O processing time.andM;Sequential I/O ProcessingandM;A test program was written in COBOL to write a sequential file with 80 byterecords and the read it back sequentially.andP;  Results are shown in Figure 2 forthe write and Figure 3 for the read.andM;The first thing that stood out is that the hardware cache performance wasspectacular.andP;  There was a savings of 20 to 25 percent for writing, regardlessof file size, and a savings of 25 to 40 percent for reading the filesequentially, regardless of file size.andP;  Writing to the hardware cache givesan immediate return after the block is copied into the cache, and will bewritten out later according to the firmware algorithms built into the cachingcontroller.andP;  The most significant reason for the hardware cache performanceis that it is able to do disk I/O at the same time that the CPU is executingthe user program.andP;  The hardware cache, by default, is geared to expectsequential access so it is able to prefetch the next block of a file beforethe user program ever requests it.andM;This is not as revolutionary on mainframes, but the design of DOS on the PCdoes not allow concurrent CPU and I/O operations.andP;  When the user programrequests a disk block, the program is halted and will only continue when thedisk block is available in main storage.andP;  Note that the hardware architectureallows concurrent CPU and I/O operations, but the operating system does notmake use of this feature.andM;One of the main advantages of the hardware cache is that it comes with theequivalent of a separate I/O processor with caching intelligence.andP;  Figures 2and 3 show elapsed times for sequential file creation and reading,respectively.andP;  The Figure 3 times are for a second reading of the file.andP;  Thiswas done to show the behavior of the IBMCACHE when the data could possibly bein the cache.andP;  For file creation, the actual I/O wait percentage is about 30percent as seen from the RAM disk comparison.andP;  The hardware cache writesrequire about 10 percent more than the RAM disk, and about 30 percent lessthan no cache.andP;  These percentages are relatively steady as the file sizeincreases.andP;  Thus, the hardware cache system can eliminate about two thirds ofthe I/O wait time for writes.andP;  For sequential reads, about 50 percent of thetime is spent in I/O wait.andP;  The hardware cache saves about 35 percent, againeliminating about two thirds of the I/O wait time.andP;  This is a very impressiveperformance whose impact is seen in almost all the experiments.andM;Figure 3 also gives some interesting insights into the impact of file sizerelative to cache size.andP;  Note that the RAM disk serves as a fine lower boundon disk read times.andP;  When the size of the file is less than the cache size,each of the three caching mechanisms give excellent results.andP;  However, whenthe file size exceeds the cache size, the two software caches lose allpossible advantages, and the Flash software cache is substantially worse thanno cache at all.andP;  Some of this is to be expected since all caches use a leastrecently used algorithm, and therefore, even at 520 kilobyte, cache blocksare being overwritten that will be needed in the very near future.andM;The reasoning is as follows.andP;  Assume for the moment that there are four cacheblocks and the file is 5 blocks long.andP;  After the file is written, the lastfour blocks are in the cache, but the first is not.andP;  Then we begin to readthe file from the beginning, which means we will overwrite block 2 of thefile which is the least recently used in order to make room for block 1.andO;Then we want to read block 2, but that is no longer in the cache because wehave just written over it; so we overwrite block 3 to put it in the cache.andO;Thus, at each step we fail to get an advantage from the cache but spend thetime to load the cache even though we will never reuse the block we load intoit.andM;File Blocking for Sequential FilesandM;Earlier measurements indicated that record blocking plays the single mostsignificant role in the performance of sequential files.andP;  We wanted to studythe impact of disk caching on file blocking.andP;  Thus, we executed a testprogram that wrote a file of 400,000 bytes, all of which can fit in the cacheand read it back sequentially.andP;  The program was repeated with variousblocking factors, and the whole set of experiments was repeated for each ofour five caching situations.andM;The results for writing the file are shown in Figure 4 and show that fileblocking is very much important even when doing disk caching.andP;  The relativeperformance does indicate, however, that the smallest improvement is for thehardware cache and the Flash software cache.andP;  Since the IBMCACHE cannot beconsidered for caching writes, it does mean that file blocking is lessimportant under caching when all of the file fits into the cache.andP;  Yet, inall cases the time required to write goes down dramatically when the file isblocked.andP;  For example, at a blocking factor of 120 records per block there isabout a 25 percent savings for the two caches and a 40 to 50 percent savingsfor the other three situations when compared to a blocking factor of 1.andO;These results are at odds with Smith's study of caching on a mainframe wherehe indicated that large blocking factors greatly reduce caching benefits forsequential files.andM;This experiment is also significant because the percent savings is alsosteeply increasing for the RAM disk which does not do I/O at all.andP;  One of themajor reasons for blocking sequential files is that it reduces the number ofI/O starts thereby reducing the number of seeks and latency waits.andP;  With aRAM disk there are no seeks and no latencies, but apparently the softwareoverhead in calling the int 13 I/O service that gives access to the RAM diskis substantially higher than the record blocking software in the COBOLruntime environment.andM;Figure 5 shows the read times for various blocking factors in the variouscaching situations.andP;  These show that blocking is less important in a cachingsituation than for a non-caching one, as can be seen by the lower savings forall the caching situations.andP;  Blocking is still of some interest to sequentialread performance, although three of the five saturate past a blocking factorof 30.andP;  There is one big surprise, however.andP;  The IBMCACHE program follows thepattern for small blocking factors, but at a blocking factor of 60 itsreading performance is 40 percent worse than no blocking and stays worse forlarger blocking factors.andP;  The reasons for this are not readily apparent.andP;  Theonly thing we could imagine is that as the block grows to over the 4,096 bytecluster size, the code that manages the fetching of data from multiplecluster blocks in the cache yields these performance degradations.andM;Sorting ExperimentsandM;A test program was written to generate random input records of 80 bytes witha 10 byte key.andP;  The results of sorting such a file with varying numberrecords are shown in Figure 6.andM;First looking at the results for files that fit entirely into the cache (256kilobytes and 400 kilobytes), we see that RAM disk and hardware cache provideimprovements in the 30 to 40 percent range.andP;  The two software cachingprograms provide a 10 to 20 percent improvement for files under the cachesize, with a steady decrease in savings as the file size grows.andM;Looking at larger sort files, we see that I/O time is about 40 percent of thetotal at twice the cache size (as seen by the RAM disk results), and thehardware cache provides a relatively steady 30 percent savings independent offile size, indicating perhaps that data prefetching is more important herethan caching.andP;  The IBMCACHE program gives a steadily decreasing improvementstarting at 10 percent and going down to 3 percent at a file factor of 6.andO;This is somewhate surprising since the sequential file access experimentsseem to indicate that the IBMCACHE as not having any advantage at all unlessthe file was read more than once.andM;We envision an external sort as writing a number of temporary subfiles, eachof which is read back exactly once during the merge phase.andP;  In such asetting, one would expect a caching program that ignores writes to be adetriment rathern than an advantage.andP;  The Flash software cache, however, doesa quick nosedive at cache size to settle down to a performance degradation ofabout 8 percent.andP;  This is a surprise since one would expect that the sortwould write out temporary subfiles and then read them back, thus benefitingfrom the blocks that were still in the cache.andP;  Thus, all one observes is theadditional overhead of writing blocks to the cache and reading blocks intothe cache without any benefits at all.andM;ISAM ExperimentsandM;Clearly one interesting test of the effectiveness of the various cachingtechniques is the random access to a file.andP;  A program was written to write afile of 80-byte records sequentially where each record contains a 10-bytekey.andP;  The file was read back sequentially and then randomly by having arandom number generator generate key values.andP;  Random read times are shown inFigure 7.andP;  The random reads are performed as many times as there are recordsin a given file, and the read times shown are calculated by taking the totaltime for the reads divided by the number of reads.andM;There are three distinct areas on the graph.andP;  For files within the cache sizeand a little above, all of the technique perform well and provide savings of55 to 75 percent over no cache.andP;  The second area is from cache size to about2.5 times cache size.andP;  Here all the caching techniques are still very muchbetter than no cache, but the two groups are slowly converging.andP;  The thirdarea is above a factor of 2.5.andP;  Here there is a steady state where thehardware cache provides a steady savings of about 14 percent.andP;  Flash asavings of about 8 percent, and IBMCACHE gives a degradation of about 4percent.andP;  The specific savings percentages for each technique are shown inFigure 8, which shows that there is little I/O for any of the cachingtechniques until the file size grown to a factor of 2.5 which is the largestfile size that we have RAM disk figures for.andP;  Note that at this file sizeonly 9 percent of the time is spent on I/O wait, and the rest is spend on CPUprocessing.andP;  Thus, the potential gains for larger files are limited not onlyby cache size, but also by the CPU overhead of going through the index tofind the appropriate record.andM;Compilation and Linkage EditingandM;A number of COBOL programs were compiled and linkage edited to look forpossible savings under caching.andP;  It seems that compilations on the COBOLcompiler are almost completely CPU-bound with I/O wait time accounting foronly about 10 to 20 percent compilation time.andP;  With the potential savings sosmall, the actual savings are not impressive.andP;  For example, a 2,520 lineCOBOL programs will save 8 percent when using a RAM disk, 7 percent for thehardware cache and IBMCACHE, and will be two percent slower using Flash.andM;Linkage editing the same program produces 66 percent savings for the RAMdisk, 55 percent for the hardware cache, 33 percent savings for Flash, and 45percent for the IBMCACHE.andM;Real ApplicationsandM;A number of real application programs were run to get a better perspective ofcache behavior under various realistic program loads.andP;  The results are shownin Table I.andP;  Asm990 is a cross assembler; xref1 generates cross-referencedlistings; matcher identifies differences between source files; and makearccreates an archive of all the files in the current directory and performsdata compression on them.andP;  While they all tend to be I/O oriented programs,they show a wide range of cache savings.andP;  Note also that there is only onecase where a software cache degrades performance, but several in which thecache is of little practical advantage.andP;  This is in contrast to the hardwarecache which always delivers an improvement, and a substantial one in four outof five cases.andM;Spreadsheet ApplicationandM;A spreadsheet of about 246 kilobytes was created, read in, read in again,modified, and written out two times.andP;  Each of these operations was timed tomeasure differences in the varying caching situations.andP;  The results are shownin Table II.andP;  The results showed that only 15 percent of the read time isinvolved with waiting for I/O, so that potential gains are small, indeed.andO;For writes, the I/O wait percent is 25, which is still small.andP;  Using the sumof all four operations, the RAM disk saves 23 percent, the hardware cache 16percent, Flash gets worse by 1 percent, and IBMCACHE saves 4 percent.andP;  Sincespreadsheet load and saves are done relatively infrequently, the savings hereare so meager that they are of little practice value.andP;  What this means isthat even for the I/O intensive task of reading and writing spreadsheetfiles, an overwhelming percentage of the time is spent in CPU processing, socaching makes little difference in such an environment.andM;Word ProcessingandM;A document consisting of 272 kilobytes was read into Wordstar 4.0, read inagain, modified, written out, and written out again.andP;  Similar to thespreadsheet discussion above, about 25 percent percent of the load and savetimes.andP;  are spent in I/O wait, and the total savings amount to 25 percent forthe RAM disk, 20 percent for the hardware cache, and about 7 percent for twotwo software caches.andP;  Since these load and save operations are doneinfrequently, the impact of disk caching on word processing applications isprobably negligible.andM;Number of DOS BuffersandM;The number of buffers impacts peformance much more significantly than iscommonly realized.andP;  To demonstrate this effect on random reads, the whole setof ISAM file experiments was repeated with buffers ranging in value from 1 to99.andP;  The results are shown in Figure 9 for a 1.2 megabyte file and a 3.6megabyte file.andP;  The number of buffers for smaller files may not be verysignificant, but for larger files, one must set the number to a minimum valuewhich is a function of file size.andP;  Each file size has a characteristic curveas shown in Figure 9.andP;  To avoid disastrous performance, one must set thenumber of buffers to at least the number at the point where the curveflattens out.andP;  Setting the number to a value larger than that minimum willdecrease performance in a minor way, but setting it too small can multiplyrandom read times by as much as 150 percent.andM;Another interesting effect is that when any kind of caching is used, thenumber of buffers used does not seem to be as critical.andP;  For example, for a3.6 megabyte ISAM file, going from 3 buffers to 80 buffers with no cachingcauses an improvement of 50 percent, but for hard cache that improvement isonly 3.7 percent.andM;Sorting is also affected by the number of buffers in a similar way to randomaccess, but not as dramatically.andP;  Increasing the number of buffers from 3 to80 causes an improvements of 20 percent for no cache, and 4 percent forhardware cache.andP;  Since the result at 20 buffers yields a savings of 25percent for no cache, one can conclude that here too, performance at acertain file size is characterized by a parabolic curve similar to thebehavior noted for ISAM reads.andM;The sequential experiments were also run with 80 buffers and generally showdegraded write performance by about 25 percent, and reduced read performanceby a few percent.andP;  Blocking factor also has a big impact on the number ofbuffers.andP;  With a small blocking factor, having a large number of buffers isquite detrimental to performance, but this effect is reduced very much whenusing large blocking factors.andM;Based upon the above experiments, the number of buffers was set to 20 for allof the caching experiments.andP;  This is an optimal number for ISAM random readsof our maximum file size of 3.6 megabytes, and is large enough for our sorttests; and yet not too large for the other sequential tests where additionalbuffers caused a performance decline.andM;dBase ApplicationsandM;Several of the COBOL tests were also implemented in dBase III Plus to observethe caching behavior in this application environment which is very typicalfor personal computers.andP;  One must take into account that file processing indBase is much slower to start with.andP;  dBase is an interpretive language whoseperformance is anywhere from three to ten times as slow as COBOL.andP;  Theperformance factor appears to be about four for sequential reads and sortsand three for random reads via key.andP;  Many of the conclusions in this sectionare based upon access to a one megabyte file with 80-byte records.andM;For sequential reading of files, dBase appears to be much more CPU bound thanthe corresponding COBOL; 85 percent of the time is spent in CPU prcessingversus 60 percent for COBOL sequential reads.andP;  This means that the potentialbenefit of caching is greatly reduced, simply because with only 15 percent ofthe time spent on I/O we could not save more than 15 percent of the time.andP;  Sowhile the relative effects of caching are unchanged, the actual percentage oftime saved is only one third to one half of the percentage for COBOLsequential file reading.andM;The earlier experiments random reads were implemented in a COBOL program.andO;That program was rewritten in dBase to look at dBases random processingperformance.andP;  Random reading in dBase of a one megabyte file is about 86percent CPU activity and 14 percent I/O wait.andP;  This compares to COBOL's 91percent CPU and 9 percent I/O wait.andP;  Clearly both are very much CPU bound andthe potential for savings is small.andP;  The results of the dBase random readexperiments are shown in Figure 10.andP;  The experiments are directly comparableto the COBOL ISAM one whose results are shown in Figure 8.andP;  There is a lot ofsimilarity in the trends seen in the two graphs.andP;  Both have good improvementpotential under and around the cache size which steadily decreases as thefile size grows further.andP;  While the COBOL random access shows improvements ofmore than 70 percent for small files, the dBase savings are never more thanabout 23 percent, decrease (as file size increases), and also show erraticbehavior near 2.5 megabytes.andM;Earlier we discussed sorting experiments that were also repeated in dBase IIIPlus to yield the results shown in Figure 11.andP;  These results are directlycomparable to those shown in Figure 6 for the COBOL sort.andP;  While the trendsfor small sorts are similar to COBOL, the trends for large sorts areradically different and show a steady 30 to 50 percent improvement regardlessof the size.andM;CONCLUSIONSandM;(1) Disk Caching on personal computers has substantial potential forimproving I/O performance.andP;  Since the files used on PCs are relatively small,most such files would likely fit into the cache size.andP;  For such files,caching provides very substantial benefits in nearly all cases (sequentialwrites for the software caches are one exception).andM;(2) Disk caching can yield very substantial benefits in performance, but canalso degrade performance unless it is used with some understanding of thetypes of processing where it provides benefits.andM;(3) The hardware cache is the only one that can be used in all circumstanceswith substantial performance benefits in sequential as well as randomprocessing.andP;  It can reduce I/O wait times by two thirds in a wide range ofapplications.andP;  This is the cleanest solution since it is part of the diskcontroller solution since it is part of the disk controller hardware and doesnot impact the operating system or other software.andP;  Its major disadvantagesare cost and that its use is more limited in the newer machines where thedisk controller is built into the mother board.andP;  Also, memory built into thecontroller is not available for any purpose other than caching.andP;  This is incontrast to all the other solutions in which that memory can be used forprograms or data some of the time and caching at other times.andP;  One otherproblem in evaluating the hardware cache is the difficulty in separating outthe advantages gained from its ability to a serve as a separate I/O channelwhich can run currently with the CPU.andP;  We believe the concurrent I/Ocapability of the hardware cache is as important as the caching itself.andO;Finally, the cost of such hardware currently is the most expensive of thealternatives tested.andM;(4) The software caches are likely to degrade the performance of sequentialwrites, provide big advantages upon reading for files that fit entirely intothe cache, and are likely to degrade performance for larger files.andO;Furthermore, the IBMCACHE can only provide benefits on the second reading soit is of no use for temporary file processing where the data is written onceand read back only once.andP;  Actually, neither software cache produces a netbenefit for the temporary file case, regardless of file size.andP;  The Flashsoftware could provide such a benefit for small files, but it is geared towrite the data to the disk immediately, or at least within nine seconds ofbeing written to the cache, so its overall performance for write + read isabout the same as no cache.andM;(5) Caching of disk writes is not as important was we initially thought fortwo reasons.andP;  The first is that writes make up only 20 percent of disk I/O,and second that these writes involve extra overhead that degradesperformance.andP;  The end-result is that the IBMCACHE, which does not cachewrites, performs almost as well in situations favorable to caching, but isless likely to degrade performance in other situations.andM;(6) The effects of aching on file sorting are substantial.andP;  The hardwarecache and RAM disk can save 30 to 40 percent; software caches perform muchworse with improvements of 10 to 20 percent for small files, and for largerfiles, a small improvement of a few percent in the case of IBMCACHE, to alarger 8 percent degradation for Flash.andM;(7) The use of additional DOS buffers does increase performance dramaticallyfor random reads of larger files and for sorting, makes sequential writes oflarger files substantially slower, and makes sequential reads a little bitslower.andP;  The optimal number of buffers in a function of both the file sizeand the number of files doing I/O in the same program.andP;  The addition of DOSbuffers under any form of catching is of much less benefit.andP;  In fact, in anumber of cases, setting the number of buffers to the optimal value does asmuch as some forms of caching.andP;  Since DOS buffers occupy, at most, 50kilobytes, the size of memory is not a great concern.andP;  The DOS buffers,however, must come from primary memory under 640K so that it will limit theprograms that can be run.andM;(8) The RAM disk is the fastest possible way of avoiding disk accesses underany circumstances.andP;  While this technique is ideal for temporary files.andO;However, and can also serve for read-only parameter files and databases, itscontents are altogether volatile and lost on any system failure.andO;Furthermore, it allocates memory permanently that could be used for otherpurposes.andP;  One partial answer to this is the RAM disk that comes with Flashwhich share the RAM disk memory with the cache dynamically so that memory notused by the RAM disk at any given instant can be used for caching.andP;  RAM disksare also unsatisfactory because they require the user to load files into itand do not work at all if the file or files do not fit into it completely.andO;The other caching techniques work quite well even if only a portion of thefile fits into the cache.andM;(9) Our experiments indicate that the potential gain from caching is lessthan what might be possible on a mainframe because the CPU power of PCs isless when compared to the I/O capabilities of hard disks.andP;  Thus, programswhich are completely I/O bound on the mainframe are much less so on PCs.andO;Furthermore, disk block prefetching for sequential files is an importantcaching function that is not easily available using the current operatingsystem.andM;(10) Extending the observations from the hardware cache, we can expectadditional throughput from Personal Computers as they start to run moresophisticated operating systems like OS/2 or Unix which do allow concurrentCPU and I/O processing; and in the case of UNIX have systemwide disk cachingbuilt into the operating system.andP;  Frthermore, as we have seen from Figure 1,the CPU power of personal computers is increasing relative to I/Operformance.andP;  Thus, the potential gain from disk caching is even more thanearlier because a larger percentage of time is spend on I/O in the newersystems.andO;</TEXT></DOC>