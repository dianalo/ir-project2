<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-864-726  </DOCNO><DOCID>07 864 726.andM;</DOCID><JOURNAL>Communications of the ACM  Nov 1989 v32 n11 p1360(3)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Abstracts from other ACM publications.andM;</TITLE><DESCRIPT/><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Abstracts from Other ACM PublicationsandM;ACM Transactions on Mathematical Software September 1989andM;A Fast Adaptive Grid Scheme for Elliptic Partial DifferentialandM;EquationsandM;We describe the Recursive Subdivision (RS) method--an efficient and effectiveadaptive grid scheme for two-dimensional elliptic partial differentialequations (PDEs).andP;  The RS generates a new grid by recursively subdividing arectangular domain.andP;  We use a heuristic approach which attempts toequidistribute a given density function over the domain.andP;  The resulting gridis used to generate an adaptive grid domain mapping (AGDM), which may beapplied to transform the PDE problem to another coordinate system.andP;  The PDEis then solved in the transformed coordinate system using a uniform grid.andP;  Webelieve parallelism is most easily exploited when computations are carriedout on uniform grids; theAGDM framework allows the power of adaptation to beapplied while still preserving this uniformity.andP;  Our method generates goodadaptive grid domain mappings at a small cost compared to the cost of theentire computation.andP;  We describe the RS algorithm in detail, briefly describethe AGDM framework, and illustrate the effectiveness of our scheme on severalrealistic test problems.andM;For Correspondence Department of Computer Science, Virginia PolytechnicInstitute and State University, Blacksburg VA 24061.andM;A Graph Partitioning Algorithm by Node SeparatorsandM;A heuristic graph partitioning scheme is presented to determine effectivenode separators for undirected graphs.andP;  An initial separator is  firstobtained from the minimum degree ordering, an algorithm designed originallyto produce fill-reducing orderings for sparse matrices.andP;  The separator isthen improved by an iterative strategy based on some known results frombipartite graph matching.andP;  This gives an overall practical scheme inpartitioning graphs.andP;  Experimental results are provided to demonstrate theeffectiveness of this heuristic algorithm on graphs arising from sparsematrix applications.andM;For Correspondence Department of Computer Science, York University, NorthYork, Ontario, Canada M3J 1P3.andM;Constrained Nonlinear Least Squares: An Exact Penalty ApproachandM;with Projected Structured Quasi-Newton UpdatesandM;This paper is concerned with the development, numerical implementation, andtesting of an algorithm for solving constrained nonlinear least squaresproblems.andP;  The algorithm is an adaptation of the least squares case of anexact penalty method for solving nonlinearly constrained optimizationproblems due to Coleman and Conn.andP;  It also uses the ideas of Nocedal andOverton for handling quasi-Newton updates of  projected Hessians, those ofDennis Gay, and Welsch for approaching the structure of nonlinear leastsquares Hessians, and those of Murray and Overton for performing linesearches.andP;  This method has been tested on a selection of problems listed inthe collection of Hock and Schittkowski.andP;  The results indicate that theapproach taken here is a viable  alternative for least squares problems tothe general nonlinear methods studied by Hock and Schittkowski.andM;For Correspondence: N. Mahdavi-Amiri,York University, Computer ScienceDepartment, Downsview, Ontario, Canada M3J 1P3; R. H. Bartels, University ofWaterloo, Department of Computer Science, Waterloo, Ontario, Canada N2L 3G1.andM;ALGORITHM 675andM;FORTRAN Subroutines for Computing the Square Root CovarianceandM;Filter and Square Root Information Filter in Dense or HassenbergandM;FormsandM;In this paper, codes are provided for two of the most popular square rootfilters: the Square Root Covariance Filter and the Square Root InformationFilter.andP;  We also give efficient implementations for the time invariant casebased on so-called condensed forms.andP;  All routines make  extensive use of BLASroutines.andM;For Correspondence: M. Vanbegin and P. Van Dooren, Philips ResearchLaboratory, Avenue Van Becelaere 2, Box 8, B-1170, Brussels,  Belgium; M.andO;Verhaegen, NASA Ames, Moffett Field, CA 94035.andM;Remark on Algorithm 587andM;The subroutine WNNLS of Algorithm 587 has exposed some shortcomings,especially in solving rank-deficient problems.andP;  They may lead to fatal errorsand (or) false results.andP;  Five improvements are proposed.andP;  The effect of thechanges is tested on four rank-deficient test problems.andP;  It is emphasizedthat the WNNLS still remains vulnerable to bad scaling of the problem.andM;For Correspondence: Astronomijos observatorija, 232009 Vilnius, Ciurlionio29, Lithuania, U.S.S.R.andM;Remark on Algorithm 630andM;This paper describes some recent changes to Algorithm 630 for minimization,namely BBVSG [2].andP;  These changes are varied and include improvements tocertain internal parts of the algorithm and modifications intended to makethe code easier to use.andP;  Facilities have been added for better handling of&quot;real-life&quot; problems; for readers with the kind of large problems for whichBBVSCG was designed, the new version is more flexible, and onerecommendations (see Sec.andP;  2) regarding use of the algorithm should be noted.andO;Most users need to be aware of only a few of the changes.andM;For correspondence: Department of Mathematics, Statistics, and ComputingScience, Dalhousie University, Halifax, Nova Scotia, Canada B3H 3J5.andM;REsidual Hermite Normal Form ComputationsandM;This paper extends the class of Hermite normal form solution procedures thatuse modulo determinant arithmetic.andP;  Given any relatively prime factorizationof the determinant value, integral congruence relations are used to computethe Hermite normal form.andP;  A polynomial-time complexity bound that is afunction of the length of the input  string exists for this class ofprocedures.andP;  Computational results for this new approach are given.andM;For Correspondence: National Institute of Standards and Technology, Appliedand Computational Mathematics Division (719), Boulder, CO 80303-3328.andM;ACM Transactions on Information Systems April 1989andM;Object SpecializationandM;Specialization hierarchies typically are treated as type-level constructs andare used to define various inheritance mechanisms.andP;  In this paper we considerspecialization at the level of objects.andP;  We show that doing so creates a moreflexible and powerful notion of inheritance by allowing objects to definetheir own inheritance path.andP;  Object specialization can also be used to modelcertain forms of versioning, implement data abstraction, and provide a&quot;classless&quot; prototype-based language interface to the user.andM;For Correspondence: Department of Computer Science, Boston University, 111Cummington St., Boston, Mass.andP;  02215, sciore@ bu-cs.bu.edu.andM;An Algebra for Structured Office DocumentsandM;We describe a data model for structured office information objects, which wegenerically call &quot;documents,&quot; and a practically useful algebraic language forthe retrieval and manipulation of such objects.andP;  Documents are viewed ashierarchical structures; their layout (presentation) aspect is to be treatedseparately.andP;  The syntax and semantics of the language are defined preciselyin terms of the formal model, an extended relational algebra.andM;The proposed approach has several new features, some of which areparticularly useful for the management of office information.andP;  The data modelis based on nested sequences of tuples rather than nested relations.andO;Therefore, sorting and sequence operations and the explicit handling ofduplicates can be described by the model.andP;  Furthermore, this is the firstmodel based on a many-sorted instead of a one-sorted algebra, which meansthat atomic data values as well as nested structures are objects of thealgebra.andP;  As a consequence, arithmetic operations, aggregate functions, andso forth can be treated inside the model and need not be introduced as querylanguage extensions to the model.andP;  Many-sorted algebra also allows arbitraryalgebra expressions (with Boolean result) to be admitted as selection or joinconditions and the results of arbitrary expressions to be embedded intotuples.andP;  In contrast to other formal models, this algebra can be useddirectly as a rich query language for office documents with precisely definedsemantics.andM;For Correspondence: R. H. Guting, Lehrstuhl Informatik VI, University ofDortmund, 4600 Dortmund 50, West Germany; R. Zicari, Politecnico di Milano,Piazza Leonardo da Vinci, 32 20133 Milano, Italy; D. M. Choy, Dept.andP;  K52/803,IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120-6099.andM;Partitioned Signature Files: Design Issues and PerformanceandM;EvaluationandM;A signature file acts as a filtering mechanism to reduce the amount of textthat needs to be searched for a query.andP;  Unfortunately, the signature fileitself must be exhaustively searched, resulting in degraded performance for alarge size.andP;  We propose to use a deterministic algorithm to divide asignature file into partitions, each of which contains signatures with thesame &quot;key.&quot;andP;  The signature keys in a partition can be extracted andrepresented as the partition's key.andP;  The search can then be confined to thesubset of partitions whose keys match the query key.andP;  Our main concern hereis to study methods for obtaining the keys and their performance in terms oftheir ability to reduce the search space.andM;Owing to the reduction of search space, partitioning a signature file has adirect benefit in a sequential search (single-processor) environment.andP;  In aparallel environment, search can be conducted in parallel effectively byallocating one or more partitions to a processor.andP;  Partitioning the signaturefile with a deterministic method (as opposed to a random partitioning scheme)provides intraquery parallelism as well as interquery parallelism.andM;In this paper, we outline the criteria for evaluating partitioning schemes.andO;Three algorithms are desrcibed and studied.andP;  An analytical study of theperformance of the algorithms is provided and the results are verified withsimulation.andM;For Correspondece: Department of Computer and Information Science, Ohio StateUniversity, 2036 Neil Ave., Columbus, OH 43210-1277.andM;ACM Transactions on Information Systems July 1989andM;The Constituent Object Parser: Syntactic Structure Matching forandM;Information RetrievalandM;The Constituent Object Parser is a shallow syntactic parser designed toproduce dependency tree representations of syntactic structure that can beused to specify the intended meanings of a sentence more  precisely that canthe key terms of the sentence alone.andP;  It is intended to improve theprecision/recall performance of information retrieval and  similar textprocessing applications by providing more powerful matching procedures.andP;  Thedependency tree representations and the relationship between the intended useof this parser and its design is described, and several problems concerningthe processing of ambiguous structures are discussed.andM;For Correspondence: Department of Information Science, University ofPittsburgh, Pittsburgh, PA 15206.andM;Storing Text Retrieval Systems on CD-ROM: Compression andandM;Encryption ConsiderationsandM;The emergence of the CD-ROM as a storage medium for full-text data-basesraises the question of the maximum size database that can be contained bythis medium.andP;  As an example, the problem of storing the Tresor de la LangueFrancaise on a CD-ROM is examined in this paper.andP;  The text alone of thisdatabase is 700 megabytes long, more than a  CD-ROM can hold.andP;  In addition,the dictionary and concordance needed to access these data must be stored.andP;  Afurther constraint is that some of the material is copyrighted, and it isdesirable that such material be difficult to decode except through softwareprovided by the system.andP;  Pertinent approaches to compression of the variousfiles are reviewed, and the compression of the text is related to the problemof data encryption.andP;  Specifically, it is shown that, under simple models oftext generation, Huffman encoding produces a bit-string indistinguishablefrom a representation of coin flips.andM;For Correspondence: Committee on Information Studies, University of Chicago,1100 E. 57th St., Chicago, IL 60637.andP;  Author's e-mail addresses: Klein:tomi@cerberus.uchicago.edu;Bookstein:bkst@cerberus.uchicago.edu;Deerwester:scott@cerberus.uchicago.eduandM;Knowledge-Based Search Tactics for an Intelligent IntermediaryandM;SystemandM;Research on the nature of knowledge-based systems for bibliographicinformation retrieval is summarized.andP;  Knowledge-based search tactics are thenconsidered in terms of their role in the functioning of a semantically basedsearch systems for bibliographic information retrieval, EP-X.andP;  This systemuses such tactics to actively assist users in defining or refining theirtopics of interest.andP;  It does so by applying these tactics to a knowledge basedescribing topics in a particular domain and to a data-base describing thecontents of individual documents in terms of these topics.andP;  This paper, then,focuses on the two central concepts behind EP-X: semantically based searchand knowledge-based search tactics.andM;For Correspondence: P. J. Smith, S. J. Shute, and D. Galdes, Department ofIndustrial and Systems Engineering, Ohio State University, 210 Baker, 1971Neil Ave., Columbus, OH 43210; M. H. Chignell, Department of Industrial andSystems Engineering, University of Southern California, Los Angeles, CA90007.andM;A Critical Investigation of Recall and Precision as Measures ofandM;Retrieval System PerformanceandM;Recall and precision are often used to evaluate the effectiveness ofinformation retrieval systems.andP;  They are easy to define if there is a singlequery and if the retrieval result generated for the query is a  linearordering.andP;  However, when the retrieval results are weakly ordered,  in thesense that several documents have an identical retrieval status value withrespect to a query, some probabilistic notion of precision has to beintroduced.andP;  Relevance probability, expected precision, and so forth, aresome alternatives mentioned in the literature for this purpose.andP;  Furthermore,when many queries are to be evaluated and the retrieval results averaged overthese queries, some method of interpolation of precision values at certainpreselected recall levels is needed.andP;  The currently popular approaches forhandling both a week ordering and interpolation are found to be inconsistent,and the results obtained are not easy to interpret.andP;  Moreover, in cases wheresome alternatives are available, no comparative analysis that wouldfacilitate the selection of a particular  strategy has been provided.andP;  Inthis paper, we systematically investigate the various problems and issuesassociated with the use of recall and precision as measures of retrievalsystem performance.andP;  Our motivation is to provide a comparative analysis ofmethods available for defining precision in a probabilistic sense and topromote a better understanding of the various issues involved in retrievalperformance evaluation.andM;For Correspondence: V. V. Raghavan and G. S. Jung, The Center for AdvancedComputer Studies, University of Southwestern Louisiana, P.O. Box 44330,Lafayette, LA 70504; P. Bollmann, Technische Universitat Berlin, FachbereichInformatik, FR5-11, Franklinstra[Beta]e 28/29, D-1000, Berlin 10, WestGermany.andM;Optimum Polynomial Retrieval Functions Based on the ProbabilityandM;Ranking PrincipleandM;We show that any approach to developing optimum retrieval functions is basedon two kinds of assumptions: first, a certain form of representation fordocuments and requests, and second, additional simplifying assumptions thatpredefine the type of the retrieval function.andP;  Then we describe an approachfor the development of optimum polynomial retrieval functions:request-document pairs ([f.sub.l], [d.sub.m]) are mapped onto descriptionvectors x([f.sub.l], [d.sub.m]), and a polynomial function e(x) is developedsuch that it yields estimates of the probability of relevance P(R +x(f.sub.l], [d.sub.m])) with minimum square errors.andP;  We give experimentalresults for the application of this approach to documents with weightedindexing as well as to documents with complex representations.andP;  In contrastto other probabilistic models.andP;  Our approach yields estimates of the actualprobabilities, it can handle very complex representations of documents andrequests, and it can be easily applied to multivalued relevance scales.andP;  Onthe other hand, this approach is not suited to  log-linear probabilisticmodels and it needs large samples of relevance  feedback data for itsapplication.andM;For Correspondence: Technische Hochschule Darmstadt, Fachbereich Informatik,Karolinen-platz 5, D-1600 Darmstadt, Germany.andM;Information Retrieval Using a Hypertext-Based Help SystemandM;Hypertext offers users a simple, flexible way to navigate through electronicinformation systems but at the potential risk of becoming lost in the networkof interconnected pieces of information.andP;  A study was conducted oninformation retrieval using a commercial hypertext-based help system.andP;  It wasfound that the predominant search strategy was &quot;browsing&quot; (characterized byscanning tables of contents and paging through topics), rather than employingthe indexes (&quot;analytical search&quot;).andP;  Although subjects did not get lost,individuals with better spatial visualization skills, as measured by astandardized test, were faster at  retrieving information and returned to thetop of the information hierarchy less often than those with poorer spatialvisualization skills.andP;  Thse results support previous studies that have founda strong preference by users for browsing in hypertext systems and extendthose findings to a new domain (help), a different type of user interface,and a different information architecture.andP;  In addition, the resultsdemonstrate the importance of spatial visualization ability for efficientnavigation and information retrieval in a hierarchical hypertext system.andM;For Correspondence: Sun Microsystems, Inc., Two Federal St., Billerica, MA01821.andM;ACM Transactions on Programming Languages July 1989andM;Integrating Nonintefering Versions of ProgramsandM;The need to integrate several versions of a program into a common one arisesfrequently, but it is a tedious and time consuming task to integrate programsby hand.andP;  To date, the only available tools for assisting with programintegration are variants of text-based differential file comparators; theseare of limited utility because one has no guarantees about how the programthat is the product of an integration behaves compared to the programs thatwere integrated.andM;This paper concerns the design of a semantics-based tool for automaticallyintegrating program versions.andP;  The main contribution of the paper is analgorithm that takes as input three programs A, B, and Base, where A and Bare two variants of Base.andP;  Whenever the changes made to Base to create A andB do not &quot;interfere&quot; (in a sense defined in the paper), the algorithmproduces a program M that integrates A and B.andP;  The algorithm is predicted onthe assumption that differences in the behavior of the variant programs fromthat of Base, rather than differences in the text, are significant and mustbe preserved in M.andP;  Although it is undecidable whether a program modificationactually leads to such a difference, it is possible to determine a safeapproximation by comparing each of the variants with Base.andP;  To determine thisinformation, the integration algorithm employs a program representation thatis similar (although not identical) to the dependence graphs that have beenused previously  in vectorizing and parallelizing compilers.andP;  The algorithmalso makes use of the notion of a program slice to find just those statementsof a program that determine the values of potentially affected variables.andM;The program-integration problem has not been formalized previously.andP;  Itshould be noted, however, that the integration problem examined here is agreatly simplified one; in particular, we assume that expressions containonly scalar variables and constants, and that the only statements used inprograms are assignment statements, conditional statements, and while-loops.andM;For Correspondence: S. Horwitz and T. Reps, Computer Sciences Department,University of Wisconsin, 1210 W. Dayton St., Madison, WI 53706; Jan Prins,Department of Computer Science, Sitterson Hall 083a, University of northCarolina, Chapel Hill, NC 27514.andM;Efficient Implementation of the First-Fit Strategy for DynamicandM;Storage AllocationandM;We describe an algorithm that efficiently implements the first-fit strategyfor dynamic storage allocation.andP;  The algorithm imposes a storage overhead ofonly one word per allocated blcok (plus a few percent of the total space usedfor dynamic storage), and the time required to allocate or free a block isO(log W), where W is the maximum number of words allocated dynamically.andP;  Thealgorithm is faster than many commonly used algorithms, especially when manysmall blocks are allocated,  and has good worst-case behavior.andP;  It isrelatively easy to implement  and could be used internally by an operatingsystem or to provide run-time support for high-level languages such as Pascaland Ada.andP;  A Pascal implementation is given in the Appendix.andM;For Correspondence: Computer Sciences Laboratory, Australian NationalUniversity, GPO Box 4, Canberra ACT 2601, Australia.andM;On Lamport's Interprocessor Communication ModelandM;Leslie Lamport presented a set of axioms in 1979 that capture the essentialproperties of the temporal relationships between complex and perhapsunspecified activities within any system, and proceeded to use this axiomsystem to prove the correctness of sophisticated algorithms  for reliablecommunication and mutual exclusion in systems without shared memory.andP;  As astep toward a more complete methatheory of Lamport's axiom system, this paperdetermines the extent to which that system differs from systems based on&quot;atomic,&quot; or indivisible, actions.andP;  Theorem 1 shows that only very weakconditions need be satisfied in addition to the given axioms to guarantee theexistence of an atomic &quot;model,&quot; while Proposition 1 gives sufficientconditions under which any such model must be a &quot;faithful&quot; representation.andO;Finally, Theorem 2 restates a result of Lamport showing exactly when a systemcan be thought of as made up of set of atomic events that can be totallyordered temporarily.andP;  A new constructive proof is offered for this result.andM;For Correspondence: Computer Science Department, Florida Institute ofTechnology, Melbourne, FL 32901.andM;Static Inference of Modes and Data Dependencies in LogicandM;ProgramsandM;Mode and data dependency analyses find many applications in the generation ofefficient executable code for logic programs.andP;  For example, mode informationcan be used to generate specialized unification instructions wherepermissible, to detect determinacy and functionality of programs, generateindex structures more intelligently, reduce  the amount of run-time tests insystems that support goal suspension,  and in the integration of logic andfunctional languages.andP;  Data dependency information can be used for varioussource-level optimizing transformations, to improve backtracking behavior andto parallelize logic programs.andP;  This paper describes and proves correct analgorithm for the static inference of modes and data dependencies in aprogram.andP;  The algorithm is shown to be quite efficient for programs commonlyencountered in practice.andM;For Correspondence: Department of Computer Science, University of Arizona,Tucson, AZ 85721.andM;Functional Computations in Logic-ProgramsandM;Although the ability to simulate nondeterminism and to complete multiplesolutions for a single query is a powerful and attractive feature of logicprogramming languages, it is expensive in both time and space.andP;  Sinceprograms in such languages are very often functional, that is, they do notproduce more than one distinct solution for a single input, this overhead isespecially undesirable.andP;  This paper describes how programs may be analyzedstatically to determine which literals and  predicates are functional, andhow the program may then be optimized using this information.andP;  Our notion of&quot;functionality&quot; subsumes the notion of &quot;determinacy&quot; that has been consideredby various researchers.andP;  Our algorithm is less reliant on language featuressuch as the cut, and thus extends more easily to parallel executionstrategies, than others that have been proposed.andM;For Correspondence: S. K. Debray, Department of Computer Science, TheUniversity of Arizona, Tucson, AZ 85721; D. S. Warren, Department of ComputerScience, State University of New York at Stony Brook, Stony Brook, NY 11794.andO;</TEXT></DOC>