<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-514-531  </DOCNO><DOCID>09 514 531.andM;</DOCID><JOURNAL>IBM Systems Journal  Sept 1990 v29 n3 p435(16)* Full Text COPYRIGHT International Business Machines Corp. 1990.andM;</JOURNAL><TITLE>Intelligent Forms Processing. (technical)</TITLE><AUTHOR>Casey, R.G.; Ferguson, D.R.andM;</AUTHOR><SUMMARY>The automatic reading of optically scanned forms consists of twomajor components: extraction of the data image from the form andinterpretation of the image as coded alphanumerics.andP;  The secondcomponent is also known as optical character recognition, or OCR.andO;We have implemented a method for entry of a wide variety of formsthat contain machine-printed data and that are often produced inbusiness environments.andP;  The function, called Intelligent FormsProcessing (IFP), accepts conventional forms that call forinformation to be printed in designated blank areas, but in whichthe information may exceed boundaries due to poor registrationduring printing.andP;  The human eye easily accommodates data thatimpinge on form boundaries or on background text; however, thesame powers of discrimination applied to machine processing pose atechnical challenge.andP;  The IFP system uses a setup phase to createa model of each form that is to be read.andP;  Scanned forms containingdata are compared against the matching form model.andP;  Specialalgorithms are employed to extract data fields while removingbackground printing (e.g., form lines) intersecting the data.andP;  Theextracted data images are interpreted by an OCR process that readstypical monospace fonts.andP;  New fonts may be added easily in aseparate design mode.andP;  If the data are alphabetic, a lexicon maybe assembled to define the possible entries.andP;  (Reprinted bypermission of the publisher.)andM;</SUMMARY><DESCRIPT>Topic:     Forms ManagementOptical Character Recognition SoftwareImage ProcessingScanning SoftwareDocument Processing.andO;Feature:   illustrationchartgraph.andO;Caption:   Intelligent Forms Processing overview. (chart)Forms editor. (chart)Methods of form registration. (chart)Various editing operations. (chart)Decision tree classification. (chart)Algorithm for pattern word analysis. (chart)Combining clustering and tree decisions. (chart)Processing time vs print quality. (graph)andM;</DESCRIPT><TEXT>Intelligent Forms ProcessingandM;A form is a conventional means for recording data, but it is seldom the finalrepository for data.andP;  Inevitably, at least part of the informationrepresented on forms is transferred elsewhere in order to be recalled forlater reference.andP;  Before the advent of computers, a bookkeeper's job in anyindustry consisted largely of filling in the columns of a lendger volume withdata from transaction slips.andP;  In the computer age, the secondary destinationis often a database system.andM;The increased use of computers has brought a continually decreasing cost formaintaining and using large volumes of data.andP;  Where the information is notgenerated by computer, or otherwise not accessible electronically, thelabor-intensive task of feeding data into a processing system has come toaccount for a greater percentage of the overall cost.andP;  Eventually, perhaps,most data now entered on paper forms will be entered into databases directlyat origination.andP;  Before the simple, conventional methods using paper formscan be replaced by electronic methods, the problems of user acceptance, dataconversion between diverse systems, and legal requirements for record keepingmust be overcome.andM;An image solution to capture the data can avoid some of these problems.andO;Ideally this approach would permit minimal disruption in the way a businessprocesses its transactions using paper records.andP;  As shown elsewhere in thisissue, [1] many of the requirements for recording, maintaining, anddistributing information on documents can be met using optically scannedrepresentations.andP;  However, database searching and other machine processing ofthe document contents require that it be recorded as alphanumeric codesrather than as image.andP;  Thus we are led to consider methods for automaticallyinterpreting the data on the form images in order to create correspondingrecords in a database.andM;The automatic encoding of character images is called optical characterrecognition (OCR).andP;  OCR originated to meet the need for high-speed input ofdata from billing statements and other documents particularly designed fordata processing.andP;  To achieve the greatest accuracy and performance, specialstylized print fonts have been developed and used in the printing of the formdata.andP;  OCR has also been developed to a high capability in the reading ofconventional machine-printed text such as typed pages or magazine articles.andO;In Japan, where key entry is more difficult because of the thousands ofdifferent symbols used, considerable progress has been made in the OCR ofhand-printed data.andM;If forms are specially designed for machine processing, and if the data areimprinted according to certain specifications, as in the case of credit cardsslips, then high accuracy can be achieved.andP;  The possibility of interferencefrom background printing can be removed completely by printing the forms in acolor such as red or green that can be made invisible to the scanner by theuse of light-restricting filters.andP;  Redundant data, such as check sums, aresometimes printed along with the data in order to permit error detection andcorrection.andM;In some cases the forms cannot be redesigned for computer simplicity.andO;Examples of this include archives containing forms from past years.andO;Currently, the authors are pursuing a project involving approximately 40million birth, death, marriage, and dissolution certificates that have beencollecting in the state of California's archives for over 80 years.andP;  A keypart of the project calls for transfer to a computer database of the datacontaining in these certificates.andM;We are also seeking to incorporate similar capability into the regular flowof data within an enterprise.andP;  Our system, Intelligent Forms Processing(IFP), a component of the state of California's Vital Records ImprovementProject (VRIP), is intended to process data on forms designed according tocurrent practices in form layout and usage.andP;  Figure 1 depicts an overview ofthe IFP.andP;  The system will accommodate misplacement of data in the fields, useof conventional print fonts, and the mixing of forms in batch processing.andO;Optical scanning of forms provides input for display applications such asdistribution, printing, and reviewing.andP;  By converting the data content tosymbol codes and organizing it in a database, IFP permits conventionalprocessing applications such as indexing, search, and retrieval based oncontent, sorting, update, statistics gathering, etc.andM;A variety of print styles is expected to be encountered.andP;  One majorconstraint in this area, in keeping with present capabilities in OCR, is thefocus on reading machine-printed rather than hand-printed characters.andP;  If abreakthrough occurs in the OCR of hand printing (or even further in thefuture, in handwritten script), the general schema of this system can bedirectly extended to include these capabilities.andP;  At present the system doespresume some previous knowledge of document typestyles or fonts.andP;  Mostmachine-printed forms are prepared on typewriters or electronic printersusing basic print styles with the primary objective of portraying dataclearly and legibly.andP;  The aesthetic considerations of document compositionthat are paramount in general publishing are absent, and so the myriad oftype styles used in books, magazines, and newspapers is not encountered.andP;  Infact, a survey of several thousand documents stored in our initialapplication reveals that only a handful of fonts predominate.andP;  [2]andM;The next section discusses the problem of reading conventional forms bymachine, and details the major functions that a form reader for suchdocuments must possess.andP;  The authors present their approach to implementingsuch a system.andP;  First a method is given for determining the genre or class towhich a form belongs, when different forms are mixed in batch processing.andO;Next a method is discussed for specifying and locating the data to be read ona form, and for separating the data image from background printing that mightinterfere with the recognition process.andP;  OCR of data images is described,with special attention to lexical processing for cases where there is advanceknowledge of admissible data values.andP;  The final section summarizes thediscussion and describes how a user can adapt the system to changingrequirements by modifying the parameters used for OCR and lexical analysis.andM;Difficulties in automated forms processingandM;This section discusses the problems that are the most critical whenconsidering the design of a system for extracting and encoding data fromforms.andM;Many form formats.andP;  In typical business environments, many different kinds offoms are used to document frequently occurring events.andP;  Often these eventshappen randomly, making it difficult to predict which type of form will berequired for the next transaction.andP;  The result is a mixed batch of forms ofdiffering types.andP;  An automated system accepting these forms must be able toidenify each piece of input with its form type.andP;  Such a system must be ableto accommodate the many different form types that might be used within asingle environment.andM;Errors in typing information onto a form.andP;  Data may be entered on a formeither by a person or by a computer-controlled printer.andP;  In either case thedata can be inaccurately positioned on the form.andP;  For instance, it requiresexperience and care to register a form in a typewriter such that entries areprinted precisely in the blank areas of each field.andP;  Typing frequentlyinpinges on lines or other background printing.andM;The encoding system must be able to detect that the data have beenmisregistered and to compensate for the misregistration.andP;  In addition, whendata come into contact with preprinted lines or text on a form, an automatedsystem should be able to differentiate between the preprinted matter and thedata.andM;Poor typing quality.andP;  If the ribbon on a typewriter is worn, or if theprinting mechanism is not functioning correctly, then data appearing on theform can be of poor quality.andP;  Likewise, a carbon copy of a document has asignificantly lower quality than the original document.andP;  Other factors thataffect typing quality include the age of the document, the color of the inkthat was used, the quality of the paper (i.e., newsprint versus linen), andthe amount of handling the document has received.andM;Many of today's OCR techniques rely on high quality type such as that foundin magazines, journals, and other typeset publications.andP;  However, the printquality on business forms varies widely, depending on the factors listedabove.andP;  In such an environment, a forms-reading system must be able toaccurately recognize print with a broad range of quality.andM;Many fonts.andP;  Printing devices differe from one business to another.andP;  Evenwithin a single enterprise that receives documents from many sources or hasaccumulated documents over an extended period of time, many differentprinting devices are represented.andP;  As a result, the size and shape ofcharacters (i.., the typestyle or font) that appear on documents will alsovary greatly.andM;Scanner misregistration.andP;  Optical scanners introduce some degradation in thequality of images.andP;  This loss of quality can be minimized by careful hardwaredesign.andP;  More difficult to avoid, however, are differences in location of agiven data field within the scanned versions of successive documents.andP;  Thismisregistration typically consists of differences both in offset and skewangle of the document.andP;  The consequence is that processing routines cannotreliably retrieve data images from a prespecified image location, especiallywhen the form has not been specially designed for OCR or is densely packedwith data.andM;The Intelligent Forms Processing systemandM;When a form is scanned, the first problem to be solved is to identify theform as a member of a group of previously defined form types.andP;  The process ofclassifying the form is called form recognition.andP;  For form recognition to besuccessful, each type of form that the system will be required to processmust be defined.andP;  The IFP system employs a setup phase, called the formseditor, to define each form.andM;Once the form has been identified, the data that the user desires to capturemust be found and extracted from the image.andP;  The data extraction function ofthe IFP system is used to locate the desire data and to extract them whileignoring the form's preprinted lines and text, as well as data that the userdoes not wish to capture.andM;After the desired image data have been extracted, they must be converted to amachine-readable format (i.e., American Standard Code for InformationInterchange [ACSII] or extended binary-coded decimal interchange code[EDCDIC]) using OCR.andP;  Within the Intelligent Forms Processing function,several methods of OCR are used in combination in order to identify asaccurately as possible the image data being processed.andP;  The first techniqueapplied, decision tree classification, relies on the configuration of thepixels that make up a character in order to recognize it.andP;  The secondtechnique, lexical analysis, modifies the initial recognition result based onlinguistic context and on the identification of groups of characters havingsimilar shape.andP;  Since these techniques complement one another, the accuracyof recognition can be greatly enhanced.andM;Two additional functions are provided to support and improve the recognitioncapability.andP;  The first function is a mechanism for extending the library offonts recognizable to decision tree classification.andP;  The second functionpermits extensions to the library of lexicons (a lexicon is a list ofpossible values for a field--e.g., a city field might have a list of citiesassociated with it) that are used by lexical analysis in evaluating thecontents of a data field.andM;The following sections define each of these processes in more detail.andM;Form editor.andP;  Before forms can be processed by IFP, a model must be createdfor each type of form to be processed.andP;  The model of a form type consists ofa form pattern and a description of each field contained on the form.andP;  A formpattern is the set of characteristics that are used to distinguish one formtype from another.andP;  The field description consists of the location of thefield on the form (expressed in Cartesian coordinates), an acceptancethreshold for OCR, and a lexicon.andM;In an IFP form model, a data field is expressed by two points that describethe opposing corners of a rectangle.andP;  The rectangular area is called a mask.andO;The assumption of a rectangle for describing a field is made in order tosimplify the geometric calculations required when extracting a field from animage.andP;  In the future, more complex field description coordinates might beused to describe a field of irregular shape.andM;The acceptance threshold is used by the decision tree classification todetermine whether the recognition of a character is reliable enough to beaccepted.andP;  The lexicon is used by lexical analysis to verify and improve OCRaccuracy.andP;  These field attributes are describes in more detail later in thepaper.andM;A system may be required to process a large variety of different forms, manyof which are slight variations of a single form type.andP;  Since it would beredundant to re-enter the same information for each of a number of similarform types, IFP provides a mechanism for grouping form types.andP;  Such a group,called a form class, is made up of a list of fields and their associatedacceptance thresholds and lexicons.andP;  Once a form class is created to defineall of the fields that may occur within a group, all that needs to bespecified for each form type is the location of its fields.andM;The forms editor specifies form information to the IFP system.andP;  The formseditor is an OS/2 [R] Presentation Manager[TM] program that allows the userto scan in a blank form that serves as a master for a specified form model.andO;With the forms editor, the form types form model.andP;  With the forms editor, theform types may be grouped into form classes.andP;  Figure 2 illustrates a birthcertificate form definition.andP;  After this information has been entered intothe forms editor, forms containing data can be read by IFP.andM;Form recognition.andP;  Several methods were considered for recognizing a formtype (Figure 3).andP;  One method matches the form number that is typicallyprinted on the document.andP;  The second method compares the layout, orgeography, of the form's text and lines to differentiate form types.andP;  Thethird method, and the one that is incorporated by IFP, relies on thehorizontal and vertical lines of a form.andP;  Each of these methods is discussedbelow.andM;Most forms contain a preprinted number that identifies the form type.andO;However, in order to read the form number it must first be located on theform, because the identifying number is often printed in different locationsfor different form types.andP;  In addition, since most forms contain a great dealof other preprinted information, the process of differentiating a form numberfrom the surrounding form information can be quite complex.andM;Using the layout of both the text and lines on the form would appear to be areliable method for distinguishing one form from another.andP;  This method has adifficulty, however, because the data recorded on the form cannot easily beseparated from preprinted matter until the form type is known.andP;  The data varyfor each instance of a form, and unless these data can be ignored, therecognition of a form is difficult.andP;  In addition, the processing involved inrecording the attributes of the form and comparing these attributes to theform patterns contained in the form library can be quite expensive, becausethe forms may be quite complex.andM;The IFP system uses the horizontal and vertical lines that are printed on theform.andP;  Form lines are usually longer and thinner than image patterns thatrepresent preprinted text or typed data.andP;  Hence it is easy to detect the formlines.andP;  Once the occurrence of each line on a form is recorded, these linescan be compared to those stored in the models in the form library todetermine a form's type.andM;In a system that processes many form types, the time required to determine aform's type would be excessive if each form had to be compared to many modelsin the form library to seek a match.andP;  IFP makes use of a binary decision treeto compare only a subset of the lines to specified lines of the form models.andO;(This decision tree logic is based on the principles used for clusteringduring OCR.)andP;  The use of this scheme greatly reduces the performance cost ofhaving a large number of form types in the form library.andM;Once the form type has been identified, a complete comparison of the formlines is made between the matching form type's model and the form beingprocessed, in order to verify that the incoming form image is indeed of theidentified form type.andP;  This is necessary in case a previously unseen formtype is scanned, or in scanning a faded form whose lines cannot be detectedadequately.andP;  If the form is successfully identified, the comparison alsoprovides information about the positional differences between the form type'smodel and the input form.andP;  This information is used by data extraction toaccurately locate the fields on the form.andM;The primary weakness of this method is that it cannot differentiate betweentwo form types if they share the same line layout.andP;  This can occur, forexample, if a form type is revised by changing the meaning of a field withoutaltering the form layout, or if the form type contains no lines.andP;  In suchcases the forms would have to be batched and manually identified to thesystem, or else other information, e.g., the form ID number, would have to bederived.andM;Data extraction.andP;  Three steps are required to extract the data from thefields of a form.andP;  The first step is to adjust the mask coordinates (thesewere defined using the forms editor) so that the positional differencebetween the master form and input form is compensated for.andP;  Second, the dataare extracted from each field using the mask coordinates.andP;  Finally anyextraneous lines that intrude into the fields are removed.andM;Registration.andP;  The degree of form skew, the horizontal offset, and thevertical offset affect the ability to accurately locate and capture the datain the desired fields.andP;  Each of these variables is used to adjust the maskcoordinates so that the data may be more accurately captured.andM;The skew of a form (Figure 4) is the degree of rotation difference betweenthe master form and the incoming form.andP;  Since each mask is defined as ahorizontal rectangle, increasing the form skew raises the chances ofbackground form information residing within the rectangle.andP;  To reduce thepossibility of the mask becoming contaminated with background information, itmust be adjusted based on the form skew.andM;IFP adjusts the masks by reducing the height of the mask as the skewincreases (see Figure 5).andP;  This reduction serves to lessen the possibility ofcapturing unwanted background information, since the adjusted mask representsthe area within the original mask that has not been affected by skew.andO;Unfortunately, the reduction of the mask size increases the possibility thatthe desired data will not lie entirely within the mask.andP;  This serves tointroduce ambiguity during the extraction process, which tends to increasethe processing time required to extract the data.andM;An alternative that was considered for adjusting the mask involves rotatingthe mask rectangle by the degree of the form skew.andP;  This solution bothreduces the possibility of capturing unwanted background information andpreserves the data the lie within the mask.andP;  The processing required tohandle a rotated mask during the extraction process was deemed to be tooexpensive.andM;The horizontal and vertical offsets represent the translation required to mapthe incoming form to the master form.andP;  For instance, if a line on the formappeared in the 100th row for the master form and in the 96th row for theincoming form, then the vertical offset would be 4, since it would requireadjusting the incoming form down 4 rows to compensate for the difference.andO;These differences are generally caused by the placement of the form in thescanner or by the differences between scanners in initiating the scanning ofa form.andM;The offsets are calculated by comparing the incoming form's line descriptionsto the master form's pattern (Figure 6).andP;  After the offsets have beendetermined, they are used to translate the masks' absolute coordinates(defined using the forms editor) into the incoming form's coordinates.andM;Masking of data.andP;  Once the mask coordinates have been mapped onto the form tobe read, the data within each mask can be extracted.andP;  During the process ofextracting the data, two types of data may be encountered: perfect data orambiguous data (Figure 7).andM;Perfect data are data that reside entirely within the mask.andP;  A check is madewhether the pixels along the perimeter of the mask are all 0, or off.andP;  If allof the pixels are off, the data within the mask are considered to be perfect.andO;Perfect data can be extracted immediately.andM;Ambiguous data exist whether there is an on bit along the perimeter of themask.andP;  Ambiguous data are data that extend outside of the mask and that mayencounter interference from the form lines or background text.andP;  Each instanceof such data must be tracked beyond the mask and extracted using specialprocessing.andM;Ambiguous data that extend sufficiently far from their mask may cross a formline.andP;  Therefore, as the ambiguous data are tracked, the tracking location iscompared with the locations of the lines that were found during the formrecognition process.andP;  Any interfering form line is detected and removed.andM;Aline removal process, as illustrated in Figure 8, is used to eliminate theline without removing the data that intersect the line.andP;  The process isperformed by deleting the line and then replicating any pixels that lie justabove and below the line through the region of the deletion.andM;Although resolving conflicts between the background text and the data is animportant process, algorithms to solve this type of ambiguity areconsiderably more complex and have not yet been implemented within IFP.andO;Simply measuring the average height of extracted data and clipping trackeddata beyond this height offers an attractive method of dealing with data thatdo not merge far into background text.andM;Removal of nondata images.andP;  In addition to the problems above, there is alsothe possibility that some ambiguous data might not be a part of the dataprinted in the field, but rather belong to an extraneous line or markintruding into the field.andP;  This type of problem is detected when the heightof the ambiguous data is determined to be larger than the maximum heightexpected for a single character.andM;When such an extraneous line (which may be curved) is detected, acontinuity-following algorithm [3] is applied to track the line throughintersections with the data (Figure 9).andP;  The tracked line is deleted exceptat intersections with the data or form lines.andM;As each field is processed and the data contained within the field areextracted, the extracted field is placed into a new area of memory.andP;  Thisarea can be thought of as a new or extracted image which contains only thedata portion of the form image.andP;  The extracted image is then passed tooptical character recognition for the automatic encoding of the data.andM;Optical character recognition.andP;  The image created by data extraction providesa clean image of the data to be recognized using OCR.andP;  As described above,this is done field-by-field, with the characters in each field extracted as asingle image block.andP;  Before a field image can be recognized, it must besegmented into individual character images.andP;  These are then recognized inturn by a classifier.andM;Segmentation.andP;  In the process of analyzing the overall image into characterpatterns, the segmentation routine performs the following functions:andM;* The pitch, i.e., the distance from character to character, is estimated.andM;* Touching characters are separated, and broken characters are merged.andM;* Skew of the typing within each field is measuredandM;* The overall image is partitioned into print lines.andM;* A baseline is computed for each line of print.andM;* The character patterns are ordered in reading sequence.andM;* The position of each character with respect to the baseline is calculated.andM;* Spaces between words are detected.andM;The input to these operations is the set of connected components of theextracted image, as shown in Figure 10.andP;  A connected component is a subimagesatisfying two conditions: (1) from any black pixel of the subimage there isa path consisting solely of black pixels that connects it to any other blackpixel of the subimage, and (2) there is no such connection from a pixel ofthe subimage to any black pixel outside the subimage.andP;  Typically a connectedcomponent is represented by its bounding rectangle, the box with vertical andhorizontal edges determined by the top, bottom, left, and right edges of theconnected component.andP;  Figure 10A shows an image whose connectivity is trackedto determine the distinct subimages indicated.andP;  The horizontal and verticalextent of each subimage is measured during the process, and for segmentationprocessing the characters are represented only by the bounding rectangles, asshown in Figure 10B.andP;  Notice that the letter i yields two boundingrectangles, which will be combined during segmentation processing.andM;Connected components of scanned printing frequently correspond to individualcharacters, thus the ensemble of connected components is analyzed todetermine pitch, locate baselines, and measure skew.andP;  Following this,touching characters are separated by searching for weak connections betweenleft and right sections of a connected component in the neighborhood of apitch boundary between characters.andP;  Joining of broken characters, sequencingof the patterns for recognition, etc.andP;  are also done by analyzing theconnected components.andM;Decision tree classification.andP;  An OCR classifier is a unit that accepts asingle character pattern as input, and returns an identification symbol, orID (for example an ASCII or EBCDIC code).andP;  It has previously been shown inReference 4 that highly accurate recognition of a given font style can beobtained using decision trees that test a series of prespecified pixels oneach character.andP;  The decision trees are designed automatically usingstatistics on the probability of black and white for each pixel, where thestatistics are gathered from scanned sample characters.andM;Figure 11 represents a decision tree classification.andP;  The circled numbersdenote (row, column) coordinates in a character array.andP;  The recognitionprocess starts at the top and determines the color of the input pattern atthe specified location.andP;  If it is white, the process repeats at the next nodedown and to the left: it black, the node down and to the right is checked.andO;Thus, the classification algorithm follows a path which terminates in asymbol class.andP;  In practice the trees are much larger, identifying 100 or moreclasses and constructed with several thousand nodes.andM;When documents arrive from many different sources, as it typically the casewith typed data on forms, a library of tree logics is needed, one for eachfont that will be encountered.andP;  The proper font logic for a given document isdetermined by trial and error using the first few lines of the docoment.andO;Fonts having size characteristics that match those of the printing are triedfor recognition, and each classifier provides its own estimate of theaccuracy of its recognition.andP;  These estimates are evaluated to select thebest classifier for reading the remainder of the image.andM;In a survey of vital records documents, the authors found that six fontscomprised more than 98 percent of the machine printing.andP;  The sections belowshow how decision trees for additional fonts can be added to the basiclibrary in order to accommodate printing not recognized in a first pass.andM;Speed of recognition with the decision tree method depends not only uponimplementation, but also upon quality of the printing.andP;  Figure 12 shows howthe time required for recognition varies as documents are successivelydegraded in quality by recopying with a poorly tuned photocopier.andP;  The rateof correct recognition remains high (over 99.5 percent), but the time spentin classifying degraded third-generation copies is more than double thatrequired for first copies.andP;  The times in Figure 12 area relative units.andP;  Theideal document is the norm.andP;  For poor quality characters, the classifiershifts to a more complex algorithm requiring additional passes through thedecision trees, thus increasing the processing time.andM;Second pass recognition.andP;  In reading large volumes of forms we expect toprocess a variety of fonts.andP;  Documents containing standard fonts can be readsuccessfully, but any documents printed in fonts not represented in thelibrary of decision trees will be rejected.andP;  If only a few documents arerejected, the data can be manually keyed.andP;  However, if many documents areprinted in alien fonts, they can still be efficiently entered by OCR by thefollowing steps:andM;1.andP;  Group rejected documents having a common print style.andM;2.andP;  Collect and identify sample characters from each group.andM;3.andP;  Design new decision tree logics for each group, using the statistics ofthe identified samples.andM;4.andP;  Repeat the recognition process on rejected documents using the newlycreated decision trees.andM;It is not necessary to use the entire collection of rejected documents insteps 1 to 3.andP;  The design stage need only have a few samples of eachcharacter type.andP;  If the character distribution is that of typical Englishtext, for example, then after several thousand characters have beencollected, there is a high probability of having a sample of each letter.andP;  Ifa letter is not represented after, say, 10 000 samples, then one anticipatesthat its frequency of occurrence in the remaining documents will be low aswell, and it can be rejected and keyed manually when encountered.andM;The following section discuss the implementation of the second-pass design.andM;Clustering.andP;  The authors have previously published (Reference 5) a method forefficiently matching character patterns.andP;  The algorithm accepts as input asequence of character patterns, plus a similarity criterion for matchingpairs of patterns.andP;  It produces as output a list of prototype patternsselected from the inputs.andP;  the prototypes serve as representatives of theinput set by virtue of two properties: (1) no two prototypes match oneanother, and (2) every input pattern matches some prototype (or possibly morethan one).andM;With each prototype is defined a cluster, consisting of the prototype itselfplus all patterns that match it.andP;  Applied to a sample of 10 000 or socharacters in a single font, this algorithm typically produces clusters foreach of the symbols present, as well as a number of small clusters resultingfrom touching or broken characters, or from shape variations.andM;When the input character sequence is printed text, cryptographic methods foridentifying the clusters have been explored.andP;  [6]  Alternatively, theclusters can be identified manually after the prototypes are displayed on ascreen.andP;  In single-font typing applications, e.g., most form data, only atypical typing keyboard of 100 or fewer characters has to be keyed in orderto label a font.andM;Document grouping by font.andP;  The clustering algorithm groups characterpatterns.andP;  A method for grouping the documents according to the similarity ofprint was explored.andP;  Each such group of documents should contain a commonprint style, and two different groups should contain dissimilar printing.andO;Thus the groups fit the definition of clusters (see above) except thatsimilarity is defined over a pair of documents rather than with respect to apair of character patterns.andM;A natural measure of similarity is the number of common characters on twodocuments.andP;  Pursuing this concept, document grouping can be integrated withcharacter clustering.andP;  This is done by clustering an input document usingeach of the prototype sets obtained from previous documents.andP;  If there areconsiderable matches between a prototype set and a document, then thecharacters on the document are clustered using the matching prototype set.andO;If no prototype set matches the document, then its characters are used tostart a new set.andM;This process can be made efficient by using early cutoff rules for thematching and by using font characteristics such as pitch, height, width,etc., to select prototype sets for matching a given document.andP;  The measure ofsimilarity for document grouping should take into account the possibility ofmultifont documents, so that, for example, bursts of non-matching charactersshould be a basis for rejection of the match, and such documents should bescreened from the design set.andM;Lexical analysis.andP;  Generally the data contained in a particular field of aform are constrained in the sense that not every character string ispermissible.andP;  An amount field is typically filled in with numeric data; aname field, with alphabetic data.andP;  Such constraints are useful for characterrecognition.andP;  For example, a classifier designed to read only ten digits ismore accurate than one that must consider not only digits, but also lettersand punctuation, as possible identities for each character pattern.andM;Even stronger constraints hold when the possible entries in a field can belisted.andP;  For example, one field may contain a reply to a question withpossible values yes or no.andP;  In such a case the system can verify that therecognition result is one of the two possibilities, and if not, can simplycount the number of character patterns in the field in order to make achoice.andM;A different field may require a longer list of candidate words.andP;  Thus, afield labeled &quot;State&quot; has only 50 possible entries, plus abbreviations.andP;  Weterm such a list of admissible entries a lexicon.andP;  The yes-no example, whilethe simplest case, illustrates the general principles in using lexicons.andO;First, the lexicon can serve to validate a recognition result obtained by aconventional classifier.andP;  Second, in case of a validation failure it canassist in making the correct recognition decision by a process of eliminationof candidates.andM;Clustering and lexicons.andP;  When a field is governed by a lexicon, recognitionmay be considered to consist of choosing the correct word from the prescribedlist.andP;  A pattern classifier, the standard tool in OCR, is useful in thisprocess, but is not the only basis for selection.andP;  Clustering in the contextof separating fonts and selecting sample characters for the design ofdecision tree classifiers has been discussed previously.andP;  Clustering resultsfrom a single field can also assist in making a selection from a lexicon.andM;Clustering reveals the similarities that exist among the characters in afield.andP;  In the name Pennsylvania, for example, letter positions 3, 4, and 10are filled by the same letter, and possitions 9 and 12 by another duplicatedletter.andP;  One way to represent the property of similarity is by atransformation in which the letters of the alphabet are successivelysubstituted for the letters of the word starting from the first letter of theword.andP;  Thus the first letter of Pennsylvania is replaced by A, the second byB, and so on, with the same symbol always substituted for any repeatingletters.andP;  By this rule, Pennsylvania is encoded as ABCCDEFGHCIH.andM;Such an encoding is called a pattern word in cryptography, and can be derivedfrom an optically scanned, segmented word image by clusterng the sequence ofcharacter patterns in order to detect similarities.andP;  Having obtained thepattern word ABCCDEFGHCIH by clustering, it is not known that the imageprocessed is the word Pennsylvania, but the system knows that it seeks a wordwith the same letter in positions 3, 4, and 10, and a second letter inpositions 9 and 12, and that all the other letters occur once each.andP;  If inaddition it is known that the word must belong to a lexicon consisting of the50 state names, the system can quickly infer that it must be Pennsylvania.andO;On the other hand, the pattern word ABCD is shared by both Iowa and Utah,illustrating in general, clustering must be combined with other forms of OCR.andM;Pattern words can be precomputed for a given lexicon, and stored inassociation with the words from which they are derived.andP;  If the lexicon issearched on the pattern word, then clustering alone quickly reduces thenumber of candidate words for a given field to the subset having a commonpattern word.andM;The fact that pattern words are derived from analyzing shapes duringclustering introduces additional complexity.andP;  Thus, Alabama yields thepattern word ABCDCEC if only the first letter is capitalized, and ABACADA ifprinted in all uppercase letters.andP;  Both forms must be considered in selectingcandidates from the lexicon.andM;Combining classifier and clustering results.andP;  both decision trees andclustering processes are subject to errors due to the variations that occurfrom sample to sample of a given symbol.andP;  However, both processes giveinformation about the contents of the field.andP;  It is therefore worthwhile toattempt to combine the two results in order to obtain maximum overallaccuracy.andM;In general, if the tree classifier yields a result that is in the lexicon andthat has a pattern word consistent with the clustering result, then therecognition can be accepted.andP;  In cases where the clustering andclassification conflict, some scheme is needed for evaluating multiplecandidates for the OCR result, or for informing the system that no clearcutdecision can be found.andP;  In the latter case, the field can be displayed to anoperator and manually keyed.andM;The algorithm that is proposed for making decisions based both on decisiontree classifiers and lexical analysis using clustering is shown in Figure 13.andO;The system initially produces a word by a sequence of tree decisions.andP;  Thepattern word for this result is computed.andP;  Another pattern word is obtainedby clustering.andP;  If the two agree, then the classifier result will be acceptedif the pattern word is also in the lexicon.andP;  Otherwise both decision resultsare in contradiction with the lexicon, and the results should be manuallyreviewed.andM;When the pattern word obtained by clustering differs from that produced bythe tree classifier, then a weighted decision is made.andP;  The classifier wordfetches one candidate list from the lexicon, the clustering pattern wordfetches another.andP;  Each candidate word is scored based on (1) agreements withthe classifier results, and (2) number of matches versus the clusteringpattern word.andP;  In addition, the tree classifier confidence measure for eachletter identification is used to weight these scores.andM;Global clustering.andP;  Often all the data on a completed form are typed by asingle printer.andP;  Clustering the character patterns of such a document yieldsa map of similarity that extends across fields.andP;  These similarityrelationships permit the results of lexical analysis to be applied even tofields that are not covered by lexicons.andP;  Thus, if Field 1 is constrained bya lexicon, and lexical analysis indicates that a certain symbol must be an R,then R is also likely substitution for a symbol belonging to the same clusterin a field not covered by a lexicon.andM;In general, as before, classifier results and lexical results must beweighted to produce a combined decision for each field.andP;  The authors areseeking to construct a statistical model that will guide the making of suchdecisions.andP;  In the meantime simple, heuristic rules for combining decisionsare being implemented.andP;  Currently the system makes independent recognitionsfor each field, using a combination of lexical analysis and classification,as described above.andM;Then it compares the results with similarities obtained by clustering anddetects any discrepancies.andP;  In Figure 14, for example, each pair of linesrepresents a data field of a test document.andP;  The tree classifier result(first line) is compared with the clustering result (second line).andP;  Thefigure shows fields which contain characters in cluster 9.andP;  A discrepancyappears in the fourth pair, where a sample of this cluster has been labeledn, but u in all other occurrences.andP;  Based on the statistical evidence, thesystem would label all members of cluster 9 as u.andP;  Additional evidence, ifneeded, would be supplied by the measure of confidence for each decisiongiven by the tree classifier, and by any lexicons that might be available forthe fields in question.andM;A disputed character is assigned an identification based on (1) the IDs ofpatterns to which it is similar, (2) the tree classifier ID obtained for it,and (3) in the case where a lexicon exists for the field, Ids which, ifassigned to the character, will yield words that are listed in the lexicon.andO;If no ID is sufficiently favored by the scoring rule, then the field isrejected in favor of manual entry.andM;Extending the library of lexicons.andP;  In order to use lexical analysis on afield-by-field basis, a mechanism for defining the possible values for afield is required.andP;  Within IFP, this mechanism is the lexicon editor.andP;  Thistool provides a means for adding a lexicon to a repository called the lexiconlibrary, deleting a lexicon from the lexicon library, or replacing a lexiconin the lexicon library.andM;To add or replace a lexicon in the library, a list of possible values isfirst created and stored in a flat file.andP;  This flat file may be created usingany means that the person creating the lexicon desires.andP;  Next, the lexiconeditor is invoked by specifying the name of the flat file, the name by whichthe lexicon will be referenced when defining forms in the forms editor (or,for replacing a lexicon, the current name of the lexicon in the library), andthe lexicon's type.andP;  IFP will add the lexicon to the lexicon library.andM;To delete a lexicon, its name is specified.andP;  If the lexicon does exist, itwill be removed from the lexicon library.andM;SummaryandM;The authors have shown how the characteristics of printed forms can beadvantageously used in constructing a system to automatically read data forinput to databases.andP;  The background structure of forms, particularly the useof lines to create field boundaries, is used for registration and forrecognition of form type.andP;  Data are first extracted as image by a searchprocess that is initiated at the central region of each form data field.andO;They are then coded into character strings using conventional OCR processingassisted by a clustering operation that takes advantage of lexicalconstraints to improve accuracy of recognition.andM;The paper has discussed how difficult problems in forms reading can be solvedwithin IFP.andP;  Thus IFP recognizes characters that cross field boundaries,detecting and removing lines that pass through the characters in the process.andO;It permits quick logic extensions in order to read unfamiliar font styles,and reports inconsistent or unreliable results to the overall system formanual correction.andM;AcknowledgmentsandM;In May of 1989, the state of California contracted with IBM to develop asystem for capturing and maintaining the Office of State Registrar's vitalrecords.andP;  The willingness of the state to invest in this system made itpossible to further refine the IFP algorithms and to develop the firstproduction system based on this technology.andP;  In addition to the authors, theIFP and forms editor development teams were key to the further refinement ofthe IFP algorithms and the successful implementation of IFP.andP;  The authorsalso wish to thank the developers of the IBM TextReader[R] product.andP;  Theirwork in OCR contributed to the design of IFP's OCR algorithms.andP;  Without thetechnical leadership provided by the system architects, Edward P. Hall andBruce D. Sayre, and the managerial support provided by Patricia Bell, CarmelaCoons, and Beatrice DeRocco, the development of IFP would not have beenpossible.andM;Cited referencesandM;[1] B. T. Perry, B. A. Wester, W. W. Baker, and J. F. Kemmis, &quot;ExperienceGained in Implementing ImagePlus,&quot; IBM Systems Journal 29, No.andP;  3, 467-488(1990, this issue).andM;[2] State of California Vital Records Improvement Project, OCR RequirementsStudy, IBM Corporation, July 21, 1989.andM;[3] T. Clement, &quot;The Extraction of Line-Structured Data from EngineeringDrawings,&quot; Pattern Recognition 14, No.andP;  1, 43 (1981).andM;[4] R. G. Casey and C. R. Jih, &quot;A Processor-based OCR System,&quot; IBM Journal ofResearch and Development 27, No.andP;  4, 386-399 (July 1983).andM;[5] R. G. Casey, C. K. Chai, and K. Y. Wong, &quot;Unsupervised Construction ofDecision Networks for Pattern Classification,&quot; IEEE 7th InternationalConference on Pattern Recognition, Montreal (August 1984).andM;[6] R. G. Casey, &quot;Text OCR by Solving a Cryptogram,&quot; IEEE 8th InternationalConference on Pattern Recognition, Paris (November 1986).andM;Richard G. Casey IBM Research Division, 5600 Cottle Road, San Jose,California 95193.andP;  Dr. Casey received a B.E.E.andP;  from Manhattan College in1954 and an M.S.andP;  and Eng.Sc.D.andP;  from Columbia University, New York, in 1958and 1965 respectively.andP;  Since joining IBM in 1963, he has been a researcherin visual pattern recognition, first at the  Thomas J. Watson Research Centerin Yorktown, New York, and after 1970 in San Jose, California.andP;  Dr. Casey hasdeveloped trainable recognition logic suitable for small computers such asPCs and has implemented a self-learning technique for reading machine-printedtext independent of typeface, using cryptographic analysis.andM;David R. Ferguson IBM Systems Integration Division, 520 Capitol Mall,Sacramento, California 95814.andP;  Mr. Ferguson received a B.S.andP;  in computerscience from California State University, Sacramento, in 1988.andP;  Since joiningIBM in 1988, he has been responsible for the development of an opticalcharacter recognition solution for the state of California's Vital RecordsImprovement Project (VRIP).andP;  His responsibilities have included participatingin the design of an image management system for VRIP and designing theIntelligent Forms Processing system.andO;</TEXT></DOC>