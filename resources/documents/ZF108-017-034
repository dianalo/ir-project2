<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-017-034  </DOCNO><DOCID>08 017 034.andM;</DOCID><JOURNAL>Patricia Seybold's Network Monitor  Jan 1990 v5 n1 p1(11)* Full Text COPYRIGHT Seybold Office Computing Inc 1990.andM;</JOURNAL><TITLE>Distributed computing environments.andO;</TITLE><AUTHOR>Millikin, Michael D.andM;</AUTHOR><DESCRIPT>Topic:     Distributed ProcessingDistributed Operating SystemsOpen Software FoundationDefinitionsConnectivityAccess MethodsNetwork Access ProceduresNetwork Architecture.andO;Feature:   illustrationchart.andO;Caption:   The virtual network. (chart)OSF's view of DCE. (chart)DE positioning. (chart)andM;</DESCRIPT><TEXT>Distributed Computing EnvironmentsandM;WHEN WE FIRST wrote about Apollo's Network Computing System (NCS) severalyears ago, commercial knowledge about and acceptance of distributed networkcomputing was scant.andP;  Distributed Computing Environments (DCEs) were in therealm of academia and scientific/engineering workstations.andP;  NCS's profile wasso low that people were likely to respond to a query about NCS with &quot;NCS?andO;You mean NFS, don't you?&quot;andM;All that has changed, and changed rapidly.andP;  The strategic architectures ofevery major vendor are now based on some form of distributed networkcomputing.andP;  The Open Software Foundation (OSF) is in the midst of evaluatingthe technologies it received in response to its request for technology (RFT)for Distributed Computing Environments.andP;  The vendor response to OSF's RFT wasstaggering: Fifty vendors submitted technologies (only 29 of those were OSFmembers).andP;  The OSF hopes to announce its decision in April of 1990.andM;Some of you who categorize OSF as being on the losing end of a contest withUnix International to deliver a Unix operating system may be a triflesurprised at the organization's interest in distributed networks.andM;OSF has a good reason for pushing the Distributed Computing Environment sohard: The primary concern of its members is interoperability.andP;  The membershipof OSF is driven, in turn, by the concerns of its users.andP;  OSF seems to bebecoming a lens focusing various technology efforts that require consensus,if not standardization.andP;  Some of the areas OSF is tackling arekernel-specific, but others--such as the DCE work--are not.andM;We're not going to adduce the business needs that mandate the transition todistributed network computing yet another time here.andP;  Instead, we will acceptthose needs as given and flesh out a technology framework for the discussionof DCE trends and the evaluation of DCE platforms and products.andM;The OSF RFT has helped considerably in this process.andP;  OSF originallyhypothesized that five technology areas comprise DCE.andP;  The submissionsencompassed 17 discrete areas, some of which OSF decided were out of thescope of its current evaluation.andP;  Nevertheless, the surfacing of thetechnology framework provides a valuable reference point for discussion andextension.andM;WHAT IS A DISTRIBUTED COMPUTING ENVIRONMENT?andP;  A Distributed ComputingEnvironment provides users and applications with transparent access to data,resources, and services strewn across a heterogeneous network.andP;  The key torealizing the theoretical benefit of such an architecture is transparency.andO;Users can't spend their time thrashing about trying to figure out wheresomething is.andP;  Nor should business users have to worry about mounting remotevolumes.andP;  Developers shouldn't have to code into their applications thelocations of resources on the net.andP;  From the MIS viewpoint, the networkshould be manageable.andP;  The final picture is one of a &quot;virtual&quot; network: acollection of workgroup, departmental, enterprise, and interenterprise LANsthat appear as a seamless and easily accessed whole to the end user.andM;Such a network, which must be multivendor, masks its fundamentalheterogeneity.andP;  The network becomes the computer (a tag line several vendorsconsidered using as a slogan until Sun trademarked it as &quot;The Network is theComputer&quot;).andM;Within this schema, there clearly is a wide range of functionality andcapability.andP;  Distributed computing can mean transparent access to a remotefile.andP;  It can also mean an environment that lets me split up the processes ofan application and parcel them out to available compute servers.andP;  The secondscenario, which takes us closer to distributed operating systems, is furtheraway from implementation than is the remote file system.andP;  But both areaspects of the same environment.andM;That, in turn, points out an important aspect of the emerging DCE--it shouldbe architected in a rational manner to allow future extensibility.andP;  Thedegree of current architectural specificity is, however, a point of greatcontention.andP;  At the OSF members meeting to discuss the DCE submissions, forexample, one faction insisted that certain submissions were too grandiose,that we are too early in the process of creating a DCE to have a soup-to-nutsplan.andP;  In rebuttal, another member offered the analogy of building a house.andO;You could design and build a house one room at a time, this response went,but the result might look a little funny and might not produce an optimalbuilding.andP;  On the other hand, drawing up a complete architecture from thebeginning does not prevent the builder from building only one room at a time.andO;The difference would be that the final structure would function much betteras a building, rather than as a collection of tacked-together rooms.andP;  Ofcourse, you could always rebut that by pointing out that even the architectedbuilding would probably need add-ons, and, by starting with a small number ofrooms, you would be better able to create a unifying architecture at a latertime when you knew what the additions would look like.andP;  And so on and so on,until you bash the analogy to splinters.andM;Regardless of factional affiliation (architect-the-whole now orarchitect-the-whole later), those concerned with designing and implementingthe DCE need to have a shared technology framework to discuss the relevanceor timeliness of a given product or approach.andM;We will use the OSF technology framework as a starting point for discussion.andO;Not being bounded by a need to produce deliverable product within a certaintime frame, we will not limit this discussion to those technologies that fitbest within the scope of the OSF RFT.andP;  Nor is this article a completediscussion of the relative merits of the technologies submitted to the OSF.andO;Where we adduce specific products, it is to provide a fairly well-knownexample within a given category, or to high-light a particular issue.andM;The DCE Technology FrameworkandM;The key characteristic of the emerging DCE is that it postulates the virtualextension of a computer system over multiple systems distributed across anetwork.andP;  Using the &quot;network is the computer&quot; model, you quickly realizethat, as a computer is more than a file system, a viable DCE must have morethan a network file system and a remote procedure call.andM;The needed components range from system services such as security and filemanagement to distributed file systems to distributed application processes.andO;If the network is truly to be the computer, then all the aspects that makeour computers into viable, functioning systems must be replicated and, inmost cases, augmented to provide transparency over a heterogeneous network.andM;Particularly when examining areas of the distributed computing environmentthat generate a great deal of partisan enthusiasm, it is important toremember that the DCE framework is multidimensional, and that technologiesare, in many cases, orthogonal.andP;  In other words, selecting a technology alongone dimension does not directly affect a technology along another dimension.andO;It is this ability to mix and match that OSF will exploit in its ultimaterecommendation for a distributed computing environment.andM;SECURITY.andP;  With the Internet virus still fresh in many people's minds,security is a major issue within the Distributed Computing Environment.andP;  Byits very nature, a distributed environment offers more points of entry tomalefactors.andP;  Two major areas of concern within DCE security are accesscontrol and authentication.andM;Although many vendors have dealt with the issue of access control for anapplication on a homogeneous network, the problem obviously becomes morecomplex when dealing with a heterogeneous environment.andP;  In addition to therequired security capabilities, access control in a DCE really requirespolicies governing security domains.andP;  As the scope of the problem expands,the capabilities of the applied solution need to become both more flexibleand broader.andP;  Security domains, as a specific example, can reduce theadministrative overhead required within a DCE.andM;Access control alone is only a partial solution, however.andP;  Although you canforce a workstation or host to prove its identity, you still end up relyingupon the system's word as to the identity of a given user.andP;  An authenticationservice is a mechanism for providing trusted third-party verification of useridentities.andP;  Authentication services must exist within a distributed networkenvironment where a workstation cannot be trusted to identify itself or itsusers correctly to shared network services.andM;An authentication service, which basically requires the user to prove his orher identity for each required service, must be secure, reliable,transparent, and scalable.andP;  A model for authentication services currentlymuch in favor is Kerberos, which was developed as part of MIT's ProjectAthena.andP;  (Kerberos was the three-headed dog that guarded the entrance to theinfernal regions.andP;  An interesting view of the nature of data servers, no?)andM;Kerberos uses private key encryption: Each Kerberos principal has a privatekey known only to the principal and to Kerberos.andP;  Kerberos maintains adatabase of clients and their keys.andP;  Although a network may run Kerberos onmore than one machine, there is only one definitive copy of the Kerberosdatabase.andP;  Other Kerberos servers may use read-only copies of the database toeliminate single point-of-failure issues.andM;Kerberos provides three levels of protection.andP;  The lowest requires only thatauthenticity be established at the initiation of a connection, assuming thatsubsequent network messages flow from the authenticated principal.andP;  The nextlevel up requires the authentication of each network message.andP;  On the levelbeyond these &quot;safe messages&quot; are private messages, which are encrypted aswell as authenticated.andM;NAMING AND DIRECTORY SERVICES.andP;  Once disparate systems are internetworked,issues such as global naming and directory services become fundamental to theseamless implementation of a distributed environment.andP;  Allowing users to findother users for mail messages is the base level of the directory services'functions.andP;  Application clients need to find servers, accounting andadministration information needs to be built on top of the basic directory,and, ultimately, objects must be able to send messages to other objects.andO;Naming and directory services, in short, enable location independence in thedistributed environment.andM;The directory service must support large-scale and long-lived networks, allowdelegations of authority, support the use of aliases, and provide lookup andbrowsing mechanisms.andP;  A hierarchical directory service that supports bothpartitioning and replication also seems appropriate for supporting aneffective distributed environment, especially since that is the structuralmodel of the emerging X.500 directory standard.andM;The designers of the X.500 standard intend it to support simple lookup,browsing, a yellow pages searching function, group membership, andauthentication services in a heterogeneous environment.andM;Indeed, X.500-based services seem to be the way the industry will go asquickly as possible.andP;  Sun did not submit its Yellow Pages directory servicefor its Network File System (NFS) to OSF, because it plans to make thetransition to X.500 itself.andP;  For some vendors, such as Sun, the issue becomeswhether or not to implement a more functional medium-term name service as aplace holder until full X.500-compliant services are ready for rollout.andM;Since the X.500 standard is still in the Draft International Standard (DIS)stage, many in the industry are making do in the interim with naming anddirectory services that position themselves closely to the emerging X.500specifications.andM;For example, Digital Equipment Corporation's DECdns/X.500 is the technologybase upon which Digital is building an X.500-conformant product.andP;  Banyan alsohas proffered its naming service, Streetalk, to OSF.andM;TIME SERVICES.andP;  With so many different systems being tied together to act asone computer, all the clocks on these different systems should besynchronized.andP;  This is important for distributed file access as well as fordistributed procedures and processes.andP;  For example, one system may bedependent on a piece of information from another remote system before a keyrequest can be handled.andP;  Systems need to know when backups can be performedand how their downtime affects the other processes in the virtual network.andM;A distributed time service is one of those easily overlooked services that isessential to the continued reliability and integrity of a distributedenvironment.andP;  Digital, again, has a nice little piece of technology to meetthis need, the Digital Distributed Time Synchronization Service (DECdts).andM;DISTRIBUTED FILE SYSTEMS.andP;  Say &quot;distributed networking,&quot; and the first thingmost people think of is a distributed file system--probably Sun's NFS.andO;Because the files that users would like to access are stored throughout thevirtual network, it is important to have a distributed file system to manageand transparently locate these pieces of information.andM;Sun designed NFS to provide interoperability among different machines andsystems with performance comparable to a small local disk.andP;  Sun believed thata client or server crash should not affect other nodes on the network, thatthe file system should remain transparent to existing applications, and thatadministering the file system should be no more taxing than managing a localhard disk.andM;NFS has succeeded greatly at one level: It is widely licensed and has becomerecognized as a de facto standard.andP;  With its stateless protocol design, NFSallows clients to survive a server crash and a planned or unplanned shutdown.andM;However, NFS really needs better cache consistency and locking for sharedread/write.andP;  (Without that, there is a problem with sharing a file betweentwo or more client machines.)andP;  And, although the administration is no tougherthan managing a local hard disk, that's still pretty bad--certainly not atask for the average user.andM;Furthermore, NFS was designed as a LAN-based distributed file system.andP;  Tobetter address the expanded requirements of a wide area network distributedfile system, Transarc is proposing the Andrew File System (AFS).andP;  Transarcwas formed by a group from Carnegie-Mellon University to commercializeaspects of the Andrew System.andM;The AFS uses a global file space, allowing all clients to see the same filename space, regardless of location.andP;  By contrast, the NFS client refers to asubtree offered by a specific node.andP;  AFS also uses Kerberos authentication tomanage autonomous administrative domains, analogous to independent Kerberosrealms.andP;  The use of these autonomous administrative domains--or cells--endowsAFS with excellent scalability.andP;  The cells cooperate to provide the globalfile space.andM;AFS tries to minimize network and server loads by caching accessed data onthe local disk and caching the status of the data in memory.andP;  In AFS, theserver notifies the client in case the data at the server changes.andP;  Thisguaranteed callback ensures cache consistency.andP;  In Carnegie MellonUniversity's experience with the AFS, the caching with callbacks allowedservers to handle 200 to 300 clients without performance degradation.andP;  (TheCMU network is the largest AFS installation, with 3 cells, 25 servers, 1,200clients, and 9,000 users.)andM;Critics brand networked file systems as being unreasonably slow.andP;  Some recentresearch points out, however, that networks can actually achieve betterperformance with a high-performance server and a distributed file system thanwith slower local disks on workstations.andP;  (Coming up with optimalconfiguration for a given application need should provide vendors with plentyof opportunity to position their products against one another.)andM;DISTRIBUTED PRINT SERVICE.andP;  In a distributed heterogeneous environment, anapplication can reside anywhere and be accessed from anywhere, and printingservices have to accommodate this model.andP;  The notion of a printer beingcontrolled and accessed by a single system no longer applies.andP;  Printing hasto migrate from the hierarchical model to the notion of a distributedprinting service available to all nodes in the environment.andM;There are several interesting models floating around, such as the Palladiumprint service software from MIT's Project Athena.andP;  (MIT is submitting anumber of major network services to OSF: Kerberos, Hesiod for naming, Zephyrfor notification, Moira for service management, and Palladium.)andM;TERMINAL SERVICE.andP;  In the future, the prices will drop low enough to permitmore users to have workstations on their desks.andP;  There will still be arolefor low-cost terminals on the desktop, however.andP;  Therefore, even within DCE,accommodation must be made for terminals.andP;  Support for terminals could rangefrom support for the newer X-terminals to a virtual terminal service forolder, character-based tubes.andM;PC INTERFACES.andP;  Because so much critical data resides on PCs, support for DOSis an important issue.andP;  At a low level, PC NFS is an example of a solutionfor client file system access.andP;  However, it's clear that PCs need to play amuch broader role within DCE simply because they are so pervasive.andP;  Also, asmore powerful PCs (Unix or OS/2 clients or, later, Macs) earn more desktopspace, the difference between the PC and the workstation will erode.andM;The two most likely solutions for integrating PCs into the DCE framework alsocorrespond to the PC LAN operating system platforms with the greatestpresence: NetWare and LAN Manager.andM;Currently, the relative share of these two platforms is a bit skewed.andO;NetWare is by far the heavyweight.andP;  However, LAN Manager should be picking upsteam in the market for a variety of reasons.andM;* The endorsement and subsequent implementation of LAN Manager by systemsvendors such as Digital, Hewlett-Packard, and IBM.andM;* The new multiprocessing implementations of LAN Manager.andM;* The delivery of LAN Manager for Unix (LM/X).andM;We are seeing the creation of two primary axes within the evolution of thedistributed environment.andP;  One is an extension of the old Apollo NetworkComputing System (NCS) camp, represented now mainly by HP, Digital, and IBM(with alliances with smaller, strategic vendors, including Microsoft).andP;  Theother is the old Sun Open Network Computing (ONC) camp, represented by Sunand the Unix International licensees.andM;It is interesting to note that LAN Manager has also been submitted forconsideration to OSF as part of a comprehensive, partnered submission fromHP, Digital, IBM, Microsoft, Transarc, and Locus.andP;  Perhaps the closest thingto a submission from the other camp in this area is Netwise's submission ofits RPC compiler technology.andP;  Following on the heels of theSun/Netwise/Novell agreement on RPC tools, accepting the Netwise technologywould be tantamount to providing a fairly straightforward avenue for theintegration of NetWare LANs into the OSF distributed environment.andM;Because of the heat generated by any discussion of the PC LAN area, we expectthis particular discussion to become quite contentious.andP;  Netwise can point toits support of NetWare, coming support for Sun, and original support for LANManager as justification for its being considered as a de facto industrystandard.andP;  On the other hand, the Netwise technology doesn't fit neatly intothe plans of the NCS/LAN Manager group.andM;RPC WARS.andP;  Communications between applications and between remote systems isa key component that enables the creation of the distributed computingenvironment.andP;  At the first OSF members meeting concerning the DCE RFT, therewas remarkable unanimity of mind in opting for a remote procedure call (RPC)mechanism as the primary enabler of interoperability across heterogeneoussystems.andM;The RPC extends the procedure call mechanism familiar to programmers fortransferring control and data within a program across a network.andP;  Remote ordistributed parts of the application thus simply become procedures thatexecute on a remote machine.andP;  The RPC uses a request/reply model ofcommunication.andM;The requesting client issues a call to a local stub routine that handles allthe network interactions necessary to complete the call and communicates witha counterpart stub on the server.andP;  The server stub then deals with the serverprogram, communicating the result back to the client stub and thence to theoriginal requesting client.andP;  Through such mechanisms, the RPC shields theprogrammer from the underlying network.andM;Having decided RPC is the way to go, there follows the politically-chargeddiscussion over which RPC to use.andP;  The two primary options are theSun/Netwise RPC from Open Network Computing (ONC) and the NCS RPC from theHP/Apollo Network Computing Architecture (NCA) with enhancements by Digital.andO;(This isn't to say that there are no other RPC mechanisms available.andP;  Thereare, and there are other binding mechanisms and RPC compilers that work withthe various RPC mechanisms, as well.andP;  But the two most visible in the marketare the ONC and NCA variants.)andM;Although nominally the two RPC mechanisms perform comparable functions, theyare architecturally quite different.andP;  The architectural difference tends todampen the hopes of those who might wish the OSF to opt for a politicallysanitized solution--trying to mix in the Sun RPC with the NCS architecture,for example.andM;The major differences between the NCA RPC and the ONC RPC appear to be intheir approaches to data representation, and their binding models.andP;  Thereused to be a difference in the approach to the interface language and to theunderlying network mechanisms as well.andP;  However, recent activity on Sun'spart has reduced those differences.andM;Sun's earlier RPC language and &quot;rpcgen&quot; tool proved a poor match for thecapabilities of the NCA Network Interface Definition Language (NIDL).andO;Pursuant to the relationship with Netwise (which has several former Apollofolks), Sun now will be offering a more robust, generalized interfacelanguage that offers developers access to a much broader market than just Sunworkstations.andP;  Furthermore, the use of the Transport Layer Interface (TLI) inUnix System V.4 frees Sun from a previous IP application programminginterface.andM;There is a clear difference in the two models of data representation.andP;  Whenon a network of heterogeneous machines, you need some form of datarepresentation protocol to take data from one type of system to another.andP;  Thedata representation protocol defines a way to present data so that machineswith varying local representation can communicate typed values to oneanother.andP;  (As an example, VAXen represent integers with the least significantbyte at the low address, while the 68000 family represents integers with themost significant byte at the low address.)andM;Sun chose to use a canonical form of data representation called XDR (ExternalData Representation).andP;  In the Sun scheme, all data is converted to the XDRcanon, even if the data happens to be passing between two machines of thesame type.andM;The NCA advocates pounce on this as waste.andP;  The Network Data Representation(NDR) uses a &quot;receiver make it right&quot; approach.andP;  In other words, it is up tothe receiver to convert the data representation only when the incoming formatdiffers from the receiver's native format.andP;  Although this pairwise conversionmay seem counterintuitive in a marketplace increasingly driven to canonicalintermediaries for data exchange by the international standards processes,the NCA supporters believe that the overhead incurred in the translation willeventually become a bottleneck as demands on the network grow.andP;  In addition,the NDR is actually something of a hybrid between the receiver-makes-it-rightapproach and the canonical approach, so the actual burden of conversion issomething less than the [n.sup.2] - n number of conversions mandated in apairwise scheme.andM;Another benefit Sun gained from the recent Netwise agreement is a migrationpath to ASN.1 (Abstract Syntax Notation One), the ISO approach to datarepresentation.andP;  The Netwise RPCTool compiler produces ASN.1 encoding.andO;(Another company, NorCom, also has submitted an ASN.1 compiler to OSF forconsideration.)andP;  Some NCA-ites, however, hint that an eventual ASN.2 will bemore akin to NDR.andP;  Given the time frames involved with the standards process,however, such speculation does seem a bit unrelated to current problems.andM;Binding refers to the process of establishing a reference to the target of aremote call--a major issue for developers who need to locate the servicesdesired for their distributed applications across the network.andP;  NCA uses anobject-oriented binding model.andP;  It encourages developers to think of remotecalls as operations upon objects, not as calls to specific machines or serverprocesses.andP;  The goal of the NCA designers was to reduce the precision withwhich a developer must specify the location of a target for a call.andM;A very important aspect of the NCA object-oriented model is its use of a128-bit, fixed-length Universal Unique Identifier (UUID) as the baseidentification mechanism for any entity on the network.andP;  The NCA architectsclaim that the advantages of the UUID over a simple string identifier at thelowest level of the system include smaller size, the ease of embedding theUUID in data structures, location transparency, and the ability to layervarious naming strategies on top of the primitive naming mechanism.andM;NCA is architected to exploit the capabilities of all its components.andP;  Thus,the NCA/RPC packet format has a space for an object UUID.andP;  And theNCA-defined interfaces for talking to the Location Broker, a distributedapplication that aids client applications in finding objects, also havearguments that are object UUIDs.andP;  For reasons like this, the concept ofswapping out the NCA/RPC for another RPC--such as the Sun RPC--as a politicalexpediency is anathema to the NCA camp, and rightly so.andP;  The result would beworse than a camel.andM;Within the NCA schema, a third-party becomes involved in the client/servercommunication: a broker.andP;  The client uses broadcast binding to find theLocation Broker (LB).andP;  The Location Broker then returns the network addressof a server process that is willing to handle operations for a specifiedclient object.andM;Developers not wishing to become embroiled with the object-orientedcapabilities of NCA can bypass those object-oriented features and can use NCAas a conventional RPC system.andP;  (The benefit here, presumably, is thepotential for migration in the future to the object-oriented model.)andM;In the ONC model, the client application first must establish contact withthe server through an RPC creation procedure that specifies the name of thehost on which the server process is running, the name and version number ofthe program to be called, and the network name used to reach the server.andP;  AnRPC service, rpcbind, locates the server program and the listener, or inetd,procedures to start the server if that application isn't running.andP;  Anotherprocedure returns the client handle.andM;A special broadcast service within rpcbind can call all servers of aparticular type on a LAN without knowing the hosts on which those serversrun.andP;  The rpcbind broadcast sends a message to all rpcbind servers requestingthat they in turn relay the call to any servers running on their machines.andO;It is this mechanism to which Sun proponents point when they claim that ONChas as much capability for providing location services as does NCA withLocation Broker.andP;  And, indeed, this rpcbind service does undermine the NCAclaim that ONC binding calls must all be explicit.andP;  But even Sun and ATandamp;Tconcede that this broadcast mechanism is heavy on network resource usage.andM;DISTRIBUTED DEVELOPMENT ENVIRONMENTS.andP;  One of the major gating factorsblocking widespread implementation of DCE is the availability andsophistication of developers' tools.andP;  Requiring application developers tobecome communications gurus is counterproductive.andP;  (Some might call it abrain-dead approach.)andP;  The DCE must offer transparency to the developer aswell as to the end user.andM;There are a variety of levels where developers' tools must appear.andO;Initially, the market needs a transport and RPC-independent programmaticinterface that will protect developers from writing to a host of differenttransports and RPCs.andM;A future requirement will be for tools that help the developer constructdistributed applications, including those that split an application in themost effective manner.andP;  Also needed are methodologies for writingapplications for DCE.andP;  System criteria such as security and scalability haveto be taken into account in the development environment.andM;The scope of the development environment must go beyond the applicationissues themselves.andP;  Security is a concern (enter services such as Kerberos).andO;But there is another need for tools that will help developers measure andevaluate configuration and performance when designing distributedapplications.andP;  This will help developers design applications that takeadvantage of the network and are optimized for that environment.andM;Providing development tools for the DCE will become an area where vendorsstrive to distinguish themselves competitively, we believe.andP;  HP has alreadytaken an interesting approach to this with its Team Computing program.andO;Ultimately, Team Computing will define a family of products that incorporatesome of the advanced distributed network computing technology of NCS.andP;  Thefirst deliverable out of the Team Computing program, the Task Broker, hasnothing to do with NCS, however.andP;  It is an intelligent batch facility thatparcels out entire applications to available and appropriate compute nodes onthe network.andP;  There is no need to change existing application code.andP;  The TaskBroker provides a very easy point of entry for distributed network computingand allows users to get an immediate appreciation of the benefits of such anenvironment.andP;  So attractive has this approach been that several largecustomers have requested alpha code, rather than the full-cycle QA'ddeliverable.andP;  (Developers at HP labs actually came up with Task Broker fortheir own use.)andM;Currently, there are two main contenders to provide basic development toolsfor the distributed computing environment.andP;  One is the Netwise RPCToolcompiler; the other is the Network Interface Definition Language (NIDL)within NCS.andP;  Each requires the developer to write only for a given set ofAPIs and then transparently generate the appropriate stub code.andP;  There areother mechanisms for generating such code, such as rpcgen within Sun's OpenNetwork Computing environment, but these don't have the potential forproviding an acceptable solution to the market in heterogeneous environments.andM;MIGRATION AND CONVERSION AIDS.andP;  Most end users will not move from theirtraditional computing environments to DCE overnight.andP;  They will, instead,migrate slowly to integrate their existing applications and systems.andO;Therefore, users need to understand what it takes to make the migration.andO;They also require software conversion aids that help them migrate aconventionally written application to DCE.andP;  At the same time, they need toolsthat will allow for part of the system to be based on DCE while the rest isstill based on the older hierarchical computing model.andM;TRANSACTION SERVICES.andP;  For a DCE to be viable commercially, it must supportOnline Transaction Processing (OLTP).andP;  Users need the confidence of knowingthat transaction integrity is guaranteed.andP;  In the long term, this shouldextend to the heterogeneous database environment.andM;DISTRIBUTED OPERATING SYSTEMS.andP;  In the long term, users will gain betterutilization of their available resources if processes and the operatingsystem itself can be migrated or distributed across a network.andM;There is a difference between distributing procedures and distributedprocesses.andP;  The procedure, using a remote procedure call, is a concept easilyadopted by most programmers.andP;  A process represents a finer degree ofgranularity within the application.andP;  Distributing those finer-grainedprocesses of an application out over a network requires either a great dealof expertise and experience or a powerful tool set.andP;  Ultimately, however, thedistribution of application and system processes across multiple processorsor multiple computers within a network will yield even greater power.andM;Within the scope of the current RFT, OSF has received a variety of responseswith differing capabilities, ranging from the Transparent Computing Facility(TCF) from IBM and Locus to the Chorus/MIX distributed operating system fromChorus Systems in France.andM;Chorus provides a network-transparent IPC facility (low-level IPC and RPC) asa foundation to support standard, transparently distributed, open operatingsystems services such as Unix services in Chorus/MIX.andM;Chorus has three main characteristics.andM;* A minimal nucleus (a.k.a.andP;  kernel) that can support a variety of operatingsystems, providing distributed processing and communication.andM;* Real-time services with multithreading and preemptive, fixed-priorityscheduling.andM;* A machine-independent, modular architecture that supports parallel andmultiprocessing systems.andM;The Chorus nucleus has both local and global responsibilities.andP;  On a locallevel, it has three components.andM;* Real-time Executive, which provides preemptive, fixed-priority schedulingand synchronization.andM;* Virtual Memory Manager (VM Manager), which directs local memory allocationand structures virtual memory address spaces.andM;* Hardware Supervisor, which provides dynamic loading of external events(such as interrupts, traps, and exceptions).andM;On a global level, there is the IPC Manager, which issues messagestransparently to any node in the system.andP;  The IPC Manager keeps track ofwhere messages are going and delivers them via RPCs and asynchonous messages.andO;Sometimes, external system servers are called upon to support various networkprotocols.andP;  The Chorus nucleus structure hides its distributed nature fromthose using it.andP;  Local seervices compute locally, and global services rely onthe cooperation among nuclei to cope with distribution.andP;  The IPC is the onlycommunication tool; all sites use the IPC rather than dedicated protocols.andM;What do you get out of such distributed operating system design (along withthat of Amoeba, Mach, and Stanford's V)?andP;  High performance and utilization ofmultiprocessing and multicomputers.andP;  Distributed (and object-oriented)operating system technology will play an increasingly important role in yearsto come in the realization of distributed computing environments.andM;As we gain the capability of transparently distributing processes acrosscommercially available networks, we come much closer to the time whenparallel processing and neural networks will become much more powerful andcommon technologies.andP;  In research labs, developers are currently at work onsuch multicomputers that rely upon proprietary communications schemes toachieve high-performance distribution of processes.andP;  Such capabilities willeventually appear on commercial networks, given the proper technologyfoundations.andM;DISTRIBUTED SYSTEM ADMINISTRATION AND MANAGEMENT.andP;  Network management andadministration is a key concern within the homogeneous systems environment.andO;There is a need to know what is happening with the network to make sure thenetwork is being run efficiently and effectively and to handle and resolveproblems.andP;  The same management issues surface in a heterogeneous environment.andO;However, because in a heterogeneous environment a user is dealing with somany more systems, applications, and data, the problems and requirements aremagnified.andM;Although some aspects of this problem are being addressed by certainsubmission in response to the DCE RFT, OSF is at work on developing aspecific system administration RFT for release sometime in the near future.andM;LICENSE SERVER.andP;  Software distribution becomes a major issue in a distributedenvironment, especially when parts of applications and operating systems aredistributed.andP;  These needs to be a method for providing users on differentsystems with updated code.andP;  The license server handles this problem byallowing servers to distribute appropriate software to all nodes.andM;A license service also controls the number of users accessing a piece ofsoftware so that a vendor can keep track of license fees.andP;  In addition, it isa means of ensuring that all users are on the same software releases, and itcan be used to update distributed directory services.andP;  Safeguards must bebuilt into the license service concept.andP;  For example, there may be instanceswhen users are not ready to update software.andP;  Therefore, they need the optionof upgrading software on a delayed basis.andP;  And finally, license servertechnology can provide a low-risk way for users to try new software, whichcould be loaded on a server for demo purposes.andM;EXTENSIBILITY.andP;  No single vendor can expect to control the whole computingenvironment of an end user.andP;  There are too many variables and specializedneeds.andP;  Therefore, the environment should support extension by third parties.andM;DE Corum As An ExampleandM;Interestingly, the most comprehensive submission to the OSF in response tothe DCE RFT came from a set of partners: the Distributed Environment (DE)proposal from HP, IBM, Locus, Transarc, Digital, and Microsoft.andP;  The scope ofthe joint submission (called DE Corum) left some members enthusiastic, otherssuspicious and contentious.andP;  It was, in fact, the scope of the productsincluded in this submission that prompted the remarks we noted earlier aboutbuilding a house (see &quot;What Is a Distributed Computing Environment?&quot; above).andM;The major components of the technology base of DE consist of the Andrew FileSystem, Kerberos, LM/X, NCS, the Network Time Protocol (NTP), and TCF.andP;  Thecompanies involved in the submission integrated the disparate components intoa single architectural framework, providing a comprehensive solution forcurrent needs as well as an extensible model to incorporate advancedsolutions in the future.andM;The DE submitters divided their architecture into five areas: services,data-sharing, compute-sharing, distributed programming and RPC, and systemadministration.andM;SERVICES.andP;  Services in the DE Corum submission include Kerberosauthentication, a time synchronization service and a name service fromDigital, an Attribute Broker (which is part of NCS Version 2.0), and aPosix-compliant, concurrent thread-processing capability (Digital's ConcertMultithread ARchitecture--CMA--services).andM;The offering of a CMA service doesn't conflict with Mach, therecently-sanctioned multiprocessing platform for OSF that also ismultithreaded.andP;  The OSF DCE process is designed to be kernel-neutral.andP;  Inother words, what OSF picks should be able to be implemented on a variety ofplatforms.andP;  Although OSF will have Mach, there obviously will be a variety ofsystems that must be integrated into a distributed computing environment thatdo not currently support multithreading.andP;  Without the support of lightweightprocesses, or threads, it can be extremely difficult to implement complexserver applications and parallel processing.andP;  CMA provides portable,process-level threads that can be implemented on a variety of platforms, thusextending the multithreaded capability to non-OSF systems, if need be.andP;  IBMand HP already have endorsed CMA.andM;These services may be decentralized and automatically replicated forreliability and availability.andP;  Objects on the network may be located by nameor by attribute.andP;  DE services are modular, with programming interfacesproviding access to developers.andP;  The current name service, based on the DARPAnaming service used by the government, for example, can be replaced by theX.500 at some point.andM;DATA SHARING.andP;  The data-sharing components include the Andrew File System(which is fully interoperable with existing NFS systems) and itsadministrative tools, as well as the PC integration provided by LM/X.andP;  Theenvironment supports diskless clients as well.andM;COMPUTE-SHARING.andP;  DE compute-sharing consists of clustered operation, remoteprocessing and process migration, remote special files, and hiddendirectories.andP;  DE supports both clusters and cells.andP;  A cluster is a group ofmachines, users, and processes that all see the same resources with the samenames.andP;  The enforced name-space uniformity demands a tighter coupling ofcluster machines that that of a cell, but is also provides the foundation fora higher degree of transparency.andP;  A cell, on the other hand, is a superset ofa cluster.andP;  The cell is an administrative domain that consists of acollection of networked computers.andP;  Cells have global names and permitlocation-transparent access to data within the cell.andP;  The storage of data andprograms in a common name space helps camouflage the distributed nature ofthe environment.andM;Within DE, the cluster is the basis for the functionally highest level ofresource-sharing--compute-sharing.andP;  The DE compute-sharing (based on theIBM/Locus TCF) provides dynamic load-balancing between heterogeneous machinesin a cluster.andM;DISTRIBUTED PROGRAMMING AND RPC.andP;  The DE submission in this area is based onNCS Version 2.0 from HP/Apollo (the current version on the market is 1.5),with extensions from Digital to augment NCS in a WAN environment.andM;SYSTEM ADMINISTRATION.andP;  DE system administration includes account and networkadministration, active site and user status information, and file systemmanagement from the AFS.andM;RuminationsandM;We have just taken a quick trot through the m ajor technology areas ofdistributed network computing--somewhat the newsletter analogue to the forceditinerary vacation where the tour guide hustles you off the bus, troops youthrough elegant buildings and then stuffs you back onto the bus to roar offto the next destination.andM;Given all that, we believe the high-level view of this write-up is valuableas a starting point.andP;  As we note in the editorial this month, our focusduring the beginning of this decade will be on the emerging DistributedComputing Environments.andP;  We promise more detailed explorations of thespecific issues within given technology areas.andM;By the middle of the year, the OSF should announce its decision and helpshape, in one way or another, the roll-out of DCE.andP;  Without speculating onthe possible outcome of the technology selection process, we offer a fewthoughts:andM;While we understand the concerns of the vendor camp that would rather notproceed too quickly for fear of proceeding in the wrong way, we are moresympathetic to the arguments of the group submitting the DE proposal.andP;  Webelieve a full-architected approach is the only way to stay out of trouble inthe long term.andM;Also percolating in the background of all of this is the ISO work on OpenDistributed Processing (ODP).andP;  We expect, though, that much of the ongoingwork by vendors submitting to the OSF will influence the ODP work in ISO (assome ISO work already has influenced the OSF submissions).andM;There is not an easy or simple solution to resolve the different proposalsfrom the various camps involved in the process.andP;  There are installed bases toconsider, existing technologies and implementations that have to beaccommodated, and compromises to be made to provide mutual benefit.andP;  We arefairly certain that no proposal--not even the DE proposal--alone will seizethe OSF recommendation.andP;  There is, however, a wealth of highly innovativetechnology out in the world.andP;  The OSF RFT process has done the industry agreat service by opening up the perspectives of engineers who may have held amore parochial view of their position in the industry.andP;  We are not technologypoor.andP;  We don't have to slow our progress to wait for the proper vehicles tospeed us along.andM;We applaud the DE partners for recognizing that cooperation and collaborationcan yield a better mutual payoff than direct, unyielding competition.andP;  And weapplaud OSF for tackling a very difficult area to deliver to its memberstheir number one desire: interoperability.andM;As we all watch and participate in the process of the definition ofarchitectures and standards during the coming years, keep in mind thefollowing assessment of the role of ISO made by Marshall Rose of NyserNet:&quot;It's the best we can do in an imperfect world...to damage-control the pain.&quot;andO;</TEXT></DOC>