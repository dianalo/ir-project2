<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-039-616  </DOCNO><DOCID>08 039 616.andM;</DOCID><JOURNAL>Communications of the ACM  Jan 1990 v33 n1 p4(3)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Commentary: human error and the design of computer systems.andO;(Viewpoint.) (column)</TITLE><AUTHOR>Norman, Donald A.andM;</AUTHOR><SUMMARY>Engineers designing computer systems must consider the humans whouse them as system components and allow for human error as aninevitable and predictable phenomenon.andP;  Trying to identify systemcharacteristics that lead to human-interface problems is a moreeffective approach than simply blaming the individual involvedwhen an error occurs.andP;  Systems should be designed with theiroperators' characteristics in mind; the RISKS forum on theelectronic computer networks provides valuable information onincidents that include design and human error, but this data iscollected from biased and often unreliable sources.andP;  NASA-Ames'Aviation Safety Reporting System (ASRS) is a more authoritativeinformation resource which could provide a model for betterdesign.andP;  The National Transportation Safety Board provides asimilar detailed analysis of transportation accidents.andM;</SUMMARY><DESCRIPT>Topic:     Human FactorsComputer DesignSoftware DesignErrors.andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>COMMENTARY: HUMAN ERROR AND THE DESIGN OF COMPUTER SYSTEMS In 1988, theSoviet Union's Phobos 1 satellite was lost on its way to Mars.andP;  Why?andO;According to Science magazine, &quot;not long after the launch, a groundcontroller omitted a single letter in a series of digital commands sent tothe spacecraft.andP;  And by malignant bad luck, that omission caused the code tobe mistranslated in such a way as to trigger the test sequence&quot; (the testsequence was stored in ROM, but was intended to be used only during checkoutof the spacecraft while on the ground) [7].andP;  Phobos went into a tumble fromwhich it never recovered.andM;What a strange report.andP;  &quot;Malignant bad luck?&quot;andP;  Why bad luck: why not baddesign?andP;  Was not the problem the design of the command language that allowedsuch a simple deviant event to have such serious consequences.andM;The effects of electrical noise on signal detectability, identification, andreliability are well-known.andP;  Designers are expected to use error-detectingand correcting codes.andP;  Suppose interference from known sources ofelectromagnetic noise had corrupted the signal to Phobos 1.andP;  We would notblame the ground controllers; we would say that the system designers did notfollow standard engineering practice, and we would reconsider the design ofthe system so as to protect against this problem in the future.andM;People err.andP;  That is a fact of life.andP;  People are not precision machinerydesigned for accuracy.andP;  In fact, we humans are a different kind of deviceentirely.andP;  Creativity, adaptability, and flexibility are our strengths.andO;Continual alertness and precision in action or memory are our weaknesses.andP;  Weare amazingly error tolerant, even when physically damaged.andP;  We are extremelyflexible, robust, creative, and superb at finding explanations and meaningsfrom partial and noisy evidence.andP;  The same properties that lead to suchrobustness and creativity also produce errors.andP;  The natural tendency tointerpret partial information--although often our prime virtue--can causeoperators to misinterpret system behavior in such a plausible way that themisinterpretation can be difficult to discover.andM;Quite a lot is known about human performance and the way it applies to systeminteraction [1].andP;  Several classes of human error have been identified andstudied, and conditions that increase the likelihood of error can bespecified in advance [3-5].andP;  Communication systems can be designed to beerror-tolerant and error-detecting or correcting.andP;  In a similar way, we coulddevise a science of error-tolerant detection or minimization interactionswith human operators [2].andM;Many advances have been made in our understanding of the hardware andsoftware of information processing systems, but one major gap remains: theinclusion of the human operator into the system analysis.andP;  The behavior of aninformation processing system is not a product of the design specifications;it is a product of the interaction between the human and the system.andP;  Thedesigner must consider the properties of all the system components--includingthe humans--as well as their interactions.andP;  The various technicalpublications of the field attest to a concern with software and hardware, butemphasis on human functionality and capability is lacking.andP;  Many failures ofinformation systems are attributed to human error rather than to the design.andO;We are going to suffer continued failures until we learn to change ourapproach.andM;One of the first things needed is a change in attitude.andP;  The behavior we callhuman error is just as predictable as system noise, perhaps more so.andO;Therefore, instead of blaming the human who happens to be involved, it wouldbe better to try to identify the system characteristics that led to theincident and then to modify the design either by elimination of the situationor at least minimization of the impact for future events.andP;  One major stepwould be to remove the term &quot;human error&quot; from our vocabulary and tore-evaluate the need to blame individuals.andP;  A second major step would be todevelop design specifications that consider the functionality of the humanwith the same degree of care that has been given to the rest of the system.andM;In the case of the Soviet Mars probe, Science wrote its report as if theincompetence of the human controller had caused the problem.andP;  Roald Kremnev,director of the Soviet Union's spacecraft manufacturing plant, wasinterviewed.andP;  Here is how Science reported the discussion:andM;...what happened to the controller who made the error?andP;  Well, Kremnev toldScience with a dour expression, he did not go to jail or to Siberia.andP;  Infact, it was he who eventually tracked down the error in the code.andO;Nonetheless, said Kremnev, &quot;he was not able to participate in the lateroperation of Phobos&quot; [7].andM;Both the reporter's question and the answer presuppose the notion of blame.andO;Even though the operator tracked down the error, he was still punished (butat least not exiled).andP;  But what about the designers of the language andsoftware or the methods they use?andP;  Not mentioned.andP;  The problem with thisattitude is that it prevents us from learning from the incident and allowsthe error-prone situation to remain.andM;Stories of related failures of computer systems due to &quot;human error&quot; are easyto find in every industry: nuclear power, aviation, business, the stockmarket, and of course, the computer industry itself.andP;  In the august 1989issue of Communications of the ACM, the following item appeared in thesection News Track:andM;A computer operator at Exxon's Houston headquarters has been fired forinadvertently destroying computer copies of thousands of documents withpotentially important information relating to the Alaskan oil spill ...andP;  Theex-employee, however, says he is being used as a scapegoat and that none ofthe tapes he erased were labeled &quot;Do Not Destroy.&quot;andM;The information provided about this incident is too sparse to form aconclusion, but if the system had been designed with the characteristics ofhuman operators in mind, the preservation of tapes would not depend upon theexistence of a simple (human-generated?) label: &quot;Do Not Destroy.&quot;andP;  Thus,either the incident would not have happened, or the excuse would not havebeen plausible.andM;Perhaps it is time for ACM to take the lead in this matter for the design ofcomputational systems.andP;  There is considerable expertise among its members,including the Committee on Computers and Public Policy and one specialinterest group devoted to related issues (SIGCHI, the Special Interest Groupon Computer-Human Interaction).andM;There is also a convenient place to start.andP;  On the electronic computernetworks, Peter Neumann modernates the valuable forum on risks to the publicin computers and related systems, labeled as an activity of the ACM Committeeon Computers and Public Policy.andP;  This RISKS forum collects, reports, andcomments on incidents that include human error and design, but these are notsufficiently precise or authoritative to be used for professional advancementof the field.andP;  The sources of the information are often reports in the mediathat are incomplete, usually written before all relevant information isgathered, and subject to other sources of inaccuracies and biases.andP;  (Theitems from Science and Communications' News Track that I cited exhibit allthese sources of unreliability.)andP;  There is a lot of potential benefit to begained through the careful study of design failures.andP;  Namely, otherdisciplines have learned to benefit through such careful review and analysis[6].andP;  In reviewing the cases presented in the RISKS forum, why not use themas guides to better design?andM;There are several existing systems used in other industries that couldprovide a model.andP;  One major source of valuable advice in the aviationcommunity is a collection of incidents known as ASRS, the Aviation SafetyReporting System, run by NASA-Ames with a computer-readable databaseadministered by Battelle.andP;  Here, people in the aviation community who witnessor commit errors or other related problems write a description of theincident and their interpretation and mail them to ASRS.andP;  The ASRSinvestigators may call back to check accuracy or get more information, butonce the information has been confirmed and clarified, the part of the formthat contains the submitter's identification is returned to that individual.andO;ASRS also removes all identifying information to make it impossible for theparticular submitter or incident to be determined.andP;  This anonymity iscritical to the accuracy and completeness of the database.andP;  Because NASA hasno regulatory power and has a good record for keeping the sourcesconfidential, this database has become trusted by the aviation community.andO;People are now willing to describe their own actions if they believe thereport will be useful for the improvement of aviation safety.andP;  A variablenumber of improvements in cockpit and other parts of aircraft x design havebeen made by designers who have studied the variable patterns of errors thatcan be seen in this database.andM;A critical aspect of the ASRS system is that the reports are not seen by anysupervisors of the submitters.andP;  Similar attempts in other industries havefailed because their reports were submitted through a chain of authority thatincluded the person's supervisor or plant management personnel--people whohave biases to sanitize the report or to form negative judgements of thereporter.andP;  Thus, the incident reporting system for the nuclear industry isnot an impartial guide to actual operating practices.andP;  Anonymity andself-reporting have worked well, along with a system of verification andclarification such as is performed by the NASA/ASRS team (mostly composed ofretired aviation professionals).andM;In similar fashion, the United States National Transportation Safety Board(NTSB) performs a detailed analysis of transportation accidents (aviation,highway, marine, railroad, and pipeline).andP;  These reports are extremelyvaluable and are a major force in the improvement of safety in the relevantindustries.andP;  (The NTSB reports are, by statute, not allowed to be used inlegal proceedings to determine culpability for an event.andP;  This kind ofprotection is essential in today's litigious society to allow theinvestigation to proceed without fear that the results will be misinterpretedor misused.)andM;Should ACM sponsor similar initiatives?andP;  I do not know, for its issues aredifferent from those faced by other industries.andP;  But I propose that ACMinvestigate the possible ways of improving this part of the profession.andP;  ACMcould take the lead in establishing some positive and constructive actions toelevate the human side of computing to a level of concern and respectabilityequal to that of the physical and symbolic side.andM;REFERENCESandM;[1] Helander, M., Ed.andP;  Handbook of Human-Computer Interaction.andO;North-Holland, New York, 1988.andM;[2] Leveson, N.G.andP;  Software safety: Why, what, and how.andP;  ACM Comput.andP;  Surv.andO;18, 2 (June 1986), 125-163.andM;[3] Norman, D.A.andP;  The Psychology of Everyday Things.andP;  Basic Books, New York,1988.andM;[4] Norman, D.A.andP;  Design rules based on analyses of human error.andP;  Commun.andO;ACM 26, 4 (April 1983), 254-258.andM;[5] Perrow, C.andP;  Normal Accidents.andP;  Basic Books, New York, 1984.andM;[6] Petroski, H.andP;  To Engineer is Human: The Role of Failure in SuccessfulDesign.andP;  St. Martin's Press, New York, 1985.andM;[7] Waldrop, M.M.andP;  Phobos at Mars: A dramatic view--and then failure.andP;  Sci.,andO;245 (1989), 1044-45.andM;Donald A. Norman is professor and chair of the Department of CognitiveScience at the University of California, San Diego.andO;</TEXT></DOC>