<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-752-730  </DOCNO><DOCID>08 752 730.andM;</DOCID><JOURNAL>Communications of the ACM  August 1990 v33 n8 p103(8)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>A bridging model for parallel computation. (parallel alternativeto von Neumann model of sequential computation) (technical)</TITLE><AUTHOR>Valiant, Leslie G.andM;</AUTHOR><SUMMARY>A 'bridging' model between software and hardware for parallelcomputation that is analogous to the von Neumann model ofsequential computation is presented.andP;  The bulk synchronousparallel (BSP) model combines several components, each of whichperforms processing and/or memory functions: 'routers' thatdeliver messages point to point between pairs of components, andfacilities for synchronizing components at regular time intervals.andO;The synchronization facility can be switched on or off for anysubset of components.andP;  A set of tightly synchronized algorithms isdescribed.andP;  BSP can be implemented on packet switching networks aswell as on optical crossbar designs.andM;</SUMMARY><DESCRIPT>Topic:     Parallel ProcessingSequential ProcessingModeling of Computer SystemsNew TechniqueTechnology.andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>A Bridging Model for Parallel ComputationandM;In a conventional sequential computer, processing is channelled through onephysical location.andP;  In a parallel machine, processing can occursimultaneously at many locations and consequently many more computationaloperations per second should be achievable.andP;  Due to the rapidly decreasingcost of processing, memory, and communication, it has appeared inevitable forat least two decades that parallel machines will eventually displacesequential ones in computationally intensive domains.andP;  This, however, has notyet happened.andP;  In order to have a chance of rectifying this situation it isnecessary to identify what is missing in our understanding of parallelcomputation that is present in the sequential case, making possible a hugeand diverse industry.andM;We take the view that the enabling ingredient in sequential computation is acentral unifying model, namely the von Neumann computer.andP;  Even with rapidlychanging technology and architectural ideas, hardware designers can stillshare the common goal of realizing efficient von Neumann machines, withouthaving to be too concerned about the software that is going to be executed.andO;Similarly, the software industry in all its diversity can aim to writeprograms that can be executed efficiently on this model, without explicitconsideration of the hardware.andP;  Thus, the von Neumann model is the connectingbridge that enables programs from the diverse and chaotic world of softwareto run efficiently on machines from the diverse and chaotic world ofhardware.andM;Our claim is that what is required before general purpose parallelcomputation can succeed is the adoption of an analogous unifying bridgingmodel for parallel computation.andP;  A major purpose of such a model is simply toact as a standard on which people can agree.andP;  In order to succeed in thisrole, however, the model has to satisfy some stringent quantitativerequirements, exactly as does the von Neumann model.andP;  Despite the clearbenefits that might flow from the adoption of a bridging model, relativelylittle effort appears to have been invested in discovering one.andP;  Some veryrelevant issues but in a slightly different context are discussed by Snyderin [23].andM;In this article we introduce the bulk-synchronous parallel (BSP) model andprovide evidence that it is a viable candidate for the role of bridgingmodel.andP;  It is intended neither as a hardware nor programming model, butsomething in between.andP;  In justifying the BSP for this role, our main argumentis that when mapping high-level programs to actual machines in a greatvariety of contexts, little efficiency is lost if we utilize this singlemodel.andP;  The adoption of such a standard can be expected to insulate softwareand hardware development from one another and make possible both generalpurpose machines and transportable software.andM;The quantitative arguments for the model are mainly efficient universalityresults.andP;  In three sections, efficient implementations on the model ofhigh-level language features and algorithms are discussed.andP;  In two others,implementations of the model in hardware are described.andP;  In all cases, we aimto achieve optimal simulations, meaning the time taken is optimal to withinconstant multiplicative factors which are independent of the number ofprocessors and usually small.andP;  We wish to avoid logarithmic losses inefficiency.andP;  Although we express the results asymptotically, we regard themodel as neutral about the number of processors, be it two or one million.andO;This is justified whenever the constants are indeed small.andM;Since the difficulties of programming present severe potential obstacles toparallel computing, it is important to give the programmer the option toavoid the onerous burdens of managing memory, assigning communication andperforming low-level synchronization.andP;  A major feature of the BSP model isthat it provides this option with optimal efficiency (i.e., within constantfactors) provided the programmer writes programs with sufficient parallelslackness.andP;  This means programs are written for v virtual parallel processorsto run on p physical processors where v is rather larger than p (e.g., v=plog p).andP;  The slack is exploited by the compiler to schedule and pipelinecomputation and communication efficiently.andP;  The high-level languages thatcould be compiled in this mode could allow a virtual shared address space.andO;The program would have to be so expressed that v parallel instruction streamscould be compiled from it.andP;  While a PRAM language [6, 11] would be ideal,other styles also may be appropriate.andM;We note that in a general-purpose setting some slack may be unavoidable ifparallel programs are to be compiled efficiently.andP;  Certainly, the prospectsfor compiling sequential code into parallel code, which is the extremeopposite case of v=1, look bleak.andP;  The intermediate case of p=v looksunpromising also if we are aiming for optimality.andP;  Hence the disciplineimplied, that of using fewer processors than the degree of parallelismavailable in the program, appears to be an acceptable general approach tocomputation-intensive problems.andP;  The importance of slack has been emphasizedearlier in [12, 27].andM;It is worth pointing out that while these automatic memory and communicationmanagement techniques are available, the model does not make their useobligatory.andP;  For the purpose of reducing the amount of slack required,improving constant factors in runtime, or avoiding hashing (as used by theautomatic memory management scheme), the programmer may choose to keepcontrol of these tasks.andP;  We shall give some illustrative examples ofbulk-synchronous algorithms that are appropriate for this model.andM;It is striking that despite the breadth of relevant research in recent years,no substantial impediments to general-purpose parallel computation as weinterpret it here have been uncovered.andP;  This contrasts with noncomputabilityand NP-completeness results that explain the intractability of numerous othercomputational endeavors that had been pursued.andP;  Many of the results that havebeen obtained, and to which we shall refer here in justification of the BSPmodel, are efficient universality results in the style of Turing's theoremabout universal machines [24].andP;  Hence, the BSP model can be viewed as apragmatic embodiment of these positive results much as the von Neumann modelis a pragmatic embodiment of Turing's theorem.andM;The BSP ModelandM;The BSP model of parallel computation or a bulk-synchronous parallel computer(BSPC) is defined as the combination of three attributes:andM;1.andP;  A number of components, each performing processing and/or memoryfunctions;andM;2.andP;  A router that delivers messages point to point between pairs ofcomponents; andandM;3.andP;  Facilities for synchronizing all or a subset of the components at regularintevals of L time units where L is the periodicity parameter.andP;  A computationconsists of a sequence of supersteps.andP;  In each superstep, each component isallocated a task consisting of some combination of local computation steps,message transmissions and (implicitly) message arrivals from othercomponents.andP;  After each period of L time units, a global check is made todetermine whether the superstep has been completed by all the components.andP;  Ifit, has, the machine proceeds to the next superstep.andP;  Otherwise, the nextperiod of L units is allocated to the unfinished superstep.andM;The definition is chosen to embody the simplest capabilities that suffice forour purposes.andP;  In separating the components from the router, we emphasizethat the tasks of computation and communication can be separated.andP;  Thefunction of the router is to deliver messages point to point.andP;  It isspecifically intended fro implementing storage accesses between distinctcomponents.andP;  It assumes no combining, duplicating or broadcasting facilities.andO;Similarly, the synchronization mechanism described captures in a simple waythe idea of global synchronization at a controllable level of coarseness.andO;Realizing this in hardware provides an efficient way of implementing tightlysynchronized parallel algorithms, among others, without overburdening theprogrammer.andP;  We note that there exist alternative synchronization mechanimsthat could have been substituted which achieve the same purpose.andP;  Forexample, the system could continously check whether the current superstep iscompleted, and allow it to proceed to the next superstep as soon ascompletion is detected.andP;  Provided a minimum amount of L time units for thischeck is charged, the results of the run-time analysis will not change bymore than small constant factors.andM;The synchronization mechanism can be switched off for any subset of thecomponents; sequential processes that are independent of the results ofprocesses at other components should not be slowed down unnecessarily.andP;  Whensynchronization is switched off at a processor it can proceed without havingto wait for the completion of processes in the router or in other components.andO;Also, operations local to the processor will not automatically slow downcomputations elsewhere.andP;  On the other hand, even when this mechanism isswitched off, a processor can still send and receive messages, and use thisalternative method for synchronization.andP;  If performance guarantees areexpected of this alternative synchronization mechanism, assumptions have tobe made about the router; for example, it might be assumed that each messageis delivered within a certain expected amount of time of being sent.andP;  Injustifying the BSP model, we use the barrier-style synchronization mechanismalone and make no assumptions about the relative delivery times of themessages within a superstep.andP;  In the simulations, local operations arecarried out only on data locally available before the start of the currentsuperstep.andM;The value of the periodicity L may be controlled by the program, even atruntime.andP;  The choice of its value is constrained in opposite directions byhardware and software considerations.andP;  Clearly, the hardware sets lowerbounds on how small L can be.andP;  The software, on the other hand, sets upperbounds on L since the larger it is, the larger the granularity of parallelismthat has to be exhibited by the program.andP;  This is because, to achieve optimalprocessor utilization, in each superstep each processor has to be assigned atask of approximately L steps that can proceed withough waiting for resultsfrom other processors.andP;  We note that along with the tension between these twofactors, there is also the pnenomenon that a small L, while algorithmicallybeneficial in general, may not yield any further advantages below a certainvalue.andM;In analyzing the performance of a BSP computer, we assume that in one timeunit an operation can be computed by a processing component on data availablein memory local to it.andP;  the basic task of the router is to realize arbitraryh-relations, or, in other words, supersteps in which each component sends andis sent at most h messages.andP;  We have in mind a charge of gh + s time unitsfor realizing such an h-relation.andP;  Here g defines the basic throughput of therouter when in continuous use and s the latency or startup cost.andP;  Since ourinterest pertains only to optimal simulations, we will always assume that his large enought that gh is at least of comparable magnitude to s.andP;  If gh [isgrater than or equal to] s, for example, and we let g = 2g, then we cansimply charge gh time units for an h-relation and this will be anoverestimate (by a factor of at most two).andP;  In this article we shall,therefore, define g to be such that h-relations can be realized in time ghfor h larger than some ho.andP;  This g can be regarded as the ratio of the numberof local computational operations performed per second by all the processors,to the total number of data words delivered per second by the router.andP;  Notethat if L [is greater than or or equal to] gho then every h-relation for h[is less than]-ho will be charged as an ho-relation.andM;Even in a fixed technology we think of the parameter g as being controllable,within limits, in the router design.andP;  It can be kept low by using morepipelining or by having wider communication channels.andP;  Keeping g low or fixedas the machine size p increases incurs, of course, extra costs.andP;  Inparticular, as the machine scales up, the hardware investment forcommunication needs to grow faster than that for computation.andP;  Our thesis isthat if these costs are paid, machines of a new level of efficiency andprogrammability can be attained.andM;We note that the von Neumann model as generally understood leaves many designchoices open.andP;  Implementations incorporating some additions, such as memoryhierarchies, do not necessarily become inconsistent with the model.andP;  In asimilar spirit we have left many options in the BSP computer open.andP;  We allowfor both single and multiple instruction streams.andP;  While it will beconvenient in this article to assume that each component consists of asequential von Neumannprocessor attached to a block of local memory, we donot exclude other arrangements.andP;  Also, we can envisage implementations of theBSP model that incorporate fetures for communications, computation orsynchronization that are clearly additional to the ones in the definition butstill do not violate its spirit.andM;A formalization of perhaps the simplest instance of the BSP model isdescribed in [29] where it is called an XPRAM.andP;  A fuller account of thesimulation results as well as of their proofs can be found there.andM;Automatic MemoryandM;ManagementandM;on the BSPCandM;High-level languages enable the programmer to refer to a memory location usedin a program by a symbolic address rather than by the physical address atwhich it is stored.andP;  For sequential machines, conventional compilertechniques are sufficient to generate efficient machine code from thehigh-level description.andP;  In the parallel case, where many accesses are madesimultaneously and the memory is distributed over many components, newproblems arise.andP;  In particular, there is the primary problem of allocatingstorage in such a way that the computation will not be slowed down by memoryaccesses being made unevenly and overloading individual units.andM;The most promising method known for distributing memory accesses aboutequally in arbitrary programs is hashing.andP;  The motivating idea is that if thememory words are distributed among the memory units randomly, independentlyof the program, then the accesses to the various units should be aboutequally frequent.andP;  Since, however, the mapping from the sumbolic addresses tothe physical addresses has to be efficiently computable, the description ofthe mapping has to be small.andP;  This necessitates that a pseudo-random mappingor hash function be used instead of a true random mapping.andP;  Hash functionsfor this parallel context have been proposed and analyzed by Mehlhorn andVishkin [17] (see also [5, 10]).andP;  These authors have suggested an elegantclass of functions with some provably desirable properties: the class ofpolynomials of degree 0(log p) in arithmetic modulo m, where p is the numberof processors and m the total number of words in the memory space.andM;In this section it is observed that for hashing to succeed in parallelalgorithms running at optimal efficiency some parallel slack is necessary,and a moderate amount is sufficient if g can be regarded as a constant.andM;To see necessity we note that if only p accesses are made in a superstep to pcomponents at random, there is a high probability that one component will getabout log p/log log p accesses, and some will get none.andP;  Hence, the machinewill have to devote [Omega](log p/log log p) time units to this rather thanjust a constant, which would be necessary for optimal throughput.andP;  Logarithmsto the base two are used here, as throughout this article.andM;The positive side is that if slightly more, namely p log p, random accessesare made in a superstep, there is a high probability that each component willget not more than about 3 log p which is only three times the expectednumber.andP;  Hence, these accesses could be implemented by the router in theoptimal bound of 0(log p).andP;  More generally, if pf(b) accesses are maderandomly for any function f(p) growing faster than log p, the worst-caseaccess will exceed the average rate by even smaller factors.andM;This phenomenon can be exploited as follows.andP;  Suppose that each of the pcomponents of the BSP computer consists of a memory and a processor.andP;  We makeit simulate a parallel program with v [is greater than or equal to] p log pvirtual processors by allocation v/p [is greater than or equal to] log p ofthem to each physical processor.andP;  In a superstep, BSP machine simulates onestep of each virtual processor.andP;  Then the v memory requests will be spreadevenly, about v/p per processor, and hence the machine will be able toexecute this superstep in optimal 0(v/p) time with high probability.andP;  Thisanalysis assumes, of course, that the v requests are to distinct memorylocations.andP;  The more general case of concurrent accesses will be consideredin the next section.andP;  To keep the constants low the machines has to beefficient both in hashing and in context switching.andM;The conclusion is that if hashing is to be exploited efficiently, theperiodicity L may as well be at least logarithmic, and if it is logarithmic,optimality can be achieved.andP;  Furthermore, for the latter, known hashfunctions fuffice (see [29]).andP;  In making this claim we are charging constanttime for the overheads of evaluating the hash function even at runtime.andP;  Injustifying this, we can take the view that evaluating the hash function canbe done very locally and hence quickly.andP;  (The O(log log p) parallel stepsneeded to evaluate the log p degree polynomials may then be regarded as beinga constant.)andP;  Alternatively, we can hypothesize that hash functions that areeasier to compute exist.andP;  For example, some positive analytic results arereported for constant degree polynomials in [1] and [22].andP;  Indeed, currentlythere is no evidence to suggest that linear polynomials do not suffice.andO;Besides ease of computation these have the additional advantage of mappingthe memory space one-to-one.andP;  Finally, we note that the frequency ofevaluating the addresses most often used can be reduced in practice bystoring these addresses in tables.andM;Concurrent MemoryandM;Accesses on the BSPCandM;In the previous section we considered memory allocation in cases in whichsimultaneous accesses to the same memory location are not allowed.andP;  Inpractice it is often convenient in parallel programs to allow severalprocessors to read from a location or to write to a location (if there issome convention for resolving inconsistencies), and to allow broadcasting ofinformation from one to all other processors.andP;  A formal shared memory modelallowing arbitrary patterns of simultaneous accesses is the concurrent readconcurrent write (CRCW) PRAM (see [11]).andM;One appraoch to implementing concurrent memory accesses is by combiningnetworks, networks that can combine and replicate messages in addition todelivering them point-to-pont [8,20].andP;  In the BSP model, it is necessary toperform and charge for all the replicating and combining as processingoperations at the components.andP;  It turns out, however, that even the mostgeneral model, the CRCW PRAM, can be simulated optimally on the BSP modelgiven sufficient slack if g is regarded as a constant.andP;  In particular, it isshown in [28] that if v = [p.sup.1+[Sigma]] for any [Sigma] [is greater than]O, a v processor CRCW PRAM can be simulated on a p-processor BSP machine withL [is greater than or equal to] log p in time O(v/p) (where the constantmultiplier grows as [Sigma] diminishes).andP;  The simulation uses a methodforsorting integers in parallel due to Rajasekaran and Reif[19] and employed ina similare context to ours by Kruskal et al.andP;  [12].andP;  Sorting is one of thebasic techniques known for simulating concurrent accesses [4].andP;  Since generalsorting has non-linear complexity we need to limit the domain, in this caseto integers, to have some chance of an optimal simulation.andM;The general simulation discussed above introduces constants that are betteravoided where possible.andP;  Fortunately, in many frequently occuring situationsmuch simpler solutions exist.andP;  For example, suppose that we are simulating vvirtual processors on a p-processor BSP computer and know that at any instantat most h accesses are made to any one location.andP;  The if v = [Omega](hplogp), concurrent accesses can be simulated optimally by simply replicating anydata items that are to be sent to r locations r times at the source processor(and charging for their transmission as for r messages).andP;  Similarly, if anycombining occurs, it does so at the target processor.andM;To show this works, we suppose that among the destination addresses of the vaccesses made simultaneously there are t distinct ones, and the numbers goingto them are [l.sub.1], ..., [l.sub.t] respectively, all at most h.andP;  Supposethese are scattered randomly and independently among p memory units.andP;  Thenthe probability that a fixed unit receives more than x accesses is theprobability that the sum of t independent random variables [n.sub.j] ~1 [isless tan or equal to] j [is greater than or equal to] t]), each taking value[l.sub.j] with probability [p.sup.-1] and value O otherwise, has value morethan x.andP;  But a corollary of a result of Hoeffding [9] (see [15]) is that if[[xi].sub.j] are independent random variables 0 [is less than or equal to][[xi].sub.j] [is less than or equal to] 1 with expectation [c.sub.j] (j = 1,..., t) and [mu] is the mean of {[c.sub.j]} then for a andless; min([mu], 1 - [mu]).andM;Prob [Mathematical Expression Omitted]andM;If we set [[xi].sub.i] = [n.sub.i]/h so that [mu] = [[Sigma]l.sub.j]/(pht) =v/(pht), and let a = [mu], then the probability of 2[mu]t being exceeded ismost [e.sup.-at/3] = [e.sup.-v/3ph] [is less than or equal to] [p.sup.-v] ifv [is greater than or equal to] [3[Upsilon]hplog.sub.e.p].andP;  Hence, theprobability that among the p processors at least one receives more than twicethe expected number of accesses is at most p times this quantity, or[p.sup.1-v].andP;  Hence [Upsilon] andgt; 1 suffices to ensure optimality to withinconstant factors.andM;We also observe that several other global operations, such as broadcasting orthe parallel prefix, that one might wish to have, are best implementeddirectly in hardware rather than through general simulations.andP;  The simulationresult does imply, however, that for programs with sufficient slack theseextra features provide only constant factor improvements asymptotically.andM;BSP AlgorithmsandM;without HashingandM;Although the potential for automating memory and communciation management viahashing is a major advantage of the model, the programmer may wish to retaincontrol of these functions in order to improve performance or reduce theamount of slack required in programming.andP;  It appears that manu computationalproblems, simply and natural assignments of memory and communication sufficefor optimal implementations on the BSP model.andP;  Fox and Walker (see [30]) havesuggested a portable programming environment based on a very similarobservation.andP;  A systematic study of such bulk-synchronous algorithms remainsto be done.andP;  We can, however, give some examples.andP;  We note that several othermodels of computation have been suggested -- mostly on shared memory models-- that allow for the extra costs of communication explicitly in some way.andO;Several algorithms developed for these work equally well on the BSPC.andP;  Amongsuch related models are the phase PRAM of Gibbons [7], which incorporatesbarrier sunchronization in a similar way to ours, but uses a shared memory.andO;Others include the delay model of Paoadimitriou and Yannakakis [18], and theLPRAM of Aggarwal et al.andP;  [|].andM;The algorithms described below are all tigtly synchronized in the sense thatthe runtime of their constituent subtasks can be predicted before runtime.andO;There is also a context for parallelism where many tasks are to be executedwith varying time requirements that cannot be determined in advance.andP;  In themost extreme case, one has a number of subtasks whose runtime cannot bepredicted at all.andP;  In this general dynamic load-balancing situation therealso exist phenomena that are compatible with barrier synchronization.andP;  Inparticular Karp has given a load-balancing algorithm that is optimal for anyL for the model of Gibbons (see [7]).andM;The advantages of implementing algorithms directly on the BSP model (ratherthan compiling them automatically), increase as the bandwidth parameter gincreases.andP;  Hence, it is appropriate to consider g explicitly in analyzingthe performance of these algorithms.andP;  An algorithm in this model will bebroken down into supersteps where the words read in each superstep are alllast modified in a previous superstep.andP;  In a superstep of periodicity L, Llocal oporations and a [L/g] -relation message pattern can be realized.andP;  Theparameters of the machine are therefore L, g and p the number of processors.andO;Each algorithm also has as a parameter n, the size of the problem instance.andO;The complexity of an algorithm can be expressed in several ways in terms ofthese parameters.andP;  We will describe parallel algorithms in which thetime-processor product exceeds the number of computational operations by onlya fixed multiplicative constant, independent of L, g, p and n, provided thatL and g are below certain critical values.andP;  In such &quot;optimal&quot; algorithmsthere may still be several directions of possible improvements, namely in themultiplicative constant as well as in the critical values of g and L.andM;As a simple example of a tightly synchronized algorithm well suited fordirect implementation, consider multiplying two n X n matrices A and B usingthe standard algorithm on p [is less than or equal to] [n.sup.2] processors.andO;Suppose we assign to each processor the subproblem of computing an n/[squareroot of p] x n/[square root of p] submatrix of the product.andP;  Then eachprocessor has to receive data describing n/[square root of p] rows of A andN/[square root of p] columns of B.andP;  Hence, each processor has to perform[2n.sup.3]/p additions and multiplications and receive [2n.sup.2]/[squareroot of p] [is less than or equal to] [2n.sup.3]/p messages.andP;  Clearly, if inaddition each processor makes [2n.sup.2]/[square root of p] messagetransmissions, the runtime is affected by only a constant factor.andO;Fortunately, no more than this number of transmissions is required even ifthe elements are simply replicated at source.andP;  This is because if thematrices A and B are initially distributed uniformly amongn the p processors,[2n.sup.2]/p elements in each, and each processor replicates each of itselements [square root of p] times and sends them to the [square root of p]processors that need these entries, the number of transmissions per processorwill indeed be this [2n.sup.2]/[square root of p].andP;  This is an instance ofthe point made in the previous section, that concurrent accesses, when theaccess multiplicity h is suitably small, may be implemented efficiently bysimply replicating data at the source.andP;  It is easily seen that optimalruntime O([n.sup.3]/p) is achieved provided g = O([/[square root of p]) and L= O([n.sup.3]/p).andP;  (An alternative algorithm given in [1] that requires fewermessages altogether can be implemented to give optimal runtime with g aslarge as O(n/[p.sup.1/3]) but L slightly smaller at O([n.sup.3]/(p log n)).)andM;A case in which it would be inefficient to realize multiple accesses byreplication at the source is broadcasting.andP;  Here, one processor needs to sendcopies of a message to each of n memory locations spread uniformly among pcomponents.andP;  Sending one copy to each of the p components can be accomplishedin [log.sub.d]p supersteps by executing a logical d-ary tree.andP;  In eachsuperstep, each processor involved transmits d copies to distinct components.andO;Time [dglog.sub.d.p] is required for this.andP;  If n/p-1 further copies are madeat each component, optimality (i.e., runtime O(n/p)) can be achieved if d =O((n/(gp log p)) log (n/(gp log p))) and L = O(gd).andP;  The constraint on dclearly implies that n = [Omega](gp log p).andP;  Examples of these constraintsare g = 1, in which case n = p log p and L = O(1) are sufficient, and g = logp, in which case n = pp(log p).sup.2] and L = O(log p) suffice.andM;An operation more powerful than broadcasting is parallel prefix [11, 13].andO;Given [x.sub.1], ...andP;  [x.sub.n], one needs to compute [x.sub.1.o] [x.sub.2.o]...andP;  o[x.sub.1] for 1 [is less than or equal to] i  is less than or equal to]n for some associative operation o.andP;  The now-standard recursive algorithm forthis, but with d-ary rather than binary recursion, yields exactly the sameconstraints as those obtained above for broadcasting.andM;There are several important algorithms such as the fast Fourier transformthat can be implemented directly on the butterfly graph.andP;  As observed in [8],an instance of such a graph with n inputs can be divided into (log n)/log dsuccessive layers, where each layer consists of (n log d)/d independentbutterfly grpahs of d/log d inputs each.andP;  This is true for any d [is greaterthan or equal to] 2 if the expressions are rounded to integers appropriately.andO;We can, therefore, evaluate such a graph on p = (n log d)/d processors in(log n)/log d supersteps, in each of which each processor computes d localoperations and sends and receives d/log d messages.andP;  Hence, optimality can beachieved if g = O(log d) = O(log(n/p)), and L [is less than or equal to] d =O((n/p) log (n/p)).andM;A further problem for which bulksynchronous algorithms are of interest issorting.andP;  Among known algorithms that are well suited is Leighton'scolumnsort [14].andP;  For sorting n items on p = (O([n.sup.1/3]) processors itexecutes eight consecutive stages.andP;  In the odd-numbered stages each processorsorts a set of n/p elements sequentially.andP;  In the even-numbered stages, thedata is permuted among the processors in a certain regular pattern.andP;  Hence,computation and communication are separated at the coarsest scale.andP;  Foroptimal runtime on the BSP model, the communication time O(gn/p) must notexceed the computation time of (n/p) log (n/p) which is required by eachstage of sequential comparison sorting.andP;  Hence, g = O(log(n/p)) and L =O((n/p) log (n/p)) suffice.andM;More generally, it is clear that any actual BSP machine would impose an upperbound on p, the number of processors, as well as a lower bound on the valueof g that can be achieved.andP;  Also, for any g to be achieved, a lower bound onL may be implied.andP;  One can, therefore, imagine transportable BSP software tobe written in a way that the code compiled depends not only on the problemsize n but also on the parameters p, g and L.andM;ImplementationandM;on PacketandM;Switching NetworksandM;The communication medium or router of the BSP model is defined to be thesimplest possible with the goal that it can be implemented efficiently invarious competing technologies.andP;  In current parallel machines, the favoredmethod of communication is via networks that do some kind of packetswitching.andP;  Therefore, our main argument will refer to this.andP;  In implementingthe BSP model on a packet switching network, the main tool available is thatof pipelining communication.andP;  The conclusion will be that a network such as ahypercube will suffice for optimality to within constant factors, but only ifits communication bandwidth is balanced with its computational capability.andO;Thus, to simulate the BSP model with bandwidth factor g it is necessary thatthe computational bandwidth of a node does not exceed the communicationbandwidth of the connection between a pair of nodes adjacent in the networkby more than a factor of g.andM;Packet routing on regular networks has received considerable attention.andO;Consider a hypercube network and suppose that in g units of time a packet cantraverse one edge of it.andP;  Thus, a single packet will typically take g log ptime to go to an arbitrary destination.andP;  A paradigmatic case of parallelpacket routing is that of routing permutations.andP;  Here each of the pprocessors wishes to send a message to a distinct destination.andP;  What isrequired is a distributed routing algorithm that needs no global knowledge ofthe message pattern, and ensures that all the packets arrive fast, even whenfully allowing for contention at the edges.andP;  It turns out that a simpletwo-phase randomized routing algorithm [26, 29] suffices to give runtime ofabout 2g log p with overwhelming probability.andM;While this is optimal for permutation routing, it does not imply optimal BSPsimulations immediately since it corresponds to the case of 1-relations andwould require a factor of at least log p more in communication compared withcomputation time.andM;In order to obtain an optimal BSP simulation, we need to use the fact thattwo-phase randomized routing can support heavier message densities.andP;  It turnsout that if there are log p packets initially at each node with at most log pdestined to any one target, O(g log p) time still suffices for all the p logp packets to reach their destinations [25, 29].andP;  In other words, logp-relations can be realized essentially as fast as 1-relations.andP;  This givesan optimal simulation of a BSP machine with L [is greater than] g log p sincein each superstep we need to simulate L local operations at each processorand realize an L/g-relation in the router.andP;  All this can be simulated in timeO(L) on the hypercube.andP;  We note that the simulations give small constantfactors and experiments show that small queues suffice.andM;Further details of results on routing can be found in [29].andP;  All theindications are that this problem has a variety of practical and efficientsolutions.andP;  For example, instead of store-and-forward message passing onecould consider bit-streamed or wormhole routing which exhibit similarphenomena [2].andP;  We also note that if the address space is already randomizedby hashing, two-phase routing can be replaced by one-phase deterministicrouting for implementing memory accesses [20].andM;Since the BSP model separates computation from communication, no particularnetwork topology is favored beyond the requirement that a high throughput bedelivered.andP;  An example related to the hypercube that suffices under similarconditions is the butterfly, which would consist of (log p) + 1 levels of pnodes each.andP;  One of the levels would be allocated to processor/memorycomponents and the rest to switches.andM;Implementation onandM;Optical CrossbarsandM;Since we envisage the BSP computer as being realizable in a variety oftechnologies, we conclude here with the observation that it can beimplemented optimally on a simple model of computation suggested by thepossibilities of optical technology.andM;In this model, in each time step each of p components can transmit a messageby directing a beam of light at a chosen component.andP;  If a component receivesjust one message it acknowledges it and transmission is consideredsuccessful.andP;  On the other hand, if more than one beam is directed at a node,none of the messages is successfully received at that node, and the absenceof a valid acknowledgment informs a sender of the failure.andP;  Such a model hasbeen considered in [3, 16].andM;In light of the earlier discussion on simulating shared memory by hashingusing periodicity L [is greater than or equal to] log p, a crucial case forthis optical model is that of a superstep in which each processor sends up tolog p messages, each receives up to about the same number, and there is noother detectable pattern to the requested global communication.andP;  It isobserved in [29] that a randomized algorithm of Anderson and Miller [3] canbe adapted to perform this communication on this optical model in O(log p)time steps, which is optimal.andP;  Therefore, if such a time step corresponds tog time units, this model can simulate a [omega](p log p) BSP computeroptimally.andM;ConclusionandM;We have defined the BSP model and argued that it is a promising candidate asbridging model for general-purpose parallel computation.andP;  As supportingevidence, we have described how a variety of efficiency phenomena can beexploited by this one model.andP;  No single factor is, or can be, decisive inconfirming the adequacy of a bridging model.andP;  It is the diversity of theconsiderations in support of the model and the apparent absence of contraryindications that are here most compelling.andM;The considerations we have analyzed are all concerned with providingguaranteed performance at near-optimal processor utilization.andP;  Since theprimary object of parallel computing is to obtain high throughput, weconsider such quantitative criteria to be critical.andP;  In the spectrum ofimaginable computations we have addressed the end that is most communicationintensive, since this case cannot be evaded in a general-purpose setting.andP;  Wehave been careful, however, to ensure that less-constrained computations, where independent processes can proceed with infrequent communication, arenot penalized.andM;The model is intended to be below the language level and we hope that it iscompatible with a variety of language styles.andP;  Several of our arguments,however, favor programs from which the compiler can efficiently abstract thenecessary number of parallel streams of computation.andP;  Highly synchronizedlanguages written in the PRAM style are clearly compatible.andP;  The model isconsistent, however, with a number of other situations also.andP;  For example, intransaction processing where the length of transactions is statisticallypredictable a random allocation of processors would suffice.andM;The arguments given in this article in support of the BSP model are of threekinds.andP;  First it is argued that if the computational and communicationbandwidth are suitably balanced (i.e., g is a small constant such as one) themodel has a major advantage regarding programmability, as least for programswith sufficient slack.andP;  In that case the memory and communication managementrequired to implement a virtual shared memory can be achieved with only aconstant factor loss in processor utilization.andP;  The constants needed in thesimulations are known to be small, except in the case that concurrentaccesses are made with high levels of concurrency to each of many singlelocations simultaneously.andP;  We note that existing machines have higher valuesof g than is ideal for a BSP computer.andP;  The arguments of this article can beinterpreted as saying that if the relative investment in communicationhardware were suitably increased, machines with a new level ofprogrammability would be obtained.andP;  We note that for certain programs inwhich automatic memory allocations is useful, the effective value of g can bemade smaller than the physical value by exploiting locality and viewing thecomputation at a higher level of granularity.andP;  For example, in finite elementmethods the virtual memory can be regarded as partitioned into segments, eachof which is to be stored in a single memory component.andP;  The number ofcomputation steps per segment may then greatly exceed the number of nonlocalmemory accesses.andM;The second kind of argument given in this article is that several importantalgorithms can be implemented directly on this model.andP;  Such an implementationavoids the overheads of automatic memory management and may exploit therelative advantage in throughput of computation over communication that mayexist.andM;The third argument is that the BSP model can be implemented efficiently in anumber of technologies.andP;  We illustrate this by giving an efficient simulationon a hypercube network as well as on a model suggested by opticalcommunication.andP;  We note, however, that the BSP model is not particularlyassociated with any one technology or topology.andP;  The only requirement on therouter is a certain level of communication throughput, however achieved.andO;Clearly, the promise of optical technologies looks attractive in the BSPcontext.andM;AcknowledgmentandM;The author is grateful to an anonymous referee for several insightfulsuggestions concerning presentation.andM;*The preparation of this paper was supported in part by the National ScienceFoundation under grants DCR-86-00379 and CCR-89-02500.andP;  A preliminary versionappeared as reference [28].andM;Permission to copy without fee all or part of this material is grantedprovided that the copies are not made or distributed for direct commercialadvantage, the ACM copyright notice and the title of the publication and itsdate appear, and notice is given that copying is by permission of theAsociation for Computing Machinery.andP;  To copy otherwise, or to republish,requires a fee and/or specific permission.andM;ReferencesandM;[1] Aggarwal, A., Chandra, A., and Snir, M. Communication complexity ofPRAMs.andP;  Theor.andP;  Comput.andP;  Sci.andP;  To be published.andM;[2] Aiello, B., Leighton, F.T., Maggs, B., and Neumann, M. Fast algorithmsfor bit-serial routing on a hypercube.andP;  Manuscript, 1990.andM;[3] Anderson, R.J.andP;  and Miller, G.L.andP;  Optical communication for pointer basedalgorithms.andP;  Tech.andP;  Rep.andP;  CRI 88-14, Computer Science Dept., Univ.andP;  ofSouthern California, 1988.andM;[4] Borodin, A. and Hopcroft, J.E.andP;  Routing merging and sorting on parallelmodels of computation.andP;  J. Comput.andP;  Syst.andP;  Sci 30 (1985) 130-145.andM;[5] Carter, J.L.andP;  and Wegman, M.N.andP;  Universal classes of hash functions.andP;  J.andO;Comput.andP;  Syst.andP;  Sci.andP;  18 (1979) 143-154.andM;[6] Eppstein, D. and Galil, Z. Parallel algorithmic techniques forcombinatorial computation.andP;  Annu.andP;  Rev.andP;  Comput.andP;  Sci.andP;  3 (1988) 233-83.andM;[7] Gibbons, P.B.andP;  A more practical PRAM model.andP;  In Proceedings of the 1989ACM Symposium on Parallel Algorithms and Architectures.andP;  (1989) pp.andP;  158-168.andM;[8] Gottlieb, A. et al.andP;  The NYU ultracomputer--Designing an MIMD sharedmemory parallel computer.andP;  IEEE Trans.andP;  Comput.andP;  32, 2 (1983) 175-189.andM;[9]  Hoeffding, W.andP;  Probability inequalities for sums of bounded randomvariables.andP;  Am.andP;  Stat.andP;  Assoc.andP;  J. 58 (1963) 13-30.andM;[10] Karlin, A. and Upfal, E.andP;  Parallel hashing-An efficient implementationof shared memory.andP;  J. ACM 35, 4 (1988) 876-892.andM;[11] Karp, R.M.andP;  and Ramachandran, V.andP;  A survey of parallel algorithms forshared-memory machines.andP;  In Handbook of Theoretical Computer Science, J. vanLeeuwen, Ed.andP;  North Holland, Amsterdam, 1990.andM;[12] Kruskal, C.P., Rudolph, L., and Snir, M.andP;  A complexity theory ofefficient parallel algorithms.andP;  Theor.andP;  Comput.andP;  Sci.andP;  To be published.andM;[13] Ladner, R.E.andP;  and Fischer, M.J.andP;  Parallel prefix computation.andP;  J. ACM 27(1980) 831-838.andM;[14] Leighton, F.T.andP;  Tight bounds on the complexity of sorting.andP;  IEEE Trans.andO;Comput.andP;  C-34, 4 (1985) 344-354.andM;[15] Littlestone, N. From on-line to batch learning.andP;  COLT 89, MorganKaufmann, San Mateo, CA., (1989) 269-284.andM;[16] Maniloff, E.S., Johnson, K.M., and Reif, J.H.andP;  Holographic routingnetwork for parallel processing machines.andP;  Society of Photo OpticalInstrumentation Engineers (SPIE), Paris, France 1989, V 1136, HolographicOptics II, Principles and Applications, 283-289.andM;[17] Mehlhorn, K. and Vishkin, U.andP;  Randomized and detrministic simulations ofPRAMs by parallel machines with restricted granularity of parallel machines.andO;Acta Inf.andP;  21 (1984) 339-374.andM;[18] Papadimitriou, C.H.andP;  and Yannakakis, M. an architecture-independentanalysis of parallel algorithms.andP;  In Proceedings of the Twentieth ACMSymposium on Theory of Computing (1988) pp.andP;  510-513.andM;[19] Rajasekaran, S. and Reif, J.H.andP;  Optimal and sublogarithmic timerandomized parallel sorting algorithms.andP;  SIAM J. Comput.andP;  18, 3 (1989)594-607.andM;[20] Ranade, A.G.andP;  How to emulate shared memory.andP;  In Proceedings of theTwenty-eight IEEE Symposium on Foundations of Computer Science (1987) pp.andO;185-194.andM;[21] Schwartz, J.T.andP;  Ultracomputers ACM TOPLAS 2 (1980) 484-521.andM;[22] Siegel, A.andP;  On universal classes of fast high performance hashfunctions.andP;  In Proceedings of the Thirtieth IEEE Symposium on Foundations ofComputer Science (1989).andM;[23] Snyder, L.andP;  Type architectures, shared memory, and the corollary ofmodest potential.andP;  Annu.andP;  Rev.andP;  Comput.andP;  Scie.andP;  1, (1986) 289-317.andM;[24] Turing, A.M.andP;  On computable numbers with an application to theEntscheidungs problem.andP;  In Proceedings of the London Mathematical Society 422 (1936) 230-265; correction, ibidem 43 (1937) 544-546.andM;[25] Upfal, E.andP;  Efficient schemes for parallel communication.andP;  J. ACM 31, 3(1984) 507-517.andM;[26] Valiant, L.G.andP;  A scheme for fast parallel communication.andP;  Siam J.andO;Comput.andP;  11 (1982) 350-361.andM;[27] Valiant, L.G.andP;  Optimally universal parallel computers.andP;  Phil.andP;  Trans.andO;R. Soc.andP;  Lond.andP;  A326(1988) 373-376.andM;[28] Valiant, L.G.andP;  Bulk-synchronous parallel computers.andP;  In ParallelProcessing and Artificial Intelligence, M. Reeve and S.E.andP;  Zenith, Eds.,andO;Wiley, 1989 15-22.andM;[29] Valiant, L.G.andP;  General purpose parallel architectures.andP;  In Handbook ofTheoretical Computer Science, J. van Leeuwen, Ed., North Holland, Amsterdam1990.andM;[30] Walker, D.W.andP;  Portable programming within a message passing model: theFFT as an example.andP;  In Proc.andP;  3rd Conference on Hypercube ConcurrentComputers and Applications (1988), ACM Press.andM;CR Categories and Subject Descriptors: C.1.2.andP;  [Processor Architectures]: Multiple Data Stream Architectures (Multiprocessors)--Parallel processors;F.1.2 [Computation by Abstract Devices]: Modes of Computation--Parallelism.andM;General Terms: DesignandM;Additional Key Words and Phrases: Bulksynchronous parallel modelandM;Leslie G. Valiant is currently Gordon McKay Professor of Computer Science andApplied Mathematics at Harvard University.andP;  His research interests are incomputational complexity, machine learning and parallel computation.andM;Author's Present Address: Aiken Computation Laboratory, Harvard University,Cambridge, MA 02138; email: valiant @ harvard.harvard.edu.andO;</TEXT></DOC>