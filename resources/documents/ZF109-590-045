<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-590-045  </DOCNO><DOCID>09 590 045.andM;</DOCID><JOURNAL>EXE  Sept 1990 v5 n4 p48(3)* Full Text COPYRIGHT Process Communications Ltd. (England) 1990.andM;</JOURNAL><TITLE>Time for metrics. (software quality measurement)</TITLE><AUTHOR>Ormrod, James.andM;</AUTHOR><SUMMARY>Several software metric methods are useful for analyzing softwarequality and development time.andP;  Structured methods such as SSADMand Yourdon/DeMarco usually incorporate several standard analysis,design and implementation models.andP;  The Function Model showshierarchies of data transformations to determine code complexity.andO;The Retained Data Model lists the items in the data dictionary andtheir interrelationships to measure complexity in data-intensivecode.andP;  Function and data models can be combined and weighted todetermine system metrics.andP;  The Design Model counts programdecisions and data and control tokens to determine the complianceof the implemented system with the design.andP;  Implementation ismeasured by the volume and complexity of original code in aproject.andP;  Project metrics can improve estimates and measureperformance, but metrics teams must be objective experts dedicatedto metrics.andM;</SUMMARY><DESCRIPT>Topic:     Software MetricsManagementDefectsProgrammingSoftware DesignProgram Development TechniquesQuality Control.andO;Feature:   illustrationcharttable.andO;Caption:   Analysis process metrics. (chart)Design process metrics. (chart)Partial list of Walston-Felix factors. (table)andM;</DESCRIPT><TEXT>Time for Metrics 'You can't manage what you can't measure.'andP;  A carmanufacturer could not survive in such a competitive industry without keepingtight control on quality, costs and time-scales throughout all stages ofproduct development, including specification and design.andP;  Tight control isachieved by the collection and collation of a wide variety of measurements,performed on a regular basis.andP;  In addition to the benefits gained bycontinuously monitoring areas critical to the profitability of the company,such measurements can also ensure compliance of the product to the design,give early indication of problems and allow accurate cost forecasts to bemade by comparing the current situation with similar ones in the past.andM;Managing without measurement is precisely what the software industry has beentrying to do for years.andP;  The results speak for themselves and have producedstatistics now well-known: Average defect rates of 10 of more defects perKLOEC (1000 lines of executable code) in delivered systems, cost and timeoverruns of 10s or even 100s of per cent and maintenance costs exceeding thecost of the original software.andP;  The trade press is full of projects that arebeing abandoned because of overruns, some of them years into development andmillions of pounds over budget.andM;Yet the benefits to be gained by measuring aspects of software developmentare every bit as great as those enjoyed by manufacturing industries.andP;  Bykeeping track of time, resources and quality you can strive to makeimprovements and know when you've succeeded.andP;  Data from past projects allowsyou to estimate accurately and credibly, while comparing project phases withone another means that you can ensure compliance between them.andM;An improtant additional benefits to the industry as a whole is the ability tomake meaningful comparisons between development teams, currently impossible.andO;The surveys on software quality and cost that appear in the press oftenconflict with each other and are rarely as conclusive as they would like toappear; those respondents that aren't simply guessing are all measuringdifferent things, some more meaningful than others.andP;  Imagine trying tocompare cars if each manufacturer quoted unrelated sets of figures!andP;  (Whichis the fastest car: Top speed 135 mph, Max power 195 bhp or Max torque 205lb/ft?)andM;The idea of collecting software metrics from the software developmentlife-cycle is not new; much work was done in trying to establish measures ofcode complexity in the 1950s.andP;  Unfortunately, early attempts were toosimplistic to provide accurate predictions about systems that were alradycomplex and diverse, and the initiative was nearly abandoned in the early1960.andP;  The work of Walston and Felix in 1977 was a major contributor insparking renewed interest within the software industry; they listed a numberof predictive indicators (Walston-Felix factors) known to affectproductivity, some of which are paraphrased in Figure 1.andM;Much attention has been paid recently to integrating the disciplines ofStructured Analysis, Structured Design and Structured Coding to produce afull structured systems life-cycle.andP;  This attention, coupled with theinvestments being made in CASE tools, has led to rapid developments in thefield of Software Metrics.andP;  In particular, Tom DeMarco's work in suggestingpredictive indicators for every phase in the structured systems life-cyclemeans that reliable metrics are now available to anyone that wants to collectthem, including his famous 'Bang Per Buck' metric; a measure of valuecalculated as the complexity of the delivered system over the cost requiredto implement it.andM;Analysis MetricsandM;The use of a structured method, such as SSADM or Yourdon/DeMarco, means thatthe analysis phase produces a number of standard models, typically a FunctionModel (data-flow-diagram), a Retained Data Model (entity relationship diagramand data-dictionary) and a State Transition Model (in real-time systems).andO;Useful measures of size and complexity are available from all three models,the metrics collected and the weight applied to them depending upon thenature of the system being constructed.andM;The Function Model is usually expressed as a hierarchy of levels of relateddata transformations with details of data inputs and outputs (data flow),each level of which is a refinement of one of the transformations in thelevel above.andP;  The lowest level consists of transformations that cannot befurther split, or partitioned.andP;  Fully partitioned transformations are knownas Functional Primitives, each of which represents a low-level process in theuser requirement.andM;Although a large number of different metrics can be collected from theFunction Model, and most useful and best known is Function Point Analysis, orTotal Weighted FP.andP;  The number of each type of functional primitive withinthe function model is counted, and a complexity weighting applied to eachtype.andP;  The sum of the results gives a useful measure of the complexity of thesystem that is consistent across systems.andP;  To be fully thetitioned intouniform primitives and the local weighting factors constantly re-estimated.andO;Figure 2 shows an example of functional primitives at the lowest level of ahierarchy of data-flow diagrams, together with the collection and calculationTotal Weighted FP.andP;  Estimated effort is based on 2 man-days/weighted FP.andM;The Retained Data Model describes the components (objects) within thedata-dictionary and the relationships between them.andP;  The number of objectsand the way they are interrelated has an effect on the complexity of thesystem, and needs to be taken into account.andP;  The number of objects in theretained data model (OB), each weighted by a count of its interrelationships(RE), prodvides a useful measure of complexity for data strong systems.andM;Total Weighted FP alone can be used as a measure of complexity for functionstrong systems (systems with little or no retained data).andP;  OB, corrected byRE weightings, serves as a measure of complexity for data strong systems(systems with large, complex databases).andP;  Systems that are neither functionfor data strong require both measurements to be made.andM;Weighting factors depend upon local considerations and need to be constantlyre-estimated.andP;  Initial values of the amount of effort required to implementeach type of function point or data relation can be estimated and thencorrected as more project data becomes available.andP;  In addition, the toolsused in system construction have a bearing, for example, the use of a 4GL toconstruct a data strong system or a third party GUI to build the userinterface will reduce the amount of effort required, as will the use ofin-house or add-on libraries of functions.andP;  The ability to re-estimate theresources required for implementation does, however, imply accurate cost andtime accounting.andM;Analysis metrics can provide the earliest accurate revision to original costand time estimates, as well as a measurement of the relative data strength ofthe system.andM;Design MetricsandM;While the specification models the requirement, the design models theproposed solution, often in the form of a structure chart expressed as ahierarchy of connected modules, with control tokens and data passing betweenthem.andP;  At the design stage, the data-dictionary is reconstructed to describethe data objects used in the structure chart.andP;  Design metrics contribute tothe estimating process by giving a more accurate prediction of complexity,but are most useful in providing measurements that can be used to check thecompliance of the eventual implementation.andM;Each module's complexity varies according to the number of data and controltokens at its boundary and the predicted number of program decisions requiredto implement it.andP;  Data and control tokens can be counted directly from thedesign Model, with program decision counts being predicted by evaluating adata-dictionary formulation for the data arriving at the module.andP;  The sum ofthe complexity ratings for all modules in the system gives a measure of thetotal system complexity.andP;  Figure 3 shows a fragment of a structure chart andsample data-dictionary formulation.andM;Decision counts for each module are calculated by evaluating each dictionaryelement for the module according to its relationship.andP;  Iterative and optionaldata each counts as one with n-way decisions counting as n-1, as depicted inFigure 4.andP;  The resulting counts can be compared with the actual decisioncounts of the code written to implement the module, forming one measure ofimplementation compliance with design: Compliance = Expected DecisionCount/Actual Decision Count.andM;As with Total Weighted FP, Design Weight depends upon historical data foraccuracy.andP;  The effort required to implement a module will vary greatly fromenvironment to environment, as factors such as language, expertise andconstruction tools come into play.andP;  The only way of producing accuratemetrics is to continually monitor and record projects, re-estimating localweighting and effort data for each project.andM;Implementation MetricsandM;Implementation is the process of translating the design into code, from whicha number of useful metrics can be collected.andP;  Decision counts of procedurescan be calculated from the code and compared with the expected decisioncounts calculated from the design.andP;  Final implementation weight can bemeasured in terms of program volume and complexity, corrected to take accountof standard library modules.andP;  Early defect rates can be used to predictfuture defect rates, giving an indication of quality and a projection offuture meaintenance costs.andM;Code volume is calculated using a method published by Halstead in 1977:Volume = Length *log2(Vocabulary), where Length = Total count of operatorsand operands and Vocabulary = Count of unique operators and operands.andP;  CodeVolume is easily calculated, as the process lends itself to automation.andO;Figure 5 illustrates some pseudo-code for this process.andM;Once the system has been in use at the user's site, a meaningful qualitymetric can be calculated by dividing the total cost of defect identificationand correction by the code volume.andP;  The cost and effort of implementing ametrics function is worth it to improve this figure alone, since it isclosely related to both profitability and customer (user) satisfaction.andM;The greatest rewards to be gained from collecting metrics are not theimmediate ones.andP;  Many of the forecasts that can be made depend upon datacollected from past projects--the more historical data available, the moreaccurate the predictions.andP;  This dependency poses a problem; early forecastswill be based on minimal data (or none at all) and will probably be no betterthan traditional 'finger in the air' predictions.andP;  However, the accuracy ofestimates based on the analysis of past metrics improves with time, bothwithin a single project and across projects, while 'finger in the air'forecasts remain pure guesswork.andM;This makes it essential to maintain some sort of database of project metrics,and to work hard at improving estimates for figures that can't necessarily bemeasured, such as the amount of resource required to implement a functionpoint.andP;  If possible, the ideal way of collecting some early data is toanalyse past projects; you may not be able to collect every metric, but thoseyou do collect will repay the effort.andP;  Analysing past projects will alsoserve to train the metric collectors and may even throw up some surprises inthe process.andM;Metrics can have other, less obvious advantages, including the ability toprovide feedback to project team members and to implement realisticperformance related incentive schemes.andP;  The software house will have a bettercompetitive advantage and the systems development department more credibilitywithin the organisation.andM;Implementing a metricsandM;functionandM;Collecting and collating metrics is a time consuming business and requires ahigh degree of expertise.andP;  Subjectivity must also be avoided at all costs,the person collecting the figures having no other connection with the projectconcerned, either managerial or technical.andP;  The best way of implementing ametrics function, therefore, is to set up a separate Metrics Team whose onlyproject role is to collect and collate data.andP;  Team members need to beexperienced analysts and programmers with a detailed understanding of allproject phases.andP;  Existing Quality Control Teams can absorb the metricsfunction if they already meet the criteria above.andM;It is the Metrics Team that does the counting, devising and refining theirown techniques.andP;  Some measurements, such as Code Volume, can be collectedautomatically but require appropriate programs to parse source code modules.andO;Since the Metrics Team will be composed of experienced project personnel, itcan be set the task of specifying and developing a metrics database with aaccompanying analysis software.andM;With a Metrics Team established and a CASE tool and structured methodology inplace, the project team can set about constructing software that is deliveredwithin budget and on time.andM;In the future, CASE tool vendors may include metrics collecting functions intheir products, and may even market databases of past project metrics.andP;  Assoftware construction becomes more of a science, so the industry will gaincredibility with the users, who are more likely to accept estimates that havebeen calculated rather than guessed.andP;  Project Managers will be able to committo fixed price projects in the same way that the construction industry does,with penalty clauses for overruns and bonuses for early completion.andP;  (In theconstruction industry a 6% overrun is considered a debacle.)andM;As more and more data becomes available it should eventually be possible toconstruct the software industries equivalent of the builders' blue-book,listing the times and resources required for each separate task for eachdifferent type of project, for each different structured method andprogramming language.andP;  Estimating then becomes a simple process ofidentifying the tasks required and then looking each one up in turn, addingup the resources and time required.andM;James Ormrod is a freelance consultant and Clipper hack who scrapes a livingfrom project management consultancy, applications development and training.andO;He can be contacted on 0491 35187.andP;  If you are interested in implementing ametrics function he recommends Controlling Software Projects (Tom DeMarco,published by Yourdon Press, ISBN: 0-13-171711-1).andO;</TEXT></DOC>