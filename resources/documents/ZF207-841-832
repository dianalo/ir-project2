<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-841-832  </DOCNO><DOCID>07 841 832.andM;</DOCID><JOURNAL>UNIX Review  Oct 1989 v7 n10 p54(8)* Full Text COPYRIGHT Review Publications Co. 1989.andM;</JOURNAL><TITLE>Getting the word. (multimedia systems)</TITLE><AUTHOR>Schmandt, C.; Arons, B.andM;</AUTHOR><SUMMARY>Interactive voice applications are among the most importantpotential uses of multimedia technology.andP;  Integrating suchapplications as voice annotation of text, interactive audiotraining systems, and voice-recognition input presents a majordevelopment challenge.andP;  Limitations of audio utility include thefact that speech is slow and the difficulty of producingintelligible synthetic speech.andP;  Real-time variants of UNIX offerthe most potential as interactive-audio platforms.andP;  Both speechrecognition and synthesis require specialized digital signalprocessors.andP;  A server approach is the best way to support audiohardware.andP;  The VOX Audio Server, a network-transparentclient/server system designed for integrated audio functions, isdescribed.andP;  VOX will run on BSD UNIX and the Mach Unix variantwith AT-bus based workstations; extensions for speech recognitionand synthesis are planned.andM;</SUMMARY><DESCRIPT>Topic:     Audio VisualClient/Server ArchitectureInteractive SystemsMultimedia TechnologyVoice Recognition.andO;Feature:   illustrationchart.andO;Caption:   VOX's underlying layered architecture. (chart)Typical configuration of the audio devices currently supported(chart)andM;</DESCRIPT><TEXT>GETTING THE WORDandM;Voice, the state the obvious, is the principal medium of communicationbetween humans.andP;  It seems only natural, then, to think of voice as apotentially efficient form of communication between humans and computers.andM;What may not be so obvious at first glance, though, are the many complextechnologies that might be needed for computers and their users tocommunicate via voice.andP;  Taken broadly, the use of speech as a command anddata channel may require digital recording and playback, speech recognition,text-to-speech synthesis, and telephone interface equipment.andM;The probable future uses of voice communication between humans and machines,and the difficulties developers can expect to encounter in integrating itinto our everyday computing environment, are the subjects of this article.andO;Our focus will be the issues involved in integrating voice technology withcomputer systems, particularly advanced workstations.andP;  Throughout, we willuse the term audio to refer to a medium, and the terms voice and speechinterchangeably to refer to the signal carried by that medium.andM;Researchers have suggested a number of many potential audio applications,including &quot;listening&quot; typewriters, voice annotation of text, interactiveaudio training systems, voice mail systems, computer conferencing, telephoneaccess to data, speech substitutes for the mouse and keyboard, auditoryicons, speed-dialing tools, and telephone answering machines.andP;  None of thesevoice utilities is overwhelmingly difficult to implement in isolation fromthe others, but a &quot;synergistic collection&quot; of multimedia applications thatmakes full use of voice communication is required to create a powerfulcommunications environment.andP;  Integrating a range of applications, though,will place though demands on the hardware and software architecture ofworkstations capable of supporting voice communication.andM;Let's consider in detail some of the applications people might want to buildusing a voice-communication component.andP;  Voice might be used to annotate text,for example--to insert editorial comments into a manuscript of as part of anon-line tutorial program.andP;  Voice could be incorporated into a more generalmultimedia document, such as a repair manual or a video-based educationalsystem.andP;  Voice, either synthesized from text or pre-recorded, might allowremote telephone access to such databases as stored electronic mail, listingsof flight departures, or up-to-the-minute stock quotations.andM;Voice-mail applications lead the multimedia pack when it comes to successfulaudio applications.andP;  Usually implemented in a centralized architectureclosely linked to a private-branch exchange (PBX) telephone system,voice-mail applications could be even more useful, both in terms of improveduser interfaces and the sharing of voice data between applications, if theyallowed for workstation access to messages.andP;  The now-ubiquitous telephonewill continue to play a key role in voice communications and, therefore, inaudio workstation technology.andP;  In particular, the telephone is likely to bethe main source of voice as data (in the form of recorded messages).andP;  It isalso a powerful channel for remote human interaction with the machine.andM;For teleconferencing, voice is, of course, a requirement.andP;  Although computersupport for teleconferences could consist simply of sharing windows amongusers and setting up separate telephone calls, the computer-mediated voicelink has several advantages.andP;  First, it allows both data and voice links tobe initiated using a single conference-management application.andP;  Second, theparties involved can use voice communication to control who has the &quot;floor&quot;.andO;Third, teleconference sessions that include voice can be logged.andP;  Multi-partyaudio conferencing can be arranged using a telephone network and conventionalaudio-bridging equipment, or with computer networks that transmit voicesignals digitally.andM;Speech recognition by computers is a more difficult problem to solve.andO;Hyperbole in the mass media notwithstanding, general-purpose,large-vocabulary &quot;listening&quot; typewriters that can transcribe free-form humanspeech are still a long way off; keyboards as a tool for data entry will bewith us for the foreseeable future.andP;  Another capability computers mightsomedays by endowed with--voice recognition over telephone lines--wouldcertainly be useful, but limited telephone audio bandwidth and noise problemsmake it one of the more challenging tasks now facing developers.andM;Specific workstation applications in which the mouse is being used for otherfunctions, such as drawing lines in a paint program, might be candidates forvoice-input capabilities.andP;  Speech input might be used alongside mouse inputfor control or for driving menus.andP;  In the Media Laboratory at MIT, one suchapproach involving the use of voice to move between windows is being exploredunder the X Window System; speech is used as the channel for the &quot;metadialog&quot;of communication with the window manager, rather than simply as a keyboardsubstitute.andM;The difficulties involved in developing full-scale voice applications forcomputers mean that voice might be more profitably used as a secondary oradjunct channel for human interaction with machines.andP;  Voice communicationwith the machine is particularly useful when the user's hands or eyes arebusy--when the user is, for example, running a CAD system, or sortingbaggage, or driving a car.andP;  Naturally, the cost of implementing voicecommunication as a method for interacting with computers is most justifiablein cases where no other means of access to computer control isavailable--such as when users are physically impaired or are working remotelyover a telephone line.andM;Limitations and User-Interface Requirements.andP;  The utility of audio is limitedin several respects, some of which are inherent in the medium and some ofwhich are attributable to current audio technology.andP;  Such limitations must betaken into account when applications and user interfaces are designed.andO;Speech is slow, for instance; at perhaps 150 words per minute, a speakerconveys information at a pace that is quite a bit slower than the speed atwhich a competent reader can comprehend a string of written words, whether onscreen or on paper.andP;  Speech is even slower than a 300-baud modem.andP;  And speechis necessarily serial, being by nature a time-varying signal; eyes can wanderaround to explore menus, but the ear cannot.andP;  Speech is &quot;bulky&quot; to store andcannot be scanned or greped the way text can.andP;  The act of listening,especially when synthetic speech is involved, puts a cognitive loan on humanusers that can interfere with other types of mental activity.andM;Using currently available devices, there are limitations on theintelligibility of synthetic speech, and the error rates of speechrecognizers are still high.andP;  For synthesis, text is analyzed by theapplication of several layers of linguistic rules and broken down intolexically meaningful sound units called phonemes, which are then realized bya computer as an acoustic waveform.andP;  Simple text-to-sound rules don't workwhen it comes to many proper nouns, and phoneme realization problems makesome sounds nearly indistinguishable from others in synthesized speech.andP;  As apractical input device, speech recognition is still in its infancy, and evensmall-vocabulary, speaker-dependent recognizers have difficulty functioningin acoustically imperfect environments, often requiring the use ofhead-mounted, noise-canceling microphones.andP;  It is usually easier to implementthe recognition of isolated words than of continous speech, andspeaker-dependent environments are easier to accommodate than arespeaker-independent ones.andM;Such limitations dictate certain requirements for user interfaces to voicedata.andP;  For example, the &quot;bulkiness&quot; of voice suggests using graphicalinterfaces to facilitate random access and to give some sense of &quot;place&quot;inside a voice document or voice file being edited.andP;  Such representationstranslate the temporal nature of speech into spatial dimensions, sometimesproviding cues like time (tick marks) and speech/silence intervals (color,size) in addition to a cursor that moves as the sound is played (see Figure1).andM;Because of its slowness, audio playback or speech synthesis always needs tobe interruptible, and mechanisms must be provided for replaying, speeding up,or skipping ahead.andP;  Transitions between modes--switching between play andrecord modes in a conversational system, or stopping the playback of promptsas immediate feedback when a caller pushes a button on a telephone keypad,for instance--must entail a minimum of overhead.andP;  The configuration of a userinterface for a speech recognition system is especially difficult, requiringthat developers choose vocabulary and craft the application's functionalitywith great care.andP;  Speech recognition as a direct replacement for the keyboardalmost never works.andM;System REquirements and Application Integration.andM;The wide range of potential applications for voice, along with the ratherburdensome requirements it makes of the user interface, also places greatdemands on the computer system that will support it.andP;  Some of theserequirements are &quot;device&quot;-related, but it is also important to develop asoftware architecture that allows resources to be shared.andP;  This is especiallytrue in light of the probability that voice will better succeed as a datatype if it can be shared across applications rather than used only within asingle program.andM;In the early days of voice systems, much work was devoted to data-compressionschemes.andP;  The goal was to cut bit rates while minimizing the negative impacton intelligibility.andP;  As computer power, memory sizes, and network speeds haveincreased, the need for many of these techniques is waning, except inspecialized applications such as encrypted communications or computerizedtoys.andP;  To produce telephone-quality speech, evolving applications tend to use64-kilobit-per-second log PCM (pulse code modulation) or perhaps 32- or16-kilobit-per-second encoding (such as Adaptive Delta PCM, also known asADPCM.)andP;  This results in continuous data-transfer rates well within thecapabilities of most workstations and filesystems.andM;UNIX is notorious for its high overhead in interrupt response, however, andthere is a real need to buffer data on both play and record operations.andP;  Asopposed to the process of refreshing a text screen or repainting an image,for example, voice output needs to be continuous, without any pauses.andP;  Ifthis is achievable only when the workstation is otherwise idle, the utilityof voice applications will be severely limited.andP;  The higher fidelity andhigher data rates associated with compact-disc-quality recordings aretypically not required for interactive voice applications.andM;Implementing low-latency operations to provide a comfortable user interfaceis an even greater challenge.andP;  Users may want to play several sound files insuccession, or to start recording immediately after playing a synthesized orpre-recorded prompt.andP;  Real-time, fine-grained silence detection duringrecording, as a termination condition, is required in a conversational systemthat alternates prompts and responses; rhythm, intriguingly, is a keycharacteristic of natural-sounding dialog.andP;  These kinds of operations aredifficult to perform in non-real-time operating systems, including mostvariants of UNIX.andM;Part of the solution to latency problems is to provide adequate eventprocessing on whatever kind of speech hardware is used, especially fordigitization and playback.andP;  The relatively few pieces of state informationrequired (such as a flag for &quot;terminate playback when a touch tone isdetected&quot;) can be implemented by a dedicated processor, which also can handlebuffer management, flow control, and possibly direct memory access.andP;  Inreal-time audio applications, the primitives that UNIX processes use forcommunication with drivers must be chosen carefully, taking into accounttradeoffs between the ease of application programming and the complexity ofthe state logic.andM;Both recognition and synthesis are likely to require a specialized digitalsignal processor (DSP).andP;  This processor may also be able to deal with certainevent-handling configurations.andP;  In the future, use of a DSP may be the mostcost-effective approach for audio applications, allowing rapid changebetween, say, synthesis and playback through the simple downloading of a newalgorithm.andP;  The same DSP may also be used for other communication tasks,including modem or facsimile emulation.andP;  Texas Instruments' Speech Card, forexample, supports recording, recognition, and text-to-speech synthesis;Natural Microsystems' Watson Card supports modem signaling in addition torecording.andM;As for software architecture requirements, it is important to reiterate thatvoice needs to be implemented as a ubiquitous data type rather than simply asa component of a specialized interface in specific applications.andP;  Although ashumans we use voice all the time, voice as a means of interacting with acomputer is unlikely to become commercially popular until it becomesrelatively easy to use--that is, until we can do with voice many of thethings we now do with text: edit it, forward it, and integrate it with otherfiles into coherent documents.andP;  Similarly, remote telephone access by voiceis only as useful as the range of data it makes accessible.andP;  Finally, we needto be able to use voice with advanced window systems, both as an interface tothe window system itself and as &quot;selected&quot; data that can be moved betweenprocesses running in different windows.andM;These requirements suggest that a general-purpose audio environment will needto allow multiple processes to access audio hardware and files.andP;  (Theseprocesses themselves may also be distributed across processors.)andP;  Mechanismsmust be provided to detect conflicting requests for scarce resources and toarbitrate between them.andP;  We must be able to build graphical interfacesclosely coupled to sound interfaces, capable of maintaining synchronizationduring record and playback activities.andP;  A means of representing multimediaobjects in the context of user selections must be provided; this will be mostuseful if it allows maximum interoperability with current text-onlyapplications.andM;Server Approaches to Voice.andP;  Many of the requirements listed above point tothe use of a server to handle audio hardware.andP;  Just as current window systems(such as MIT's X and Sun Microsystems' NeWS) may employ a single server todraw on the screen for multiple clients, an audio server must be able tohandle requests from multiple application processes.andP;  The server must alsodeal with conflicting requests from multiple processes, of course, and thestate associated with each.andM;A server can provide event queues to ensure low latency between time-criticaloperations.andP;  It can provide for synchronization, which tends to be difficultin the audio domain as voice operations extend over time.andP;  An audio requestmay be de-queued and processed only after the previous request has finished,which may be long after either request was submitted; thus, queue handling isasynchronous with respect to the application.andM;A server also can provide a way to establish a device-independent interfaceto a variety of vendor hardware.andP;  The server does not have to reside on thesame processor as the application, making feasible the use of bus-specifichardware or a real-time operating system on the server without eliminatingthe UNIX runtime and application-development environments for the client.andO;This may allow for eventual server implementations using expensivecentralized resources, such as specialized speech recognizers that can beswitched between many client workstations on the basis of trafficrequirements without needing replicated hardware.andM;A relatively small number of audio servers have made their way to market (see&quot;Audio Servers: A Brief History&quot;, on page 56), spanning a range ofarchitectures.andP;  For the purposes of this discussion, we will focus on aserver that is currently being designed and built in a research context, butwhich developers hope will eventually be widely available and generallyaccepted.andP;  This example will provide us with a means for describing a few ofthe technical details of audio server architectures.andM;The VOX Audio Server, under development at the Olivetti Research Center (ORC)in Menlo Park, CA, is designed to include the full range of audio featuresdiscussed in this article.andP;  The VOX server provides a device-independentinterface for audio functions, including play, record, and telephony; thereare also plans to include extensions such as speech recognition andsynthesis.andP;  In addition, the VOX server will provide support for audiorouting and mixing, two capabilities which must be an integral part of anyrealistic audio workstation environment.andM;VOX is based on a network-transparent client/server architecture heavilyinfluenced by contemporary window systems (see Figure 2).andP;  As a server, VOXpermits multiple clients to share audio resources.andP;  Hooks are provided thatallow a privileged client, analogous to the window manager in a windowingenvironment, to arbitrate conflicting requests.andP;  Network transparency meansthat the client interface is the same whether the server is located on thesame processor as the client or resides elsewhere on a network.andM;The low-level building block of the audio server is the logical audio device(or LAUD, pronounced &quot;loud&quot;).andP;  A LAUD is a device-independent abstraction ofan audio resource.andP;  For example, playback and recognition are represented byseparate LAUDs, even though these functions may be implemented on a singlepiece of hardware.andP;  Client applications may request multiple instances of thesame LAUD.andP;  LAUDs have &quot;audio ports&quot; that can be interconnected, and eachLAUD controls a device that performs the desired audio activity.andM;LAUDs can be combined to form a composite LAUD (or a CLAUD, pronounced&quot;cloud&quot;).andP;  The primary goal of such a construct is synchronization betweenLAUDs working together to provide a higher-level service.andP;  For example, ananswering machine requires synchronization between record, playback, andtelephony events.andP;  To that end, VOX multiplexes input events from a CLAUD'scomponent LAUDs into a single, time-stamped stream, and similarlyde-multiplexes output requests within the server.andP;  For example, the clientmay submit play and record requests to the CLAUD, and receive input tokensfrom the telephone through the CLAUD.andM;To ensure real-time response, output requests can be prepared in advance ofexecution.andP;  Examples of this pre-processing include opening a sound file,fetching a pre-recorded sound, or establishing the state of a speechrecognizer.andP;  All these tasks can be executed before a request is actuallyserviced--the latency between related requests, such as is involved inswitching from synthesis to recognition in a conversational application, isreduced.andM;Additional synchronization and resource-allocation mechanisms are provided toallow a client to gain exclusive access to a limited resource.andP;  When only onetelephone line is installed, for example, it is impossible to place a secondcall without disconnecting the first, or blocking the process placing thesecond call until the first finishes.andM;VOX is being written to run in BSD UNIX and Mach operating-systemenvironments, using AT-bus-based workstations to take advantage of thevariety of low-cost speech boards and peripherals that fit in such machines.andO;The first &quot;reference implementation&quot; uses an Olivetti i386-based workstationrunning Mach.andP;  It supports the Natural Microsystems VBX speech board (whichoffers record, playback, and telephone functions), a Yamaha mixer, and anAkai audio crossbar switch (the latter two are controlled via a MIDIinterface).andP;  A VideoTelecom full-duplex echo canceler will be used to providea high-quality, hands-free speakerphone using microphones and speakersavailable on each desktop (see the schematic in Figure 3).andM;The audio crossbar switch handles routing and device interconnection withinthe workstation and connects to other workstations with tie-line links.andP;  Allthe inputs from, and outputs to, audio peripherals are routed through theswitch, which provides a flexible environment for the rapid prototyping ofaudio applications.andP;  Applications of particular interest include voiceannotation, telephone management, real-time teleconferencing, conversationalanswering machines, and, more generally, computer-based tools to supportcollaborative work.andM;The current thrust of the VOX Audio Server development project is thebuilding of research prototypes to demonstrate the utility of the server andof desktop audio in general.andP;  However, the software architecture of theserver can be carried over into a product environment.andP;  Indeed, the OlivettiResearch Center is making the software interfaces to the servernonproprietary so that it can be supported across multiple hardware andsoftware platforms--thus promoting interoperability between clients andservers running in a heterogeneous network of machines.andM;Audio is a demanding medium, but is clearly one that is ripe for integrationinto our everyday computing environment.andP;  At present, there is great demandfor interactive voice applications; unfortunately, the medium is difficult towork with, and the technology currently available has many weaknesses.andO;Creating an integrated environment using voice (both as data and for control)across a range of applications suggests adopting a server-based approach.andO;While current technology can support such a server, its architecture remainsan important topic for further research.andM;Chris Schmandt is a principal research scientist at the MIT Media Laboratory,as well as director of its Speech Research Group.andP;  For the past ten years hehas been conducting research into conversational computer systems as well asinto voice-telecommunications applications.andM;Barry Arons is a research engineer and leader of the Multimedia TechnologiesProject at the Olivetti Research Center.andP;  His research interests are focusedon interactive voice and video systems.andM;BibliographyandM;B.andP;  Arons, C. Binding, K. Lantz, and C. Schmandt, &quot;The VOX Audio Server&quot;, inProcedings of the Second IEEE ComSoc International Multimedia CommunicationsWorkshop, IEEE Communications Society (April 1989).andM;C.andP;  Binding, K. Lantz, B. Arons, and C. Schmandt, &quot;Workstation Audio andWindow-Based Graphics: Similarities and Differences&quot;, in Engineering forHuman-Computer Interaction, IFIP Working Group 2.7 (August 1989).andM;G.L.andP;  Martin, &quot;The Utility of Speech Input in User-Computer Interfaces&quot;,International Journal of Man/Machine Systems, Vol.andP;  30, pp.andP;  355-375 (1989).andM;C.andP;  Schmandt and B. Arons, &quot;A Conversational Telephone Messaging System&quot;, IEETransactions on Consumer Electronics, Vol.andP;  CE-30, No.andP;  3, pp.andP;  xxi-xxiv(August 1984).andM;C.andP;  Schmandt and M.A.andP;  McKenna, &quot;An Audio and Telephone Server forMulti-Media Workstations&quot;, in Proceedings of the Second IEEE Conference onComputer Workstations, IEEE Computer Society, pages 150-159 (March 1988).andM;P.T.andP;  Zellweger, D.B.andP;  Terry, and D.C.andP;  Swinehart, &quot;An Overview of theEtherphone System and Its Applications,&quot; in Proceedings of the Second IEEEConference on Computer Workstations, IEEE Computer Society, pages 160-168(March 1988).andM;AcknowledgementsandM;Along with the authors, Keith Lantz and Carl Binding were members of the VOXAudio Server design team.andP;  In addition, Lantz made numerous helpful editorialcomments and contributed to the section of VOX, for which the authors thankhim.andO;</TEXT></DOC>