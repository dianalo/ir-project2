<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO> AP880906-0198 </DOCNO><FILEID>AP-NR-09-06-88 1212EDT</FILEID><FIRST>a a BC-EXP--ChattyComputers Adv08   09-06 1184</FIRST><SECOND>BC-EXP--Chatty Computers, Adv 08,1222</SECOND><NOTE>$adv08</NOTE><NOTE>For Release Thursday, Sept. 8, and thereafter</NOTE><HEAD>Computers Breaking Down The Language Barrier</HEAD><HEAD>With LaserGraphic</HEAD><BYLINE>By MARCIA DUNN</BYLINE><BYLINE>Associated Press Writer</BYLINE><DATELINE>PITTSBURGH (AP) </DATELINE><TEXT>   Don't expect the Tower of Babel to tumble, butcomputer scientists are breaking down language barriers withmachines that can translate languages in seconds and recognizespeech regardless of accent.   ``Where can it lead to?'' said National Science FoundationAssistant Director Bill Wulf. ``How big is your imagination?''   Need to call Tokyo? Hotel switchboards, within a few years, willbe able to connect English-speaking callers to a special computerthat translates their words into Japanese, then responds by machinein English.   Stuck in a foreign hospital? Patients and doctors who speakdifferent languages will be able to communicate via computer inabout a decade.   Traveling abroad and can't speak the language? Forget adictionary. Plan on packing a pocket translator.   Too busy to type information into the computer? Just dictate.   The list goes on and on, including eyeglasses for the deaf withscreens that provide transcripts of conversation, and the abilityto order merchandise like plane tickets by talking on the phone tocomputers.   ``One of the most distinguishing characteristics of human beingsis our ability to communicate,'' Wulf said. ``What you're seeinghere are techniques that improve that ability.''   Companies are beginning to cash in on voice recognition programseven while experts are trying to make the computers capable ofdealing with the ambiguity of human language, bad grammar anddifferent accents, among other things.   ``We need to reduce the costs and increase the size of thevocabulary and increase the capabilities,'' said Raj Reddy,director of Carnegie Mellon's Robotics Institute. ``All of thoseare yet to be solved.''   ``What is there is obviously a very significant improvement overwhat we used to know how to do even six months ago.''   Dragon Systems Inc. of Newton, Mass., developed a program XeroxCorp. used in 1986 to save nearly $10 million by inventorying all2.2 million of its parts for the first time, said Dragon assistantmarketing manager Jonathan Robbins.   The $100,000 system recognized 1,000 words. A 5,000-word programis out, and a 20,000-word version is expected by early 1989, saidRobbins. Dragon also is working with the federal Defense AdvancedResearch Projects Agency to develop a voice-controlled jet fightercockpit.   Kurzweil Applied Intelligence Inc. of Waltham, Mass., hasdeveloped voice recognition systems for radiologists and emergencyrooms. Doctors dictate their reports to a computer, and the reportsare printed. The programs originally recognized 1,000 words, butthe vocabulary has been expanded to 5,000.   More than 100 U.S. hospitals use the radiology system, VoiceRAD,and about a dozen have VoiceEM for emergency rooms, said spokesmanMartin Schneider.   Both Dragon and Kurzweil's systems require brief pauses betweenwords and must be trained to recognize each speaker.   ``Most of us don't speak properly most of the time,'' saidReddy, president of the American Association for ArtificialIntelligence. ``If you transcribe anybody's voice ... there are alltypes of pauses and repetition, hums and haws, which we know how toignore. Computers don't know that yet.''   ``Either we have to come up with ways to deal with (this) or wehave to teach people to speak right,'' said Kai-Fu Lee, a CarnegieMellon researcher who has developed a speech recognition systemknown as Sphinx.   It's a slow, complex task because human language is rife withambiguity, unlike mathematical, scientific or computer language.   Take, for example, this sentence: ``The box is in the pen.''   ``For humans, it's obvious because we all know that the boxcannot be in the writing pen (through) our unconscious kind ofcommon sense,'' said Masaru Tomita, associate director of theCarnegie Mellon University Center for Machine Translation.   ``If you want computers to do this, it's going to be verydifficult,'' Tomita said. ``The computers have to know the typicalsize of a box and the typical size of a pen. One thing we can dovery quickly is that whenever the word `pen' appears, we can assumeit's a writing pen. But on the other hand, then the system willmake mistakes.''   A classic example of machine misinterpretation involves theepigram, ``The spirit is willing, but the flesh is weak.'' Earlytranslating systems turned that into, ``The vodka is good, but themeat is rotten.''   Lee's Sphinx, unlike earlier systems, can identify English wordsspoken continuously. It boasts up to 96 percent accuracyidentifying 997 words, people don't have to spend hours training itto recognize their speech patterns, and minor variations in accentpose little problem.   ``It's the first system that has all these capabilities,'' saidLee, 26, who has been working on Sphinx for 1{ years, funded by theDefense Advanced Research Projects Agency.   Sphinx transforms spoken words from changes in air pressure intochanges in voltage. Each 10-millisecond slice of sound is assigneda string of digits and, through a mathematical process, matchedwith other sequences to yield the word with the best possiblemeaning. So time isn't wasted searching the entire vocabulary,Sphinx takes syntax into account.   Tomita, 30, who plans to work this fall with Lee, has developeda system capable of recognizing 100 words of doctor-patientdialogue in Japanese, then translating the Japanese into Englishuttered by a computer's speaker.   The computer transforms the spoken Japanese into its writtenequivalent, which is translated into written English and sent toanother part of the computer that generates English speech.   Tomita also has devised a program that translates writtenEnglish or Japanese into written English, Japanese or German. Ahypothetical exchange between a doctor and patient _ ``I have aheadache'' or ``Take two aspirin'' _ is typed into the computer andthe translation appears within seconds.   Like Sphinx, Tomita's 1,000-word systems consider context.   He hopes to increase the programs' vocabularies and eventuallyexpand them to conversation used in making hotel reservations andregistering for conferences. His work is funded, in part, by IBMCorp. of Tokyo.   ``We need to pick some domain where it's very, very defined,always clear what you're talking about,'' he said.   The need for translating systems is considerable, especially forthose not fluent in English, according to Tomita, who countshimself in that category.   There are more than 3,000 world languages and dialects, saidJaime Carbonell, director of the Center for Machine Translation.   ``Americans don't seem to be aware of the fact that maybe 90percent of the world population doesn't speak English at all,''said Tomita. ``But everywhere you go, there is somebody who canspeak English.''   Because of that smugness and sense of superiority as well asinsufficient funding, machine translation has received scantattention in the United States since the 1960s, experts say. Mostwork has been in Europe and Japan.   ``The man on the street may or may not directly benefit from amachine translation system,'' Wulf said. ``On the other hand, hemay benefit indirectly from American scientists being able tointeract better with the Russian or the Japanese or the Frenchscientists.''</TEXT><NOTE>End Adv for Sept. 8</NOTE></DOC>