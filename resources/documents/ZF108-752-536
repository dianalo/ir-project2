<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-752-536  </DOCNO><DOCID>08 752 536.andM;</DOCID><JOURNAL>Communications of the ACM  August 1990 v33 n8 p72(11)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Natural language understanding and speech recognition. (includesrelated summary article) (technical)</TITLE><AUTHOR>White, George M.andM;</AUTHOR><SUMMARY>The importance of natural language understanding (NLU) indeveloping effective speech recognition systems is discussed.andO;Automatic speech recognition (ASR) development has paralleled NLUdevelopment for the past 20 years but has been largely separatedfrom it.andP;  Integrating NLU and ASR knowledge sources involvesdeveloping probabilistic state machine networks and developingschemes for interconnecting disparate knowledge domains.andO;Asynchronous processing of knowledge sources at different levelsallows parallel rather than serial processing and avoidsbottlenecks.andM;</SUMMARY><DESCRIPT>Topic:     Natural Language InterfacesArtificial intelligenceVoice RecognitionKnowledge-Based SystemsExpert SystemsTechnologyResearch Design.andO;Feature:   illustrationchart.andO;Caption:   Hierarchy of knowledge sources. (chart)Typical ASR System. (chart)Generic finite state machine. (chart)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>NATURAL LANGUAGE UNDERSTANDING AND SPEECH RECOGNITIONandM;Natural language understanding (NLU) refers to computer understanding ofhuman language, which includes spoken as well as typed communication.andP;  Mostof the techniques developed for NLU over the last 25 years are largelyconcerned with syntax analysis of grammatically correct typed sentences.andO;These techniques may not be useful in dealing with the nongrammatical natureof normal speech.andP;  This article focuses on speech understanding and themarriage of NLU techniques with speech recognition techniques needed toachieve speech understanding.andM;Automatic speech recognition (ASR) as a field of research has proceeded on aparallel but separate track from NLU for more than 20 years.andP;  For most ofthis time, ASR research has been preoccupied with translating acousticinformation into computer commands or text without involving formal NLUtechniques.andP;  (We use the term &quot;recognition&quot; as distinct from &quot;understanding&quot;when no linguistic or semantic analysis is involved.)andP;  It might appearreasonable for speech understanding to be achieved by acoustic phoneticanalysis followed by NLU analysis.andP;  However, the subtleties of normal speechrender it impractical to create a simple feed-forward interface between anacoustic analysis stage and current NLU techniques that expect reliableword-sized units as input.andM;For normal spontaneous speech, acoustic-phonetic analysis techniques bythemselves will never be able to produce an unambiguous stream of textequivalent to the typed input expected for NLU systems.andP;  Thus, at the veryleast, an NLU stage would have to be modified to handle errorful input.andP;  Inaddition, a feedback mechanism should be provided between the NLU andacoustic phonetic stages.andP;  Otherwise, the acoustic-phonetic stage mustincrease the data processed by an order of magnitude in order to produce asufficient set of alternatives to be passed on the NLU stage.andP;  In otherwords, both NLU and acoustic analysis stages must be modified to achieve acomputationally efficient system.andM;Historically, NLU research has focused on grammatically correct typed queriesof databases and automatic language translation of textbook documents, (e.g.andO;technical journals, legal briefs, financial statements).andP;  The most successfulcommercial examples of NLU systems are computerized information retrievalsystems from typed queries (e.g.andP;  Symantec's Qandamp;A [9]).andP;  There are only a fewlanguage translation systems and they work with restricted sources of text[16].andP;  There are no commercially successful NLU systems built specifically towork with speech input.andP;  (There are dozens of firms offering ASR, most arefor personal computers, and none contain NLU technology to assist in therecognition of utterances.)andM;It is widely believed that NLU techniques must be used in conjunction withacoustic analysis techniques to achieve recognition of continuous speech.andO;This is because continuous speech is normally filled with acoustic ambiguitywhich can only be resolved through the use of higher sources of knowledge,principally semantic and pragmatic sources.andP;  The Defense Advanced ProjectsResearch Agency (DARDA), which provides the major funding for nonproprietaryASR research as well as natural language understanding, is sponsoringinterdisciplinary projects explicitly to combine NLU and ASR disciplines.andO;The VOYAGER system, developed at the Laboratory for Computer Science at MIT,is an example of this type of interdisciplinary project.andP;  VOYAGER is aspeaker-independent, continuous speech-understanding system that employs anatural language component to understand and answer queries concerning thelocations of public buildings in the Harvard/MIT area.andP;  The system is able tohandle about 66 percent of the spontaneous questions asked of it in spokenEnglish within its task domain.andM;Even though DARPA first began sponsoring speech-understanding research (SUR)in 1970, DARPA contractors are just beginning to successfully merge NLU andASR research disciplines.andP;  Earlier SUR work produced systems that hadsemantic and prosodic components that theoretically were integrated withacoustic phonetic analysis components.andP;  However, these integrated systemswere not successfully demonstrated because of inadequate computing power.andO;Most ASR research has not focused on NLU but rather on the recognition ofisolated utterances, or continuous utterances from highly constrainedvocabularies (such as the digits); or speaker-dependent input instead ofspeaker-independent input; or carefully articulated continuous speech innarrow task domains with artificial language syntax constraints.andM;MEanwhile, the need for integrating NLU and ASR techniques has continuallyincreased as the opportunity for spoken language communication with personalcomputers has increased, fueled by the increasing computational power ofpersonal computers (and &quot;workstations&quot;) for interactive problem solving.andM;Natural language understanding plays two roles in translating speech queriesor statements into useful computer commands.andP;  The first role is imputing thecorrect meaning of the speech so the computer gets the right message...is oflittle concern here.andP;  The second, more subtle role (and main focus of thisarticle) is reducing acoustic phonetic ambiguity in normal speech based on&quot;understanding&quot; of meanings.andM;Without NLU techniques, ASR systems using a purely acoustic-phonetic analysisperform well only on slowly pronounced isolated words.andP;  This is not the waywe usually talk to one another, and pronouncing words in isolation is slowand tedious compared to continuous speech.andP;  However, it is useful and is thebasis of a 30,000 word recognition system, DragonDictate [2] from DragonSystems.andP;  This is the largest vocabulary system commercially available in theworld.andP;  The automatic recognition of continuous speech for thousands of wordsis much more difficult because continuous speech is filled withacoustic-phonetic shorthand.andM;The extent of acoustic-phonetic shorthand can be understood from researchperformed by Dennis Klatt [11].andP;  After recording normal continuous speech,Klatt would splice out individual words and play them back to listeners inrandom order.andP;  Typically, listeners could only understand about 70 percent ofthe words.andP;  Yet listeners got 100 percent of the words right when the wordswere played in the correct order.andP;  The conclusion of this experiment is thatit is the grammatical rules, prosodics, and semantic content of a messagethat allows humans to overcome acoustic ambiguity in at least 30 percent ofthe words in normal conversation.andM;ASR system builders have long been aware that semantic and syntacticknowledge must be integrated with acoustic phonetic sources to deal with mostforms of continuous speech.andP;  Integrating diverse knowledge sources has beenat the heart of DARPA-sponsored ASR research for nearly 20 years.andP;  An earlysolution was put forth in the blackboard model of HEARSAY [7] developed atCMU (circa 1974) which simply states that all knowledge sources (KSs) shouldwork in parallel.andP;  In addition, KSs should exchange results via a commonmemory scheme or blackboard.andP;  This solution, while general enough to coverall situations of interest, does not actually guide the integration of KSs.andO;The HEARSAY model was eclipsed by a system based on Hidden Markov Models(HMMs) that implicitly addressed the issue of combining scores from differentKSs.andM;The technique used with HMM systems for integrating KSs was to representknowledge as finite state networks with transition probabilities betweenstates, with integration being achieved by allowing different KSs toinfluence the transition probabilities.andP;  The HMM technique was not conceivedas a technique for integrating KSs, but rather as a technique for applyingsound stochastic modeling techniques to the problem of decoding strings ofelemental sound symbols.andP;  However, HMM techniques did succeed at integratinginformation from at least three KSs.andP;  The KSs wereandM;1.andP;  phone temporal duration,andM;2.andP;  phone level elemental sound similarity, measurements and,andM;3.andP;  word pair statistics.andM;The most notable and recent of these HMM systems was Sphinx [12] (circa1988)--the world's first speaker-independent, continuous speech recognitionsystem operating on a large vocabulary (1,000 words or more).andP;  Today, the HMMapproach is the most widely used pattern-matching technique in ASR systemsworldwide.andM;Despite the widespread use of the HMM approach, it has not had much impact onNLU or integration of NLU techniques into ASR systems.andP;  In fact the potentialof HMM methodology to guide integration of diverse KSs is not at all obvious.andO;It was first mentioned by Jim Baker in his doctoral thesis [3] which was alsothe first published explanation of HMM applied to ASR.andP;  Unfortunately, theBaker thesis only suggested that the HMM approach might apply to NLU.andM;In fact, it appears that it is not the hidden Markov process itself that isrelevant; rather it is the state machine representation, and the methodologyof processing the time evolution of the state machine that are relevant.andM;The Nature ofandM;UnderstandingandM;Computerized understanding of any subject raises philosophical issues.andP;  Weoperationally avoid these issues by defining understanding as the ability torespond appropriately to directives or queries in normal human language basedsolely on information we already possess.andM;NLU by machines is the decoding of messages encoded with the symbols andconventions that humans uses among themselves.andP;  Extensive use of context isrequired since human communication is so extensively conditioned by the worldmodels of the sender and receivers of messages.andP;  If the communication iswritten, the symbols are restricted to text and a few punctuation andhigh-lighting symbols.andP;  Speech, on the other hand, contains more symbolcarriers, namely pitch, volume and segment duration which together are saidto supply prosodic information.andP;  To understand the conventions for impartingmeaning to sequences of utterances, we need to understand the roles ofsyntax, pragmatics, and semantics.andP;  These are cornerstone issues in humancommunication.andM;Communication is the exchange of messages that described the state of a modelor a change in state of a model.andP;  These models are typically mental modelsbut they can be computer models equally well.andP;  The messages are serialencodings that serve to compress information and distribute it over orderedsets of symbols.andP;  Serial encoding, and the possibilities for contextualinformation thus created are powerful information compression methodologies.andO;They are fundamental not only to efficient communication, but probably tointelligence itself.andM;Contextual information is the local environment provided by nearby symbolsthat literally redefine a symbol.andP;  The fact that any given symbol can takedifferent meanings depending on context means that communication can proceedwith fewer symbols that would otherwise be needed.andM;To appreciate the extraordinary power of contextual information to buildmeaning into simple sequences, consider two extreme forms, providing the mostpowerful compression known: information encoding in chromosomes and fractals.andM;Chromosomes in the germ cells in no way specify directly how many cells ofany sort should appear in an adult being.andP;  Rather, they specify how each cellshould react to differing contexts provided by other cells through chemicaland electrical interchanges as the body matures.andP;  The collective behavior isstriking as new cell characteristics are created according to the sequencesof cell development.andP;  The full message encoded in a chromosome cannot be readdirectly, but must be allowed to create all the intermediate contexts inorder to give expression to the final form.andM;Fractals [8] encode information in much the same way.andP;  As pointed out inScientific American [13], factals provide the most powerful techniques forpicture data compression invented to date.andP;  They work by creating irregulargeometric shapes that reproduce themselves on smaller and smaller scales,using the &quot;context&quot; of their previous larger structures to guide the smallerones.andM;These are examples of extreme data compression using recursive contextualinformation encoding, in which context begets new context, which begets newcontext, etc.andP;  While not directly applicable to natural languageunderstanding, such examples serve to focus attention on the data compressionaspect of contextual information processing and the extraordinary power ofcontext to reveal information to decoders with sufficient memory to utilizeit.andM;If spreading information over sequences makes an efficient coding scheme, wecould argue on principles of least effort that it would be used widely inhuman communication.andP;  And we see empirically that context is everywhere inhuman discourse.andM;Returning to the core issues of syntax, pragmatics, and semantics, it wouldappear that they address different levels of contextual information encodingprovided by strings of words.andM;From among these three KSs, the shortest strings and most local context isthe realm of language syntax.andP;  Syntax rules are typically concerned only withphrases and sentences rather than information spreas over paragraphs orbooks.andM;Pragmatic and semantic models deal with larger contexts than encoded inisolated sentences.andP;  They generally cover encodings that span severalsentences.andM;Pragmatic models reflect the intermediate states of mind that might be put inplace by the progress of discourse or exposition.andM;Semantic models impart meaning to messages.andP;  Semantic models are the objectthat messages describe the state of or the changes in.andP;  They are also calledworld models and are the ultimate repository of intelligence in a system.andO;This is the tru frontier of NLU and artificial intelligence in general, whichwill properly be the focus of research for years to come.andM;The underlying notion behind NLU semantic models is that meaning can bederived from models of the domain of discourse rather than exhaustiveenumeration of word sequences.andM;This reduces the problem of recognizing word strings, from a possiblyinfinite number of variations, to a problem of paraphrasing or transforming afinite number of manipulations that can be performed on finite world models.andM;In other words, there may be an infinite number of ways of saying the samething; and the way to handle this situation is to encode the basicinformation once and use procedures to generate the different ways of sayingit.andP;  Furthermore, in many domains of discourse, the basic underlyinginformation may be small and represented well ina semantic model with arelatively small number of facts and properties.andP;  In this case, the problemof modeling human discourse is largely an issue of finding the right semanticmodels and combining them with the right linguistic transformation rules.andM;Roger Schank's [13] suggestion that the world of normal everyday discoursemight be reduced to a few hundred generic situational models with 30 or sounderlying actions that might be performed on the models, is another example.andO;Sentences that could be parsed were interpreted as a paraphrase of thesebasic actions and the paraphrase was to be guided by conceptual dependencynetworks.andM;Noam Chomsky's [6] famous transformational grammars which transform a finiteset of basic sentence structures, (deep structures), into an infinite numberof ways of actually composing sentences (surface structures) provide anotherexample.andP;  Semantics are encoded at the deep structure level.andO;Transformational grammars play the role of conceptual dependency networks bytranslating underlying information embodied in the deep structures into theexpressive forms actually found in normal human communication.andM;In summary, it is not the goal of NLU to prerecord all reasonable statementsthat might be made about any nontrivial subject and attach intended meanings.andO;The goal is to create semantic models from which meanings may be generatedand then to develop pragmatic and syntactic encoder/decoders (and prosodicencoder/decoders for speech) to spread the meaning over a serial stream ofwords.andM;The Nature of SpeechandM;Speech is remarkable for the variety of rules it follows and even moreremarkable for the rules it violates.andP;  Written sentences may be expected toobey most grammatical rules most of the time.andP;  However, spontaneous speechhas at least some grammatical probabilistic errors most of the time; [4] itcontains false starts, dangling phrases, mixed cases, mixed tenses, etc.andP;  Ihave a foreign friend who simply leaves out verbs and nouns at random:&quot;Children making kites.andP;  All day running in the park.andP;  Hot.andP;  Tired.andP;  Sweaty.andO;We going home now.&quot;andP;  Makes perfect sense, doesn't it?andP;  But it certainly isnot grammatically correct.andM;In his scholarly book, &quot;On Human Communication&quot; Colin Cherry [5] points outthat we can strip off all grammatical clues and still communicate (p122):&quot;woman, street, crowd, traffic, noise, thief, bag, loss, scream, police...andM;.&quot;While spontaneous speech may not be as &quot;nongrammatical&quot; as this, it stillviolates many of the rules on which typical NLU systems are based.andP;  On theother hand, most users of computerized NLU systems, whose goal is presumablyto help the dumb machine understand, could be expected to obey most of therules most of the time.andP;  Since most talkers obey many more grammatical rulesthan they disobey, syntactic information is certainly present.andP;  When thespeakers fail to be perfect, however, it must not stop the NLU processes,just slow them down.andM;N NLU systems need to extract meaning from partially completed parses (i.e.andO;when the parse has gone as far as it can and it is not yet finished); theymust shed light on the potential completions and what each would mean ifcompleted.andP;  At the very least, this will require a change from deterministicto a probabilistic parsing strategy.andP;  In fact, probabilistic parsing isessential and one of the more important messages of this article.andM;NLU systems use many grammars for parsing (e.g.andP;  transformational, ATN,chart, bottom up, top down, semantic, conceptual dependency) and most may begracefully applied to imperfections of real speech, ...but only if parsing ismodified to be probabilistic.andP;  In other words, decisions to follow particularbranches in a parse tree must be treated as tentative rather than final (i.e.andO;the decision to pursue any particular branch must be handled as aprobabilistic decision); and provisions must be incorporated to retrace stepsto follow alternative branches if the probability of the processed branchgrows too low.andP;  Pioneering work in this area has been done by Seneff[15].andO;Such search strategies are well developed in the world of information-codingtheory and automatic speech recognition.andP;  An appropriate approach for NLU isprobably stack decoding [10].andM;While it might be universally agreed that probabilistic parsing is a greatgoal and that ASR systems would be more accurate if they incorporatedsyntactic, prosodic, pragmatic, and semantic knowledge sources, there islittle agreement on how to encode this knowledge and how to apply it.andO;However, it is clear that a uniform method for representing and communicatingbetween syntactic, prosodic, pragmatic, and semantic knowledge sources wouldhelp to promote information sharing between these different levels.andM;A proposal was put forth by Jim Baker [3] to provide a uniform representationin the form of &quot;finite state machines.&quot;andP;  The idea was to represent allknowledge sources (semantic, prosodic, pragmatic, syntatic, phonemic andphonologic) as networks (or finite state machine equivalents) and then todetermine what states are most probable using the mathematics of the HMMapproach.andP;  The approach has been applied with great success to the lowersources of knowledge, namely phonologic, phonemic, and word order syntaxbased on simple word pair statistics.andM;Figures 2 thru 7 show popular finite state machine models associated with theHMM approach.andP;  Figure 2 shows how finite state machines might be used in anoperational ASR system.andP;  This description is applied to the current leadingASR systems worldwide and is similar to the one described in Kai Fu Lee'spaper on Sphinx [12].andM;It is important to note that figures 2 through 7 do not directly address theuse of NLU techniques.andP;  They do not address semantic, prosodic, or pragmaticknowledge sources.andP;  Instead, they only show one way to integrate the lowersources of knowledge (phone to phoneme, phoneme to word, and word to wordsyntax).andP;  This integration has been implemented by embedding the lowersources explicitly within higher ones (see Figure 2).andM;Unfortunately Baker's thesis did not actually combine higher sources ofknowledge (semantic, pragmatic and prosodic) nor did it explain how to do itbeyond suggesting that finite state machines and the HMM formalism should behelpful.andP;  It is only now, 15 years later, that we have enough experience withstate machines to appreciate why they might be useful for integrating highersources of knowledge.andM;The main purpose of this article is to explore the elements of current HMMASR methodology and show how they might be extended to encompass theintegration of NLU with ASR.andP;  These techniques have not been put forth tosolve this problem before but they are nonetheless well known techniques.andM;The main thesis of this article is, then, that NLU knowledge sources can andshould be integrated with ASR sources by: first, developing probabilisticstate machine networks for semantic, pragmatic and prosodic types ofknowledge; and then developing a scheme for interconnecting differentknowledge domains.andM;Once this is done, the time evolution of the probabilities of states can betreated in the same way that the lower model states are treated.andP;  We can seefrom Figure 2, that one way is to simply embed some finite state modelswithin the states of other finite state models.andP;  We seek a more generalanswer here.andM;The state machines shown in Figure 11 are symbolic structures used simply toillustrate how conditional probabilities increase as contextual scopechanges.andP;  The bottom state machine with generic transition probability PT(j,i|X,C1,C2,C3,C4) says that the probability of transition between elementalsound units is conditioned by (dependent on) the states occupied at thesyntactic (C4), prosodic (C3), pragmatic (C2), and semantic (C1) levels.andP;  Inpractice, the different domains may be treated as independent.andP;  For example:PT(j,i|X,C1,C2,C3,C4)=PT(i,j|X, C4)*P(C4||C3)*P(C3|C2)*P(C2|C1) whereP(Ci|Cj) is the probability of being in state, Ci, in the ith level giventhat the state Cj is occupied in the jth level.andM;Operationally, the conditional probabilities may be interpreted to mean thatlower level models should be trained on data labeled by higher level states,and the same set of transition probabilities so gathered should be usedduring the recognition phase when the specified higher level states areactive.andM;Recall Figure 1 and the hierarchy of contexts provided by task domain,pragmatic, prosodic, syntactic, phonemic, and phonological domains.andP;  Thelower their position in the hierarchy, the more conditional are theprobablities.andP;  More precisely, the longer the time span, the more global thecontext; and domains of lesser extent should be conditioned by the context ofmore global ones.andM;Figures 8, 9, and 10 show higher level knowledge sources represented as statemachines.andP;  Figure 11 summarizes the method for integrating the NLU and ASRknowledge sources discussed in this article.andM;I believe the correct way to combine NLU and ASR is to specify task domainstates first and then proceed to specify the lower states.andP;  In other words,first determine the different system states that might occur for eachapplication.andP;  Next, determine the questions the user might ask and thecommands the user might give for each state.andP;  This information would beencoded in grammars that are customized for particular situations.andP;  Grammarsare preferred to simple lists of statements, even with paraphrase andmorphological rules to achieve data compression, because grammars take upless memory.andM;The challenge is to assign probabilities to the various statements, and topass these probabilities along to the lower domains of knowledge.andP;  The singlemost fruitful area is likely to be probability assignments to task domainstates and to the sentences that would issue from these states.andP;  Initially,the key statistic may be PT4(i,j|X,C1)*P(C4|C1)(in place of the currentprobablities that are used by ASR systems that would correspond to PT4(i,j|X)).andP;  Later, it will be possible to add prosodic and pragmaticknowledge sources when they are better understood.andM;ConclusionandM;I have noted that NLU as a research discipline has not given greatconsideration to the needs of speech recognition per se.andP;  However, it iswidely believed that NLU techniques will someday significantly improveperformance of ASR systems.andP;  An obstacle to fruitful collaboration betweenASR and NLU research areas has been the lack of a good theory combiningimformation from different knowledge sources, a fundamental question inartificial intelligence research.andP;  I have a tentative answer to thisquestion.andM;The solution I propose requires that knowledge be represented as networks ofstates.andP;  Communication as the time evolution of a network of states isenvisioned.andP;  The time evolution is treated as a first order Markov process ina state machine, (i.e., the states are linked together with transitionprobabilities with new state probabilities being computed from old stateprobabilities multiplied times the transition probabilities...with summationover all products that connect old states to new ones).andP;  A significantelement in the proposal is that the transition probabilities be conditionedboth by the states (context) of more global state machines and also by thenew acoustic information driving the communication.andM;This is not a simple extension of the HMM approach.andP;  There is no need totreat a hidden process in the model proposed here.andP;  On the other hand, mostof the background work that has a direct bearing on this approach comes fromthe literature on HMM.andP;  It is expected that readers will want to refer to theHMM literature to see specific examples of the use of probability to combineknowledge surces.andM;It should be noted that knowledge sources at different levels may beprocessed asynchronously, in parallel, instead of serially.andP;  This hasimportant consequences for improving the computational underpinnings formachine intelligence in general since parallel processing avoids the VonNeumann bottleneck by promoting the use of multiple hardware processors toincrease the power of computing hardware.andM;I believe the approach described in this article will be immediatelyapplicable to natural language understanding for well defined domains ofdiscourse with limited vocabularies, and a manageable set of semanticalternatives that are characteristic of the task domains within text editors,page layout programs, spreadsheets, telecommunications systems, anddatabases.andP;  These are the areas of practical interest to personal computermanufacturers and significant progress can be expected in these areas.andP;  Thetime has come in the evolution of our society for us to start talking to ourcomputers.andP;  The NLU community can play a key role in ushering in this newmodality, possibly by using the techniques suggested.andM;ReferencesandM;[1] Allen, J. Natural Language Understanding.andP;  The Benjamin CummingsPublishing Company, Inc., 1988.andM;[2] Jim Baker.andP;  Dragon Dictate.andP;  Speech Tech.andP;  J., Media Dimensions, NewYork, N.Y., (Spring 1989), 20-25.andM;[3] Baker, J., Stochastic modeling for speech recognition.andP;  Doctoral thesis,Dept.andP;  of Computer Science Carnegie Mellon University, Pittsburgh, PA 1979.andM;[4] Bodmer, F. The Loom of Language.andP;  W.W.andP;  Norton andamp; Company, New York, 1944.andM;[5] Cherry C. On Human Communication.andP;  MIT Press, Cambridge, Mass.andP;  April1975.andM;[6] Chomsky, N., Aspects of the Theory of Syntax.andP;  MIT Press, Cambridge,Mass., 1965.andM;[7] Erman, L.D., Lesser, V.R.andP;  Trends in Speech Recognition.andP;  W. Lea, Ed.andO;Prentice Hall, Englewood Cliffs, NJ, 1980, pp.andP;  361-381.andM;[8] Feder, J. Fractals.andP;  Plenum Press, New York and London, 1988.andM;[9] Harvey, Hendrix, G. Mastering Qandamp;A.andP;  Sybex, 1988.andM;[10] Jelinek, F. Design of a Linguistic Statistical Decoder for theRecognition of Continuous Speech, Automatic Speech and Speaker Recognition,Dixon and Martin, IEEE Press, 1978, 231-237.andM;[11] Klatt, D. personal communication.andM;[12] Lee, K.F., Reddy, R., Hsiao-Wuen Hon.andP;  An overview of the SPHINX speechrecognition system.andP;  IEEE Trans.andP;  Acous.andP;  Speech, and Sig.andP;  Process.andP;  38, 1,(January 1990), 34-35.andM;[13] Schank, RogerandM;[14] Not just a pretty face; compressing pictures with fractals.andP;  Sci Am.andO;(March, 1990), 77-78.andM;[15] Seneff, S. TINA: A probabilistic syntactic parser for speechunderstanding systems.andP;  Tech.andP;  Rep.andP;  LCS Laboratory for Computer Science,MIT, Cambidge, MA, 1989.andM;[16] Slocum, J. Mach.andP;  Transl.andP;  Syst.andP;  Cambridge Union Press, 1988.andM;[17] Zue, V. et.andP;  al.andP;  The voyager speech understanding system: Preliminarydevelopment and evaluation.andP;  In Proceedings of ICASSP 90.andM;GEORGE M. WHITE began research in Automatic Speech Recognition (ASR) as theStanford Artificial Intelligence Project in 1969.andP;  Today, he is manager ofspeech recognition research at Apple Computer.andP;  Author's Present Address:Apple Computer, 20525 Mariani Ave., Cuptertino, CA 95014.andO;</TEXT></DOC>