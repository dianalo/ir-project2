<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO> DOE2-82-0507 </DOCNO><TEXT>The ordering of data acquisitions in many computational problems is anartifact of algorithms developed for serial computers. Often these serialalgorithms are highly parallel and thus are mapped directly onto a parallelprocessing system. This technique, however, does not fully exploit theadditional opportunities provided by the system's parallelism. The designof optimal parallel algorithms requires new and different techniquesand insights. Unfortunately, parallel performance can still be degradedeven with optimal parallel algorithms that preserve the ordering of dataaccess (that is, the data dependences between different computationalcode blocks) by synchronization primitives, such as locks, events, andbarriers. Furthermore, additional decreases in performance are introducedby the various hardware/software components integrated to coordinatethe particular parallel processing system. An alternative approach tothe class of parallel iterative algorithms is to ignore the orderingof data accesses and allow the computational algorithm to execute asynchronously.These algorithms are referred to as chaotic algorithms and provide programmingstrategies that would otherwise be too cumbersome to implement and tooinefficient to execute on a sequential processor. Although chaotic algorithmsare difficult to analyze formally, the comparisons of their observedperformance could lead to improved serial and deterministic (nonchaotic)parallel schemes. This paper will examine several chaotic iteration schemesand, based on their results, offer an alternative scheme that substantiallyimproves the serial time to execute this algorithm. 11 refs., 11 figs.</TEXT></DOC>