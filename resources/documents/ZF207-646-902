<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-646-902  </DOCNO><DOCID>07 646 902.andM;</DOCID><JOURNAL>Communications of the ACM  Sept 1989 v32 n9 p1091(11)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>The future of high performance computers in science andengineering. (new applications and ways of computing with highlyparallel machines) (technical)</TITLE><AUTHOR>Bell, G. Gordon.andM;</AUTHOR><SUMMARY>VLSI and parallelism enabled the research community to create anumber of innovations.andP;  Some progress is the result of theevolution of multiple vector-processing, or supercomputers, andmany accomplishments are the result of DARPA's Strategic ComputingInitiative (SCI).andP;  Many companies are forming to exploit the newtechnology.andP;  Companies that are building high performancecomputers for control and artificial intelligence (AI)applications, and traditional supercomputers for scientific andengineering applications are listed.andP;  Three kinds of computers aredescribed: multiprogrammed general purpose computers, run-timedefined applications-specific computers, and applications-specificcomputers used for one application.andP;  The range of opportunities insupercomputing is examined.andM;</SUMMARY><DESCRIPT>Topic:     Performance ImprovementMultiprocessingFuture of ComputingApplicationsProduct DevelopmentEmployment OpportunitySupercomputers.andO;Feature:   illustrationtable.andO;Caption:   (Simplified taxonomy of computer structures for high performancecomputing). (table)Companies building high performance computers and supercomputers.andO;(table)Comparison of Central Cray YMP and Distributed Titan GraphicsSupercomputer. (table)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>The Future of High Performance Computers in Science and Engineering Spurredby a number of innovations from both industrial and academic researchestablishments made possible by VLSI and parallelism, we can expect the nextgeneration of scientific and engineering computing to be the most diverse andexciting one yet.andP;  Some of the research accomplishments have been stimulatedby DARPA's Strategic Computing Initiative (SCI).andP;  Other progress is a resultof the evolution of understanding the technology of multiplevector-processing computers (i.e., supercomputers).andP;  However, without thescientific base, creative talent, understanding and demanding users, andinfrastructure to design complex VLSI chips, these innovative machines wouldnot be possible.andP;  A variety of established and newly formed companies haveorganized to exploit the new technology.andP;  Table I lists the number ofcompanies building high performance computers for control and artificialintelligence applications, and traditional supercomputers for scientific andengineering applications.andM;The impressive gains in performance and the large number of new companiesdemonstrate the effectiveness of the university-based research establishmentand their ability to transfer technology to both established companies andstart-up firms.andM;Three kinds of computers, distinguished by the way they are used, areemerging:andM;* General purpose computers are multiprogrammed and can handle a variety ofapplications at the same time.andP;  General purpose computers include supers,mainframes, various minis, and workstations.andM;* Run-time defined, applications-specific computers are used on only a fewproblems, on a one-at-a-time basis.andP;  These mono-programmed computers areuseful for a more limited class of problems where a high degree of dataparallelism exists.andP;  This class of computers can achieve an increase inperformance or performance/price of a factor or 10 or so.andM;* Applications-specific computers solve only one problem such as speech orimage processing.andP;  By binding the applications in hardware and software atdesign and construction time, an increase by a factor of 100 to 10,000 inperformance or performance/price is possible.andM;Figure 1 shows the alternative forms of the computer structures beingconsidered in the preceding section.andP;  Figure 2 shows various approaches tobuilding high performance computers and the estimates of performance versustime.andM;MAINLINE SUPERCOMPUTERS FOR SCIENCE ANDandM;ENGINEERINGandM;The main line of scientific and engineering supercomputer development followsthe Cray formula: the fastest clock; a simple pipelined scalar instructionset; a very complex, pipelined vector processing instruction set; andmultiprocessors.andP;  Software is evolving to automatically vectorize andparallelize so that the increase in the number of processors is oftentransparent to users.andM;The supercomputer has been defined three ways by supercomputer architects andengineers [11]:andM;1.andP;  The fastest machine with the most memory available.andM;2.andP;  Ability to solve numerically intense problems.andP;  Such a computer istypically characterized using the Livermore Loops or Linpack benchmarkexecution rates measured in floating point operations per second.andM;3.andP;  The fastest computer in a particular, named price class with both thehighest scalar and vector processing performance.andM;The three classes of supercomputers are also characterized by price: The Crayor &quot;true&quot; supercomputer at $1 million to $20 million; the mini-supercomputerat $100,000 to $1 million; and the graphics supercomputer at $100,000.andM;In 1989 personal supers costing between $15,000 and $50,000 will beintroduced.andP;  Such machines are based on Intel's 80860 microprocessor, forexample, which operates at 40 Mhz and delivers roughly 10 megaflops on theLinpack benchmark.andM;In this article, a supercomputer is defined as a computer that:andM;* Is used for numerically intense computations.andM;* Has the highest scalar and vector processing capacity with the appropriateprimary and secondary memory resources.andM;* Falls into one of the supercomputer price classes (i.e., super, mini-super,graphics super, personal super).andM;* Can supply the computational resources (i.e., processing, primary andsecondary memory, graphics, and networking) to a single user or set of usersin one day that could be obtained using the largest shared supercomputer.andM;Tha Japanese computer industry's first supercomputers aimed at having thefastest uniprocessors as demonstrated by NEC's SX-2, Fujitsu's VP series, andHitachi's S-820-80.andP;  These uniprocessors provide very fast single streamexecution, but the Cray XMP still has more aggregate throughput.andP;  Through itsparallelizing compiler, the Cray YMP is the world's fastest supercomputer[6], and is likely to remain so until the Japanese start buildingmultiprocessors.andP;  In April 1989 NEC introduced the SX-3, which it believeswill be the world's fastest computer.andP;  Scheduled for delivery in 1990, it israted at 22 gigaflops using four processors, each operating at 5.5 gigaflops,and a 2.9 ns clock.andM;In 1990, Cray Research plans to introduce the Cray 3 (16 processors at twicethe speed of the Cray 2), to be followed in 1992 by the Cray 4.andP;  The Cray 4operates at a clock rate of 1 ns and delivers 128 gigaflops, using 64processors.andP;  Figure 3 shows the evolution of clock speed, number ofprocessors, and aggregate computing power for Cray computers over a 30-yearperiod beginning with the CDC 6600.andM;Getting peak power on a single program will require the user to do somereprogramming to exploit the multiple processors.andP;  Each year one Gordon BellPrize acknowledges the fastest program execution on a traditionalsupercomputer.andP;  In 1988 the prize went to the Long Range Global WeatherModelling Program at NCAR, which was parallelized manually and operates atover 400 megaflops (versus a peak of 840 megaflops) on the Cray 416.andP;  Giventhe relatively small memory and the growth in application program size,researchers are beginning to use the XMP in parallel mode to take fulladvantage of its processors.andP;  In 1989, the award went to Vu, Simon, Ashcraft,Grimes, Lewis, and Peyton [5], who used an 8-processor Cray YMP operating at1.68 gigaflops (versus a peak of 2.7 gigaflops, or 5.5 times the uniprocessorrate) to solve a large statistics problem using a finite element model.andM;DISTRIBUTED SUPERCOMPUTINGandM;Given the interest in supercomputing in government, academe, and industry itis worth looking at the range of opportunities for different styles ofcomputing from the highly centralized or regional supercomputer to a fullydistributed computing environment using personal supercomputers.andP;  Just asdistributed computing using LAN-interconnected workstations and personalcomputers has become an alternative to mainframes, fully distributed personalsupercomputing is likely to play a similar role with respect to traditionalsupercomputers.andP;  However, since the central approach is so stronglysubsidized by providing &quot;free&quot; supercomputer time to the scientificcommunity, it is unlikely that distributed supercomputing will evolve quiteas rapidly.andM;Supercomputing PolicyandM;With the entry of the Japanese into the supercomputer market, supercomputinghas become an issue of national pride and a symbol of technology leadership.andO;While Japan now builds the fastest uniprocessors, the multiprocessor approachprovides more throughput, and with parallelization, more peak power.andP;  Table Iindicates that a large number of new companies have started up to build highperformance computers since 1983.andP;  Both Steve Chen, formerly of CrayResearch, and Burton Smith, formerly of Dennelcor, have started suchcompanies.andP;  A recent report by the Office of Science and Technology Policyurges the adoption of a government initiative for high performance computing,including a National Research Network [7].andM;When the Cray 1, the first modern supercomputer, appeared, university usersand researchers were using a highly distributed approach using the VAX 780.andO;Several thousand VAX 7xx-series computers were running scientific andengineering applications., and before the Cray XMP was introduced, only oneCray 1 was available at a university.andP;  The run-time for the VAX could be upas much as a factor of 100 times the Cray for a stretch factor of 100.andP;  Theacquisition costs for processing, measured in millions of floating pointoperations per second per dollar, were about the same.andP;  VAX allowedcomparatively large programs to be run easily using its large virtual memory.andM;With the emergence of the Cray XMP in 1983, the performance, capacity, andcost-effectiveness of supercomputers was increased by factors of 1.5, 6, and3, respectively.andP;  New VAX computers with adequate power and performance/pricewere not introduced to compete with the Cray XMP.andP;  Only in 1984, after theXMP was already established, did mini-supercomputers with comparableperformance/price emerge.andP;  By 1984 a strong national initiative based on theNSF Bardon and Curtis report had begun [2].andP;  It resulted in the establishmentof the NSF's Advanced Scientific Computing (ASC) and its five NationalComputing Centers.andP;  The centers are now completely institutionalized into theNSF infrastructure at a budget level of approximately $60 million in FY1989.andO;The centers and the 5 to 10 percent of NSF researchers who use them have madea very strong case for government-funded supercomputing centers.andM;State and federal governments are very receptive to large centers.andP;  Severalstates are building supercomputer centers to serve their university systems.andO;Similarly, many corporations believe a supercomputer is required forcompetitiveness.andP;  For example, Toyota has more supercomputer power than allof the U.S.andP;  automotive industry.andP;  At least one U.S.andP;  research laboratorybelieves that a supercomputer is essential for recruiting.andM;Performance and Cost-EffectivenessandM;There appears to be no economy of scale across the supercomputer classes.andO;Lower cost, higher production quantity components (processor, memory,peripherals, etc.) result in an inherent dis-economy of scale in performance/price for smaller-scale applications.andP;  This is demonstrated in acomparison between the Cray YMP and Ardent's Titan Graphic Supercomputer,both of which were introduced in 1988.andP;  On the other hand, having a largecentral facility is critical for certain large programs and databases.andM;Table II gives the purchase price, performance, and performance/price forseveral benchmarks run both sequentially and in parallel for the twoapproaches.andP;  Observe that for the purchase price of the YMP, one could have166 graphics supercomputers in a highly distributed fashion for project,departmental, and even personal use.andP;  The comparison ignores operating costs,which in the case of a central computer are quite visible.andP;  In thedistributed approach operations costs (e.g., file backup) are buried in theorganization.andP;  Similarly the cost of support, purchasing, and maintainingsoftware, and maintaining files may vary using the two approaches.andO;Well-funded, large centers provide a complete and balanced set of facilitiesincluding large primary memories of gigawords as well as being able to handletens or even hundreds of gigabyte files, and archival files for collaborativescience.andP;  However, few centers operate at this level.andP;  The &quot;right&quot;environment for a user depends on need.andP;  Ideally, most users would have theirown distributed, cost-effective personal supercomputers for smallerapplications with extensive, 3-D graphics and access to centralsupercomputers via fast networks for running large community programs andaccessing large databases.andM;Obviously, all of the benchmarks run longer on the slower machine.andP;  Thestretch factor is the increased time that a program runs using the Titan ascompared to run-time on the Cray YMP.andP;  Also associated with each benchmark isthe cost-effectiveness or performance/price (e.g., megaflops/second/dollar)of the YMP versus the Titan.andP;  The range of results is comparable with ananalysis of Titan and the Cray XMP by Misak and Lue at the Institute ofSupercomputer Research [8] where for scalar and vector loops, the Cray wasfaster by about a factor of 5 and 10, respectively.andP;  The Whetstone benchmarkis indicative of such use.andP;  For simple integer-oriented benchmarks like thoseencountered in editing, compiling, and running operating systems, the YMP isill-suited since it is about the speed of the Titan, indicating that whilethe YMP is still faster for utility programs, it is not cost-effective byover an order of magnitude.andM;At first glance, the small Linpack case seems irrelevant to supercomputing.andO;However, the average speed which the Cray XMPs run at various large computercenters has in the past been equal to about 25 megaflops/second, or the speedof the Linpack 100 X 100 prior to Cray's recent compiler improvements.andP;  Notethat for a single processor, it takes 12 times longer to get the same amountof work done on the distributed approach.andP;  However, the distributed approachis almost three times more cost effective or in principle, users spending thesame amount could get three times as much computing done.andM;By automatically parallelizing Linpack even for the small case, the Cray YMPruns about 2.5 times faster using the eight processors and has once againbecome the world's fastest computer.andP;  Since the small Linpack benchmark istoo small to run efficiently in parallel, the cost-effectiveness of theapproach decreases over a factor of three.andP;  Since the Titan has only tworelatively slower processors, the effect of parallelization is not as greaton cost-effectiveness.andP;  Stretch times of around 10 to 20 for the distributed,dedicated approach mean that even large users can get about the same amountof work done as with a centralized approach.andP;  Very large projects using aCray center get only a few processor hours per day or about 1,000 hours peryear, while large users get an hour a day, and average users an hour a week.andM;By using the peak speeds that are obtained only by running each of theprocessors at peak speed and in parallel, the difference in speed between theCray YMP and the Titan is finally apparent.andP;  While the times stretch toalmost 90 (i.e., to do an hour of computing on the YMP requires almost 90hours on the Titan), the cost-effectiveness of the Titan still remains, butonly by a factor of two.andP;  Thus, we see the importance of parallelization toincrease speed on a super in order to provide significantly more power than auser could get in a single day on a personal super.andM;Finally, using the graphic supercomputer, visualization is implicit in thesystem since each computer has a significant amount of power to render anddisplay data.andP;  Modern supercomputing requires additional resources such asgraphic supercomputers or high performance workstations just to handle thedisplay of computed data.andP;  Current networks operating at T1 (1.5 megabits persecond) are inadequate for applications requiring extensive computation andvisualization such as molecular modelling, computational fluid dynamics,finite element modeling, and volume data visualization [4].andP;  Futuresupercomputers may have embedded rendering hardware to provide bothcost-effective and truly interactive graphics supercomputing.andM;FUELING HIGH PERFORMANCE COMPUTERSandM;The future of the conventional uniprocessor appears to be a simple RISC-basedarchitecture enabling high speed execution and low cost, one-chip (i.e.,andO;microprocessor) implementation.andP;  Such a processor is able to tracksemiconductor process technology and is inherently faster than a multiplechip approach.andP;  Current microprocessor performance is comparable to that ofthe IBM 3090 mainframe, which has historically evolved at 14 percent peryear.andP;  By the end of 1989, the performance of the RISC, one-chipmicroprocessor should surpass and remain ahead of any available minicomputeror mainframe for nearly every significant benchmark and computationalworkload.andP;  By using ECL gate arrays, it is relatively easy to buildprocessors that operate at 200 Mhz (5 ns clock) by 1990.andP;  One such company,Key Computer (recently purchased by Amdahl Corporation), is doing just thisto build very fast uni- and multi-processors.andP;  Prisma is building aGaAs-based computer using the Sun Sparc architecture.andP;  By adding an attachedvector processor, users can see a very bright picture for scientific andengineering computation in workstations and simple computers.andM;The projected evolution of the leading edge one-chip POPs (plain oldprocessors) using the RISC approach is given in Table III.andP;  Unlike thehistorical leading edge clock evolution, POP clock speed has evolved a factorof 5 in four years (1.5 per year) because the processor is on a single chip.andO;Shifting to ECL can give an aggregate speed-up of a factor of 20 over asix-year period (1.65 per year).andM;USING LOW-COST, FAST RISC MICROSandM;The very fast CMOS and soon-to-come ECL microprocessor will compete withevery other computer and be a good component for both multiprocessors andmulticomputers.andP;  Also, the micros can be used in a redundant fashion toincrease reliability and build what is fundamentally a hardware fault-freecomputer.andM;MultiprocessorsandM;The next generation, high-performance micros are designed for buildingmultiple microprocessor computers, or &quot;multis&quot; [3].andP;  Thousands of multis arenow in operation, and will probably become the mainline for traditionaltime-shared computers and smaller workstations for the next decade.andP;  However,given the speed and simplicity of POPs, users may ask Why bother with so muchperformance?andP;  By 1990 workstations with four to ten 20-mips processorsattached to a shared bus in a multi configuration that sells for under $50,000 may exist.andP;  With only a small incremental design effort, all computerscan be implemented as multis; however, the 50-mips microprocessor will placemuch pressure on the viability of the multiprocessor.andM;The utility of the multiprocessor as a general purpose device is provenbecause it can be used in a multiprogrammed and time-shared fashion.andP;  It hasalso been the object of training and research in parallel processing by thecomputer science community because it provides the most general purpose toolin that it can provide an environment for a number of computational models.andM;The Alliant and Convex mini-supercomputers, and Ardent and Stellar graphicssupercomputers use Cray-style multiprocessor designs, for general purposescientific and engineering computation.andP;  Other approaches to largemultiprocessors do not have vector facilities, and hence may not be viable orperformance/price competitive for highly vectorized applications sinceautomatic compilation or parallel constructs to use a large number of scalarprocessors don't appear to offer the speed of the vector approach for theseapplications.andP;  Furthermore, it is difficult to build very largemultiprocessors as cheaply.andP;  Hence, multicomputers have been demonstrated andhave gained utility for user-explicit, parallel processing of single problemsat supercomputer speeds.andM;The most ambitious multiprocessor being introduced in 1990 for bothscientific and transaction processing is by Kendall Square Research.andP;  Each64-bit processor operates at 40 megaflops and has an associated primarymemory of 32 megabytes and and 80 megabyte/second I/O channel.andP;  The combinedsystem with 480 processors operates at 19.2 gigaflops with 16 gigabytes ofmemory.andM;Given the ease of building very high performance multiprocessors based onPOPs, a major transition is likely to occur in the traditional mainframe andminicomputer industries.andP;  Using this multiprocessor approach, severalcomputers that execute programs at over 100 million instructions per secondand cost less than $500,000 have entered the market.andP;  Such a structureprovides 2 to 4 times the power of IBM's largest mainframe at 1/20th itscost.andP;  The transition is predicated on the fact that the performance/pricediscontinuity will cause users to consider switching new and even existingprograms to the new micro-based environment and away from mainframes andminis on what is used as a &quot;code museum&quot; to run existing programs.andM;Hypercube-Connected MulticomputersandM;In the early 1980s, Seitz and his colleagues at Caltech developed a large,multicomputer structure known as the hypercube.andP;  By using commoditymicroprocessors, each with private memory to form a computer, andinterconnecting them in a hypercube, grid, or switching network, eachcomputer can pass messages to all other computers.andP;  Today's multicomputersprovide substantially higher performance using faster interconnectionnetworks for message passing than their first generation ancestors, makingthem more widely applicable.andP;  For a particular application, a factor of 10 inperformance/price over mainline supercomputers has been observed.andM;Multicomputers are not generally applicable to all problems and are usuallymono-programmed since they are used on only one program at a time.andO;Hypercube-connected computers now exist with 32-1024 computers that are beingmanufactured by about a half dozen companies.andP;  Several hundred are currentlyin use.andP;  Programs have to be rewritten to use the multi-computer messagepassing system, but the peak performance (see Table IV) and price performanceappears to be worth the effort, as a lab can have its own Cray for aparticular problem.andM;The European multicomputer counterpart to the hypercube is based on Inmos'Transputer computer node.andP;  A transputer is a processor with a small on-chipmemory and four, full duplex interconnection ports operating at 20megabits/second which are used to pass messages to other transputers,especially when they are interconnected in a grid.andP;  Several companies arebuilding general purpose, multicomputers by connecting a large number oftransputers together.andP;  The transputer is proving especially useful in systemsto build application-specific systems for everything from communications torobots.andM;Over a quarter of a billion dollars has been spent to build the 15 presentgeneration multicomputers.andP;  Users have undoubtedly invested a comparableamount in programming.andP;  It is unclear how successful these machines have beenas measured either by establishing healthy, growing, profitable, and hence,sustaining businesses or by providing users with computational power to solveunique problems.andP;  Current systems only approach supercomputer power whilerequiring users to reprogram; thus, users trade-off lower operational costsfor a higher initial investment.andM;Multicomputers can become an established structure only if they provide powerthat is not otherwise available and the performance/price is at least anorder of magnitude cheaper than traditional supercomputers.andP;  The nextgeneration of multicomputers (e.g., NCUBE's 8K nodes operating at 2.4megaflops) with power in the same range as the 20-gigaflop supercomputersavailable in 1990 will be important in establishing multicomputers.andP;  TheDARPA-funded Intel multicomputer with 2K nodes that operate at 80 megaflopseach is equally important.andM;NEW RESEARCH MACHINESandM;The following machines have emerged from DARPA's Strategic ComputingInitiative (SCI) or other research in basic computer science.andM;Systolic ProcessorsandM;Kung's work at Carnegie Mellon University on systolic arrays is beginning topay off and arrays are being applied to a variety of signal and imageprocessing tasks in military applications.andP;  The 10-cell WARP operates at anaverage of 50 percent peak for the problems of interest (e.g., speech andsignal processing).andP;  This provides 50 megaflops for $350,000 (142flops/second/dollar) and is available from General Electric.andP;  Intel isbuilding a single systolic processing chip, iWARP, that's capable ofoperating at a 24-megaflop rate.andP;  Such a chip would be an ideal component ina PC for vector processing in the 1991 time frame.andP;  Using the chip, a smallboard could compute at roughly 100 megaflops.andP;  It would not be unreasonableto expect this component to sell for $10,000 or provide the user with 10,000flops/second/dollar.andP;  While the initial product was developed for a specialpurpose, the ability the use the WARP for a range of applications isimproving with better understanding of the compilation process.andM;Text Searching MachinesandM;Several companies have built specialized text and database machines.andP;  Hollaarat Utah has built and field tested machines for very high speed, largedatabase text searches.andP;  The result to data is that inquiries are processedseveral hundred times faster than on existing mainframes, and the improvementincreases with the size of the database since pipelined searching hardware isadded with each disk.andM;The Connection MachineandM;The Connection Machine grew out of a research effort at MIT by Danny Hillis,which resulted in the establishment of Thinking Machines Corporation.andO;Several machines are installed in a variety of applications that requireparallel computation, including text searching, image processing, circuit andlogic simulation, computational fluid dynamics, and finite element methodcomputation.andP;  The current CM 2 model is a uniprocessor with 64K processingelements.andP;  It has up to 1/2 gigabytes of primary memory, and operates atspeeds up to 10 giga-floating point operations per second.andP;  Thus the CM 2 isthe supercomputer for a number of applications.andP;  While the Connection Machineoperates on one problem at a time in a mono-programmed fashion, it should beable to be multi-programmed.andM;Evolution of the Array ProcessorandM;Multiflow Corp. was started up based on Fisher's work at Yale on a compilerto schedule a number of parallel (7 to 28) execution units using an extrawide instruction word.andP;  In fact, the Multiflow can be looked at as either aSIMD computer with a small number of processing elements, or an extension ofthe traditional array processor, such as the Floating Point Systemscomputers.andP;  Multiflow's first product runs the Linpack benchmark atmini-super speeds and costs half as much.andP;  One feature of this approach isthat a compiler can exploit a substantial amount of parallelism in existing&quot;dusty Fortran decks&quot; automatically.andP;  Cydrome, now deceased, built a similarproduct using ECL technology that provided higher performance and betterperformance/price using a similar architectural approach.andM;EXPERIMENTAL RESEARCH MACHINES TOandM;WATCHandM;Berkeley and Stanford MultiprocessorsandM;Both of these RISC-based, multiprocessor architectures are beginning to comeinto operation.andP;  So far, both have influenced commercial ventures both inRISC and in multiprocessors.andP;  The Stanford project was the prototype for theMIPS Co. chip design.andP;  The Berkeley chip designs were the precursor to Sun'sSparc chips.andP;  Another group at Berkeley has produced a first generationProlog machine that has outperformed the fastest Japanese special fifthgeneration machines.andP;  The next generation Prolog computer is amultiprocessor/multicomputer to exploit both fine grain and message passingfor parallel processing.andP;  Given the rapid increase in speed of POPs, it ishighly unlikely that any computer specialized for a particular language willbe able to keep up.andM;University of Illinois Cedar Multiprocessor ProjectandM;Cedar is aimed at a multiprocessor with up to 32 processors in 4 clusters of8 for executing Fortran in a transparent fashion.andP;  It is based on Alliant'sFX-8.andP;  The prototype will likely operate in 1989.andP;  Future work is aimed atmore and faster processors.andM;Very Large MultiprocessorsandM;Three SCI projects--BBN's Monarch, Encore's Ultramax, and IBM's RP3--allexplore the size and utility of large multiprocessors and provide over 1,000mips in sizes of 1,000 at 1,128 at 16, and 512 at 2 mips performance,respectively.andP;  None have vector processing, and hence may not be used formainline scientific and engineering applications requiring large numbers offloating point operations.andP;  However, the machines and automatic parallelizingcompilers could provide sufficiently large amounts of power to attack newproblems.andM;University of North Carolina's Pixel PlanesandM;This machine is scaleable, highly parallel, SMD architecture that providesthe highest performance for a variety of graphics processing tasks such assolids rendering under varying and complex lighting situations.andM;ATandamp;T's Speech and Signal ProcessorandM;A large number of signal processing computer chips arranged in atree-structured multicomputer configuration provides over 250 gigaflops on32-bit numbers.andP;  The machine fits in a rather small rack, and the resultingnumber of flops/second/dollar is nearly one million.andP;  The machine came, inpart, from Columbia University's tree-structured multicomputer work and ispart of DARPA's SCI.andM;IBM Research GF11 and TF1andM;GF11 was designed specifically for Quantum Chromodynamics calculations and isa SIMD computer providing 11 gigaflops.andP;  TF1 has a goal of achieving 1.5teraflops using 32K 50-megaflop computers connected via a large, centralswitch to pass messages among the computers.andM;Special Algorithm Attached ProcessorsandM;Several computers for special applications in chemistry, computational fluiddynamics, genome sequencing, and astronomy have been built or are indevelopment.andP;  Each provides factors of 100 to 1000 over comparable hardwareusing a general purpose approach.andM;COMPUTER TECHNOLOGY, RESEARCH, ANDandM;TRAINING NEEDSandM;Faster CircuitryandM;University research in high speed circuitry, interconnection, and packagingis virtually nonexistent.andP;  Very high speed processors required much betterinterconnection and packaging density.andM;Mass StorageandM;No radical improvements in size or speed are in progress to keep up with theprocessing developments that come from VLSI.andP;  A project that would couple1,000 one-gigabyte drives in a parallel fashion to provide an essentiallyrandom access to a terabyte of memory and use a variety of specializedarchitectures is feasible.andP;  A project at Berkeley is exploring this approach[10].andP;  Such a system would be useful both as part of the memory hierarchy forthe teraflop computer and as a database where high performance is demanded.andO;The Connection Machine's data vault and Teradata's database computer areexamples of what is possible in restructuring mass storage.andM;VisualizationandM;In order to effectively couple to high performance computers, the scientificuser community has, under NSF sponsorship, recommended a significant researchand development program in visualization [9].andP;  Today's supercomputers arecapable of generating data at video rates.andP;  In order for humans to interpretdata, it appears that the best way is to use direct couple, high performanceconsoles.andP;  Two companies, Ardent and Stellar introduced graphicsupercomputers based on this principle.andP;  Traditional workstation companiesare increasing their computational abilities.andP;  While supercomputers andmini-supercomputers currently rely on LAN-connected workstations, it is morelikely that both structures will evolve to have direct video coupling.andM;Room Area and Campus Area Networks (RAN/CAN)andM;A RAN is needed to replace the various proprietary products and ad hocschemes for interconnecting arrays of computers within the computer room andwithin systems involving high speed computation.andP;  At least three companiesare building links and switches using proprietary protocols to operate in thegigabit range.andP;  An alternative to the Hyperchannel LAN, which operates at apeak of 50 megabits/second, is needed.andP;  The next generation must be a publicstandard.andP;  A combined fiber distributed data interface (FDDI) and anonblocking, public standards-based switch that operates at 100megabits/second using fiber optic seems like a necessary first step thatcould evolve over the next several years.andM;Wide Area NetworksandM;Intermediate speed (45 megabit) and fiber optic (multigigabit/second) switchand network development and research are not occurring rapidly enough forcomputer networking.andP;  The networking dilemma is well-defined [4, 7].andP;  Thesenetworks are badly needed to interconnect the plethora of local area andcampus area networks.andP;  Today, many campuses have installed networks with anaggregate switching need of over 100 megabits per second, which implies anoff-campus traffic need of 20 megabits/second to connect with the 1,000 to2,000 academic, industrial, and government research organizations.andP;  By makinga system that could be used for both computers and communications, the twodisciplines and industries could begin to become synergistic rather thanantagonistic.andM;Memory Address LimitsandM;The address limit of 32 bits on most of today's computers begins to be asevere constraint for every configuration but multiple computers.andP;  Forexample, a solids data set could easily have an array of 1,000 X 1,000 X1,000 elements, requiring a 33-bit address.andM;Data as an Alternative Computational ModelandM;Arvind's group at MIT has progressed to the point where it is building adataflow computer that might outperform the largest supercomputer in problemdomains with a high degree of parallelism and where vector computationtechniques do not apply.andP;  Independent of the computer, a dataflow languagemay be the best for expressing parallelism in ordinary computers.andP;  Again,given the rate of increase in POP's, it is unlikely that a specializedarchitecture will be able to keep up with the main line.andM;Neural Computing NetworksandM;Various efforts aimed at highly parallel architectures that attempt tosimulate the behavior of human processing structures continue to showinteresting results.andM;Changing the Programming ParadigmandM;It is necessary to change the programming paradigm to get the highestperformance from new computer structures.andP;  However, the variety ofprogramming models really isn't very large, given the variety of what wouldappear to be different computers.andP;  Table V summarizes the main line ofcomputational models and the corresponding near-term computer structures thatsupport the model.andM;Other computer structures such as the WARP (a pipelined array of systolicprocessors), neural networks, specialized SIMD computers, and the dataflowcomputer may ultimately require different computational models.andP;  In the longterm, the models in Table V may not be the best or even adequate to expressparallelism.andP;  For now, however, we should build on what we know, learn fromexperience, and research alternatives.andM;THE TERAFLOP COMPUTER BY 1995andM;Two relatively simple and sure paths exist for building a system that coulddeliver on the order of 1 teraflop by 1995.andP;  They are:andM;1.andP;  A 4K node multicomputer with 800 gigaflops peak or a 32K nodemulticomputer with 1.5 teraflops.andM;2.andP;  A Connection Machine with more than one teraflop and several millionprocessing elements.andM;Both types of machine require reprogramming according to either the messagepassing or massively parallel data with a single thread of controlprogramming model.andP;  However, to exploit the evolving supercomputers with 16to 64 processors, users will have to do extensive reprogramming that will usea combination of micro-and multi-tasking and message passing.andM;Today's secondary memories are hardly adequate for such machines.andP;  The lackof multiprogramming ability today may dictate using these machines on onlyone or a few very large jobs at the same time, and hence making the cost perjob quite high, thus diminishing the performance/price advantage.andP;  Based oncurrent applications, it is likely that either approach will be more than 10percent as efficient as a general purpose, multiprogrammed, shared memorycomputer.andP;  The cost of such machines will be comparable to the supercomputerof the 1990s, which is likely to be over $50 million.andM;APPLICATIONS CAN EXPLOIT THE OPPORTUNITYandM;While it is difficult to predict how the vast increase in processing powerwill affect science and engineering generally, its impact in the followingspecific areas is clear.andP;  Given the current level of training, however, it isunclear how rapidly applications will evolve to exploit the enormousopportunity available in significantly faster and more cost-effectivecomputers.andM;Mechanical EngineeringandM;Computers are being used in the design of mechanical structures ranging fromautomobiles to spacecraft, and cover a range of activities from drafting toanalyzing designs (e.g., crash simulation for cars).andP;  Designers can alsorender high quality images and show the objects in motion with video.andP;  Thevast increase in power should provide mechanical engineers with computingpower to enable a significant improvement in mechanical design.andP;  Under thisdesign paradigm, every facet of product design--including the factory toproduce the product--is possible without prototyping.andP;  Within the next decadethe practice of mechanical engineering could be transformed providedcompanies seize the opportunity.andP;  For starters, better product quality couldresult from the new technology, but the big impact comes from drasticallyreduced product gestation periods and the ability to have smallerorganizations, which also contributes to product elegance and quality.andP;  Asimilar change in chip and digital systems design has occurred in the lastdecade, whereby almost any chip or system can be designed and built in abouttwo years.andM;Biochemistry, Chemistry, and MaterialsandM;In molecular modeling and computational chemistry, the design of molecules isnow being done interactively with large-scale computers.andM;Large-Scale Scientific Experiments Based onandM;SimulationandM;A dramatic increase in computational power could make a range of systemsimulation involving many bodies (e.g., galaxys, electron interaction at theatomic level) feasible.andP;  Noble laureate Ken Wilson characterizes computersimulation as the third paradigm of science, supplementing theory andexperimentation.andP;  This paradigm shift will transform every facet of science,engineering, and mathematics.andM;AnimationandM;Large-scale computers can compute realistic scenes, providing an alternativeto traditional techniques for filmmaking.andM;Image ProcessingandM;Various disciplines including radiology rely on the interpretation of highresolution photographs and other signal sources.andP;  By using high performancecomputers, the use of digital images and image processing is finallyfeasible.andP;  The use of satellite image data is transforming everything frommilitary intelligence to urban geography.andM;Personal ComputingandM;Today's large computers will continue to be used to explore applications thatwill be feasible for the PC.andP;  For example, Ardent's graphic supercomputer,Titan, which currently sells for about $100,000 is an excellent model of whatwill be available in 2001 for less than $6,000 (assuming a continued pricedecline at the rate of 20 percent per year).andP;  Every home will have an almostunlimited laboratory to conduct experiments.andM;ROLE OF THE FEDERAL GOVERNMENTandM;Dis-economy of scale continues to exist and favor small machines.andP;  Thissuggests that personal supercomputing, like its counterpart in ordinarycomputing, will migrate to a distributed approach provided decisions are madeon some rational or economic basis.andP;  Large regional computers are difficultto access effectively using todays's limited networks, especially forinteractive visualization.andP;  NSF's Advanced Scientific Computing (ASC) programsuccessfully supplies 5,000+ researchers (representing only a few percent ofthe scientific community) with an average of 1 hour of supercomputing timeper week.andP;  Smaller supers such as mini-supers or graphics supers could easilysupply researchers with the equivalent of 10 to 40 hours per week.andP;  Mostagencies, however, have no means to support smaller, more cost-effectivecomputers unless the prices are at workstation levels that can be supportedby research grants.andP;  By not using small, dedicated computers that are underthe direct control of the researchers who use them, users are deprived of atool that should supply at least an order of magnitude more power than theynow achieve through a shared super.andP;  One could imagine that this kind ofinfusion of processing power, directed at particular experiments could changethe nature of science and engineering at least as much as the current ASCprogram.andM;While highly specialized computers offer the best performance/price andoperate at supercomputer speeds, they cost more than workstations.andP;  ATandamp;T'sspeech processor that carries out 1/4 tera floating point operations persecond on 32-bit data is an excellent example of the gains possible byapplications specific hardware and software.andP;  Again, agencies are unlikely torisk building such specialized computers, nor does the community have theright combination of computer scientists, scientists, and engineers workingto tackle the problem of programming in any general way.andM;For the ultimate performance, SIMD machines such as the Connection Machineappear feasible provided the problem is rich in data parallelism and usersare willing to reformulate and reprogram their applications.andP;  NSF's researchand computing directorates support only traditional supercomputing.andP;  Only noware agencies with the greatest need (DOD, DOE, HHS, and NASA) beginning toexamine the Connection Machine for computation.andP;  The need is clear if my ownthesis is correct about achieving the highest performance.andM;If we want peak speed from any computer, programs will have to be rewrittento some degree to operate in parallel.andP;  One model using message passing wherelarge amounts of data parallelism occur, will work on nearly all highperformance computers including the Connection Machine, multicomputers, andmultiprocessor supercomputers such as the Crays.andP;  The shared memory, micro-and multi-tasking models used by multiprocessors, including the Cray doesn'twork on the multicomputers since the computers don't share the same addressspace.andP;  The recent improvement in the Cray compiler to automaticallyparallelize as indicated in the Linpack benchmarks to bring it nearer thestate of the art is an important development that will begin to allow generaluse and exploit the more rapid increase in available power throughparallelism than through faster clocks.andM;The good news is that a vast array of new, highly parallel machines arebecoming available and that autotasking compilers are evolving to takeadvantage of this in a transparent fashion to a limited degree.andP;  The bad newsis that not all applications can be converted automatically.andP;  Users are notbeing trained to use such machines in an explicit fashion.andP;  No concertedeffort is in place covering the spectrum from training to research.andP;  Thiswill require a redirection of resources.andM;A ROLE FOR THE COMPUTER ANDandM;COMPUTATIONAL SCIENCE COMMUNITIESandM;The computer systems research and engineering community is to becongratulated on the incredible computer performance gains of the last fiveyears.andP;  Now is the time for the rest of the computer science community tobecome involved in using and understanding the plethora of computers that canbe applied to the endless frontier of computational science.andM;The computer science community can continue to ignore applications incomputational science.andP;  Alternatively, it can learn about the various formsof parallelism supported by the evolution of mainline supercomputers and theonslaught of new, highly parallel machines by being involved with theapplications, by using and understanding the new structures, by writingtexts, training students, and carrying out research.andP;  For example, no currenttexts about programming are based on the mainline development of thesupercomputer.andP;  Understanding may enable work on automatic programmingsystems to analyze and rewrite programs for these computational models.andP;  As aminimum, understanding will greatly facilitate exploiting both the evolutionand the radically new machines.andM;REFERENCESandM;[1.] Althas, W.C.andP;  and Seitz, C.L.andP;  Multicomputers: Message-passingconcurrent computers.andP;  IEEE Comput.andP;  21, 8 (Aug.andP;  1988), 9-19.andP;  [2.] Bardon,M., and Curtis, K.andP;  A national computing environment  for academic research.andO;NSF Working Group on Computers for Research.andP;  National Science Foundation,July 1983.andM;[3.] Bell, C.G.andP;  Multi's: A new class of multiprocessor computers.andP;  Science228 (Apr.andP;  26, 1985), 462-467.andM;[4.] Bell, C.G.andP;  A national research network.andP;  IEEE Spectrum 25, 2 (Feb.andO;1988), 54-57.andM;[5.] Brown, J., Dongarra, J., Karp, A., et al.andP;  1988 Gordon Bell Prize.andP;  IEEESoftware 6, 3 (May 1989) 78-85.andM;[6.] Dongarra, J.J.andP;  Performance of various computers using standard linearequations software in a Fortran environment.andP;  Argonne National LaboratoryTechnical Memorandum, 23 (Feb.andP;  23, 1989).andM;[7.] Office of Science and Technology Policy.andP;  A Research and DevelopmentStrategy for High Performance Computing.andP;  (Nov.andP;  1987).andM;[8.] Misaki, E., and Lue, K.M.andP;  Preliminary CPU performance study: The ArdentTitan and the Cray X-MP-1 on the Livermore Loops.andP;  Vector Register of theInstitute for Supercomputing Research 2, 1 (Aug.andP;  1988), 8-11.andM;[9.] McCormick, B.H., DeFanti, T.A., and Brown, M.D., Eds.andP;  Visualization inScientific Computing.andP;  Comput.andP;  Graphics 21, 6 (Nov.andP;  1987).andM;[10.] Patterson, D., Katz, R., and Gibson, G.andP;  A case for redundant arrays ofinexpensive disks, RAID.andP;  In Proceedings of the ACM SIGMOD InternationalConference on the Management of Data (June 1988).andM;[11.] Perry, T.S.andP;  and Zorpette, G.andP;  Supercomputer experts predict expansivegrowth.andP;  IEEE Spectrum 26, 2 (Feb.andP;  1989), 26-33.andM;[12.] Smith, N.P., Ed.andP;  Supercomputing Review 1, 1 (1988).andM;[13.] Smith, N.P., Ed.andP;  Supercomputer Review 1, 4 (Apr.andP;  1989).andM;CR Categories and Subject Descriptors: B.0 [Hardware]: General; C.0 [ComputerSystems Organization]: General; C.1 [Computer Systems Organization]:Processor Architectures; C.3 [Computer Systems Organization]: Special-purposeand Applications-based Systems; [Computer Systems Organization]: Performanceof Systems; C.5.1 [Computer System Implementation]: Large and Medium(mainframe) Computers--super (very large) computers; J.2 [ComputerApplications]: Physical Sciences and Engineering; K.1 [Computing Milieux]:The Computer Industry; K.3.2 [Computers and Education]: Computer andInformation Science Education; K.4.1 [Computers and Society]: Public PolicyIssuesandM;General Terms: Design, Human FactorsandM;Additional Key Words and Phrases: Circuit technology evolution, parallelprocessing, supercomputers and other high performance parallel computersandM;ABOUT THE AUTHOR:andM;G.andP;  GORDON BELL is Vice President of Research and Development at the ArdentComputer Company.andP;  His research interests include the contributions to andthe understanding of the design of interesting, useful, and cost-effectivecomputer systems, and the organizations that build them.andP;  Author's PresentAddress: Ardent Computer Company, 800 West Maude, Sunnyvale, CA 94086.andO;</TEXT></DOC>