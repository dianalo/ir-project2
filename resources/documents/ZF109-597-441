<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-597-441  </DOCNO><DOCID>09 597 441.andM;</DOCID><JOURNAL>Communications of the ACM  Nov 1990 v33 n11 p58(17)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Connectionist ideas and algorithms. (neural networks)</TITLE><AUTHOR>Knight, Kevin.andM;</AUTHOR><SUMMARY>One obvious idea for artificial intelligence is to imitate thefunction of the human brain directly on a computer.andP;  There iscurrently a resurgence of interest in neural networks due in partto the emergence of faster digital computers on which to simulatelarger networks.andP;  Other important factors include the discovery ofpowerful network learning algorithms and interest in buildingmassively parallel computers.andP;  These new neural networkarchitectures are called connectionist architectures.andP;  They arenot intended to duplicate the operation of the human brain butinstead gather inspiration from facts about the workings of thebrain.andP;  Researchers believe that using the brain metaphor inthinking about computers, instead of a digital computer metaphor,will bring a better understanding of the nature of intelligentbehavior.andP;  These topics and others are discussed extensively.andM;</SUMMARY><DESCRIPT>Topic:     Artificial intelligenceNeural NetworksParallel ProcessingProcessor ArchitectureParallel AlgorithmsScientific Research.andO;Feature:   illustrationdiagram.andO;Caption:   A simple Hopfield network. (diagram)The four stable states of a particular Hopfield net. (diagram)A Hopfield net as a model of content-addressable memory. (diagram)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>CONNECTIONIST IDEAS AND ALGORITHMS In our quest to build intelligentmachines, we have but one naturally occurring model: the human brain.andP;  Itfollows that one natural idea for artificial intelligence (AI) is to simulatethe functioning of the brain directly on a computer.andP;  Indeed, the idea ofbuilding an intelligent machine out of artificial neurons has been around forquite some time.andP;  Some early results on brain-line mechanisms were achievedby [18], and other researchers pursued this notion through the next twodecades, e.g., [1, 4, 19, 21, 24].andP;  Research in neural networks came to avirtual halt in the 1970s, however, when the networks under study were shownto be very weak computationally.andP;  Recently, there has been a resurgence ofinterest in neural networks.andP;  There are several reasons for this, includingthe appearance of faster digital computers on which to simulate largernetworks, interest in building massively parallel computers, and mostimportantly, the discovery of powerful network learning algorithms.andM;The new neural network architectures have been dubbed connectionistarchitectures.andP;  For the most part, these architectures are not meant toduplicate the operation of the human brain, but rather receive inspirationfrom known facts about how the brain works.andP;  They are characterized byandM;* Large numbers of very simple neuron-like processing elements;andM;* Large numbers of weighted connections between the elements -- the weightson the connections encode the knowledge of a network;andM;* Highly parallel, distributed control; andandM;* Emphasis on learning internal representations automatically.andM;Connectionist researchers conjecture that thinking about computation in termsof the brain metaphor rather than the digital computer metaphor will lead toinsights into the nature of intelligent behavior.andM;Computers are capable of amazing feats.andP;  They can effortlessly store vastquantities of information.andP;  Their circuits operate in nanoseconds.andP;  They canperform extensive arithmetic calculations without error.andP;  Humans cannotapproach these capabilities.andP;  On the other hand, humans routinely performsimple tasks such as walking, talking, and commonsense reasoning.andP;  Current AIsystems cannot do any of these things better than humans.andP;  Why not?andP;  Perhapsthe structure of the brain is somehow suited to these tasks, and not suitedto tasks like high-speed arithmetic calculation.andP;  Working under constraintssuggested by the brain may make traditional computation more difficult, butit may lead to solutions to AI problems that would otherwise be overlooked.andM;What constraints, then, does the brain offer us?andP;  First of all, individualneurons are extremely slow devices when compared to their counterparts indigital computers.andP;  Neurons operate in the millisecond range, an eternity toa VLSI designer.andP;  Yet, humans can perform extremely complex tasks, likeinterpreting a visual scene or understanding a sentence, in just a tenth of asecond.andP;  In other words, we do in about a hundred steps what currentcomputers cannot do in ten million steps.andP;  How can this be possible?andP;  Unlikea conventional computer, the brain contains a huge number of processingelements that act in parallel.andP;  This suggests that in our search forsolutions, we look for massively parallel algorithms that require no morethan 100 processing steps [9].andM;Also, neurons are failure-prone devices.andP;  They are constantly dying (you havecertainly lost a few since you began reading this article), and their firingpatterns are irregular.andP;  Components in digital computers, on the other hand,must operate perfectly.andP;  Why?andP;  Such components store bits of information thatare available nowhere else in the computer: the failure of one componentmeans a loss of information.andP;  suppose that we built AI programs that were notsensitive to the failure of a few components, perhaps by using redundancy anddistributing information across a wide range of components?andP;  This would openthe possibility of very large-scale implementations.andP;  With currenttechnology, it is far easier to build a billion-component integrated circuitin which 95 percent of the comonents work correctly than it is to build aperfectly functioning million-component machine [8].andM;Another thing people seem to be able to do better than computers is handlefuzzy situations.andP;  We have very large memories of visual, auditory, andproblem-solving episodes, and one key operation in solving new problems isfinding closest matches to old situations.andP;  Inexact matching is somethingbrainstyle models seem to be good at, because of the diffuse and fluid way inwhich knowledge is represented.andM;The idea behind connectionism, then, is that we may see significant advancesin AI if we approach problems from the point of view of brain-stylecomputation rather than rule-based symbol manipulation.andP;  At the end of thisarticle, we will look more closely at the relationship between connectionistand symbolic AI.andM;Hopfield NetworksandM;The history of AI is curious.andP;  The first problems attacked by AI researcherswere problems like chess and theorem proving, because these were thought torequire the essense of intelligence.andP;  Vision and languageunderstanding--processes easily mastered by five-year-olds--were not thoughtto be difficult.andP;  These days, we have expert chess programs, and expertmedical diagnosis programs, but no programs that can match the basicperceptual skills of a child.andP;  Neural network researchers contend that thereis a basic mismatch between standard computer information-processingtechnology and the technology used by the brain.andM;In addition to these perceptual tasks, AI is just stating to grapple withfundamental problems in memory and commonsense reasoning.andP;  Computers arenotorious for their lack of common sense.andP;  Many people believe that commonsense derives from our massive store of knowledge, and more importantly, ourability to access relevant knowledge quickly, effortlessly, and at the righttime.andM;When we read the description &quot;gray, large, mammal,&quot; we automatically think ofelephants and their associated features.andP;  We access our memories by content.andO;In traditional implementations, access by content involves expensivesearching and matching procedures.andP;  Massively parallel networks suggest amore efficient method.andM;A neural network, introduced by Hopfield [12], proposed one theory of memory.andO;A Hopfield network has the following interesting features:andM;* Distributed representation.andP;  A memory is stored as a pattern of activationacross a set of processing elements.andP;  Furthermore, memories can besuperimposed upon one another--different memories are represented bydifferent patterns over the same set of processing elements.andM;* Distributed, asynchronous control.andP;  Each processing element makes decisionsbased only on its own local situation.andP;  All of these local actions add up toa global solution.andM;* Content-addressable memory.andP;  A number of patterns can be stored in anetwork.andP;  To retrieve a pattern, we need only specify a portion of it.andP;  Thenetwork automatically finds the closets match.andM;* Fault tolerance.andP;  If a few of the processing elements misbehave or failcompletely, the network will still function properly.andM;How are these features achieved?andP;  A simple Hopfield net is shown in Figure 1.andO;Processing elements, or units, are always in one of two states, active orinactive.andP;  Units are connected to each other with weighted, symmetricconnections.andP;  A positive connection indicates that the two units tend toactivate each other.andP;  A negative connection allows an active unit todeativate a neighboring unit.andM;The network operates as follows.andP;  A random unit is chosen.andP;  If any of itsneighbors are active, the unit computes the sum of the weights on theconnections to those active neighbors.andP;  If the sum is positive, the unitbecomes active, otherwise it becomes inactive.andP;  Another random unit ischosen, and the process repeats until the network reaches a stable state(i.e., until no more units can change state).andP;  This process is calledparallel relaxation.andP;  If the network starts in the state shown in Figure 1,the unit in the lower left corner will tend to activate the unit above it.andO;This unit, in turn, will attempt to ativate the unit above it, but theinhibitory connection from the upper-right unit will foil this attempt, andso on.andM;This network has only four distinct stable states.andP;  They are shown in Figure2.andP;  Given any initial state, the network will necessarily settle into one ofthese four configurations.andP;  (1)  The network can be thought of as storing thepatterns in Figure 2.andP;  Hopfield's major contribution was to show that givenany set of weights and any initial state, his parallel relaxation algorithmwould eventually steer the network into a stable state.andP;  There can be nodivergence or oscillation.andM;The network can be used as a content-addressable memory by setting theactivities of the units to correspond to a partial pattern.andP;  The network willthen settle into the stable state that best matches the partial pattern.andP;  Anexample is shown in Figure 3.andM;Parallel relaxation is nothing more than search, albeit of a style notusually employed in AI.andP;  It is useful to think of the various states of anetwork as forming a search space as in Figure 4.andP;  A randomly chosen statewill utlimately transform itself into one of the local minima namely thenearest stable state.andP;  This is how we get the content-addressable behavior.andO;We also get an error-correcting behavior.andP;  Suppose we read the description,&quot;gray, large, fish, eats plankton.&quot;andP;  We imagine a whale, even though we knowthat a whale is a mammal, not a fish.andP;  Even if the initial state containsinconsistencies, a Hopfield network will settle into the solution thatviolates the fewest constraints offered by the inputs.andP;  Traditionalmatch-and-retrieve procedures are less forgiving.andM;Now, suppose a unit occasionally fails, say, by becoming active or inactivewhen it should not.andP;  This causes no major problem: surrounding units willquickly set in straight again.andP;  It would take the unlikely concerted effortof many errant units to push the network into the wrong stable state.andP;  Innetworks of thousands of moe highly interconnected units, such faulttolerance is even more apparent--units and connections can disappearcompletely without adversely affecting the overall behavior of the network.andM;As we can see, parallel networks of simple elements can compute interestingthings.andP;  The next important question is: What is the relationship between theweights on the network's connection and the local minima into which itsettles?andP;  In other words, if the weights encode the &quot;knowledge&quot; of aparticular network, how is that knowledge acquired?andP;  Knowledge acquisition isa difficult problem in AI, and one attractive feature of connectionistarchitectur is that their method of representation (namely, real-valuedconnections weights) lends itself very nicely to automatic learning.andM;In the next section, we will look closely at learning in several neuralnetwork models, including perceptrons, backpropagation networks, andBoltzmann machines.andM;Learning in NeuralandM;NetworksandM;The perceptron, an invention of [24] was one of the earliest neural networkmodels.andP;  A perceptron models a neuron by taking a weighted sum of its inputsand sending an output I if the sum is greater than some adjustable thresholdvalue (otherwise it sends 0).andP;  Figure 5 shows the device.andM;The inputs [x.sub.1], [x.sub.2] .andP;  .  .  [x.sub.n]) and connection weights([w.sub.1], [w.sub.2] .andP;  .  .  [w.sub.n]) in the figure are typically realvalues, both positive and negative.andP;  If the presence of some feature[x.sub.i] tends to cause the perceptron to fire, the weights [w.sub.i] willbe positive; if the feature [x.sub.i] inhibits the perceptron, the weight[w.sub.i] will be negative.andP;  The perceptron itself consists of the weights,the summation processor, and the adjustable threshold processor.andP;  Learning isa process of modifying the values of the weights and the threshold.andP;  It isconvenient to implement the threshold as just another weight [w.sub.0] (as inFigure 6).andP;  This weight can be thought of as the prosperity of the perceptronto fire irrespective of its inputs.andP;  The perceptron of Figure 6 fires if theweighted sum is greater than zero.andM;A perceptron computes a binary function of its input.andP;  Multiple perceptronscan be combined to compute more complex functions, as shown in Figure 7.andM;Such a group of perceptrons can be trained on sample input/output pairs untilit learns to compute the correct function.andP;  The amazing property ofperceptron learning is this: whatever a perceptron can compute, it can learnto compute!andP;  We will demonstrate this in a moment.andP;  At the time perceptronswere invented, many people speculated that intelligent systems could beconstructed out of perceptrons (see Figure 8).andM;Since the perceptrons of Figure 7 are independent of one another, they can beseparately trained.andP;  Let us concentrate on what a single perceptron can learnto do.andP;  Consider the pattern classification problem shown in Figure 9.andP;  Givenvalues for [x.sub1] and [x.sub.2], we want to train a perceptron to output 1if it thinks the input belongs to the class of white dots, and 0 if it thinksthe input belongs to the class of black dots.andP;  We have no explicit rule toguide us; we must induce a rule from a set of training instances.andP;  We willnow see how perceptors can learn to solve such problems.andM;First, it is necessary to take a close look at what the perceptron computes.andO;Let [x.e an input vector ([x.sub.1], [x.sub.2] .andP;  .  .  [x.sub.n]).andP;  Noticethat the weighted summation function g(x) and the output function 0(x) can bedefined as:andM;[Mathematical Expression Omitted]andM;0(x) = [1 if g(x) [is greater than] 0 / 0 if g(x) [is less than] 0andM;Consider the case where we have only two inputs (as in Figure 9).andP;  Then:andM;g(x) = [w.sub.0] + [w.sub.1.x.sub.1] + [w.sub.2.x.2]andM;If g(x) is exactly 0, the perceptron cannot decide whether to fire or not.andP;  Aslight change in inputs could cause the device to go either way.andP;  If we solvethe equation g(x) = 0, we get the equation for a line:andM;[x.sub.2] = [w.sub.1] / [x.sub.2] [x.sub.1 - [w.sub.0] / [w.sub.2]andM;The location of the line is completely determined by the weights [w.sub.0][w.sub.1], and [w.sub.2].andP;  If an input vector lies on one side of the line,the perceptron will output 1; if it lies on the other side, the perceptronwill output 0.andP;  A line that correctly separates the training instancescorresponds to a perfectly functioning perceptron.andP;  Such a line is called adecision surface.andP;  In perceptrons with many inputs, the decision surface willbe a hyperplane through the multidimensional space of possible input vectors.andO;The problem of learning is one of locating an appropriate decision surface.andM;We will present a formal learning algorithm in a moment.andP;  For now, considerthe informal rule:andM;If the perceptron fires when it should not fire, make each [w.sub.1] smallerby an amount proportional to [x.sub.1].andP;  It the perceptron fails to fire whenit should fire, make each [w.sub.1] larger by a similar amount.andM;Suppose we want to train a three-input perceptron to fire only when its firstinput is on.andP;  If the perceptron fails to fire in the presence of an active[x.sub.1], we will increase [w.sub.1] (and we may increase other weights).andO;If the perceptron fires incorrectly, we will end up decreasing weights tatare not [w.sub.1].andP;  In addition, [w.sub.0] will find a value based on thetotal number of incorrect firings versus incorrect misfirings.andP;  Soon,[w.sub.1] will become large enough to overpower [w.sub.0], while [w.sub.2]and [w.sub.3] will not be powerful enough to fire the perceptron, even in thepresecne of both [x.sub.2] and [x.sub.3].andM;Now let us return to the functions g(x) is critical to determining g(x) iscritical to determining whether the perceptron will fire, the magnitude isalso important.andP;  The absolute value of G(x) tells how far a given inputvector x lies from the decision surface.andP;  This given us a way ofcharacterizing how good a set of weights is.andP;  Let w be the weight vector([w.sub.0], [w.sub.1] .andP;  .  .  [w.sub.n]), and let X be the subset oftraining instances misclassified by the current set of weights.andP;  Then definethe Perceptron Criterion Function, J(w), to be the sum of the distances ofthe misclassified input vectors from the decision surface:andM;[Mathematical Expression Omitted]andM;To create a better set of weights than the current set, we would like toreduce J(w).andP;  Ultimately, if all inputs are classified correctly, J(w) = 0.andM;How do we go about minimizing J(w)?andP;  We can use a form of local search knownas gradient descent.andP;  (2)  Four our current purposes, think of J(w) asdefining a surface in the space of all possible weights.andP;  Such a surfacemight look like the once in Figure 10.andM;In the figure, weight [w.sub.0] should be part of the weight space, but itomitted here because it is easier to visualize J in only three dimensions.andO;Now, some of the weights vectors constitute solutions, in that a perceptronwith a solutions, in that a perceptron classify all of its inputs correctly.andO;Note that there are an infinite number of solution vectors.andP;  For any solutionvector w, we know that J(w) = 0.andP;  Suppose we begin with a random weightvector w that is not a solution vector.andP;  We want to slide down the J surface.andO;There is a mathematical method for doing this--we compute the gradient of thefunction J(w).andP;  Before we derive the gradient function, we will reformulatethe Perceptron Criterion Function to remove the absolute value sign:andM;[Mathematical Expression Omitted] Recall that X is the set of misclassifiedinput vectors.andM;Now, here is *J, the gradient of J(*) with respect to the weight space:andM;[Mathematical Expression Omitted]andM;The gardient is a vector that tells us the direction to move in weight spacein order to reduce J(*).andP;  In order to find a solution weight vector, wesimply change the weights in the direction of the gradient, recompute J(*)recompute the new gradient, and iterate until J(*) = 0.andP;  The rule forupdating the weights at time t + 1 is:andM;[Mathematical Expression Omitted]andM;Or in expanded form:andM;[Mathematical Expression Omitted]andM;[eta] is a scale factor that tells us how far to move in the direction of thegradient.andP;  A small [eta] will lead to slower learning, but a large [eta] maycause a move through weight space that &quot;overshoots&quot; the solution vector.andO;Taking [eta] to be a constant gives us what is usually called the&quot;fixed-increment perceptron learning algorithm&quot;:andM;Algorithm: Fixed-IncrementandM;Perceptron LearningandM;Given: a classification problem with n input features ([x.sub.1],[x.sub.2].andO;.  .  [x.sub.n]) and two output classes.andM;Compute: a set of weights [w.sub.0], [w.sub.1], [w.sub.2] .andP;  .  .  [w.sub.n])that will cause a perceptron to fire whenever the input falls into the firstoutput class.andM;1.andP;  Create a perceptron with n + 1 inputs and n + 1 weights, where the extrainput [x.sub.0] is always set to 1.andM;2.andP;  Initialize the weights ([w.sub.0], [w.sub.1] .andP;  .  .  [w.sub.n]) torandom real values.andM;3.andP;  Iterate through the training set, collecting all of the examplesmisclassified by the current set of weights.andM;4.andP;  If all examples are classified correctly, output the weights and quit.andM;5.andP;  Otherwise, compute the vector sum S of the misclassified input vectors,where each vector has the form ([x.sub.0], [x.sub.1] .andP;  .  .  [x.sub.n).andP;  Increating the sum, add to S a vector * if * is an input for which theperceptron incorrectly fails to fire, but add vector -- * if * is an inputfor which the perceptron incorrectly fires.andP;  Multiply the sum by a scalefactor [eta].andM;6.andP;  Modify the weights ([w.sub.0], [w.sub.1] .andP;  .  .  [w.sub.n]) by addingthe elements of the vector S to them.andP;  Go to step 3.andM;The perceptron learning algorithm is a search algorithm.andP;  It begins in arandom initial state and finds a solution state.andP;  The search space is simplyall of the possible assignments of real values to the weights of theperceptron, and the search strategy is gradient descent.andM;So far, we have seen two search methods employed by neural networks: gradientdescent in perceptrons and parallel relaxation in Hopfield networks.andP;  It isimportant to understand the relation between the two.andP;  Parallel relaxation isa problem-solving strategy, analogous to state space search in symbolic AI.andO;Gradient descent is a learning strategy, analogous to inductive techniques insymbolic AI.andP;  In both symbolic and connectionist AI, learning is viewed as atype of problem solving, and this is why search is useful in learning.andP;  Butthe ultimate goal of learning is to get a system into a position where it cansolve problems better.andP;  Do not confuse learning algorithms with others.andM;The Perceptron Convergence Theorem, due to Rosenbiatt [24], guarantees thatthe perceptron will find a solution state (i.e., it will learn to classifyany linearly separable set of inputs).andP;  Figure 11 shows a perceptron learningto classify the instances of Figure 9.andP;  Remember that every set of weightsspecifies some decision surface--in this case some two-dimensional line.andM;The introduction of perceptrons in the late 1950s created a great deal ofexcitement in the research community.andP;  Here was a device that stronglyresembled a neuron and for which well-defined learning algorithms wereavailable.andP;  There was much speculation about how intelligent systems could beconstructed from perceptron building blocks.andP;  The book, Perceptrons, [20] putan end to such speculation by analyzing the computational capabilities of thedevices.andP;  The authors, Minsky and Papert, noticed that while the ConvergenceTheorem guaranteed correct classification of lineraly separable data, mostproblems do not supply such nice data.andP;  Indeed, the perceptron is incapableof learning to solve some very simple problems.andP;  One example given in thebook is the exclusive-or (XOR) problem: Given two binary inputs, output 1 ifexactly one of the inputs is on, and output 0 otherwise.andP;  We can view XOR asa pattern-classification problem in which there are four patterns and twopossible outputs (see Figure 12).andM;The perceptron cannot learn a linear decision surface to separate thesedifferent outputs, because no such decision surface exists.andP;  No single linecan separate the &quot;1&quot; outputs from the &quot;0&quot; outputs.andP;  Minsky and Papert gave anumber of problems with this property: telling whether a line drawing isconnected, separating figure from ground in a picture, etc.andP;  Notice that thedeficiency here is not the perceptron learning algorithm, but in the way theperceptron represents knowledge.andM;If we could draw an elliptical decision surface, we could encircle the two&quot;1&quot; outputs in the XOR space.andP;  However, perceptrons are incapable of modelingsuch surfaces.andP;  Another idea is to employ two separate line-drawing stages.andO;We could draw one line to isolate the point ([x.sub.1] = 1.andP;  [x.sub.2] = 1)and then another line to devide the remaining three points into twocategories.andP;  Using this idea, we can construct a multilaver perceptron (aseries of perceptrons) to solve the problem.andP;  Such a device is shown inFigure 13.andM;Note how the output of the first perceptron serves as one of the inputs tothe second perceptron, with a large, negatively weighted connection.andP;  If thefirst perceptron sees the input ([x.sub.1] = 1, [x.sub.2] = 1) it will send amassive inhibitory pulse to the second perceptron, causing that unit tooutput 0 regardless of its other inputs.andP;  If either of inputs is 0, thesecond perceptron gets no inhibition from the first perceptron, and itoutputs 1 if either of the inputs is 1.andM;The use of multilayer perceptrons, then, solves our knowledge representationproblem.andP;  However, it introduces a serious learning problem: the ConvergenceTheorem does not extend to multilaver perceptrons.andP;  The perceptron learningalgorithm can correctly adjust weights between inputs and outputs, but itcannot adjust weights between perceptrons.andP;  In Figure 13, the inhibitoryweight &quot;-9.0&quot; was hand-coded, not learned.andP;  At the time Perceptrons waspublished, no one knew how multilaver perceptrons could be made to learn.andP;  Infact, Minsky and Papert speculated:andM;The perceptron .andP;  .  .  has many features that attract attention: itslinearity, it intriguing learning theorem .andP;  .  .  there is no reason tosuppose that any of these virtues carry over to the many-layered version.andO;Nevertheless, we consider it to be an important research problem to elucidate(or reject) our intuitive judgement that the extension is sterile.andM;Despite the identification of this important research problem, actualresearch in perceptron learning came to a halt in the 1970s.andP;  The field sawlittle interest until the 1980s, when several learning procedures formultilayer perceptrons--also called multilayer networks--were proposed.andP;  Thenext few sections are devoted to such learning procedures.andM;Backpropagation NetworksandM;As suggested by Figure 8 and the Perceptrons critique, the ability to trainmultilayer networks is an important step in the direction of buildingintelligent machines out of neuron-like components.andP;  Let us reflect for amoment on why this is so.andP;  Our goal is to take a relatively amorphous mass ofneuron-like elements and teach it to perform useful tasks.andP;  We would like itto be fast and resistant to damage.andP;  We would like it to generalize from theinputs it sees.andP;  We would like to build these neural masses on a very largescale, and we would like them to be able to learn efficiently.andP;  Perceptronsgot us part of the way there, but we say that they were too weakcomputationally.andP;  So we turn to more complex, multilayer networks.andM;What can a multilayer network compute?andP;  The simple answer is: anything!andO;Given a set of inputs, we can use summation/threshold units as simple AND,OR, and NOT gates by appropriately setting the threshold and connectionweights.andP;  We can build any arbitray combinational circuit out of such units.andO;In fact, if we are allowed to use feedback loops, we can build ageneral-purpose computer with them.andM;The major problem is learning.andP;  The knowledge representation system employedby neural nets is quite opaque: they must learn their own representationsbecause programming them by hand is impossible.andP;  Perceptrons had the niceproperty that whatever they could compute, they could learn to computer.andO;Does this property extend to multilayer networks?andP;  The answer is yes, sortof.andP;  Backpropagation is a step in that direction.andM;It will be useful to deal first with a subclass of multilayer networks,namely fully connected, layered, feedfordward networks.andP;  A sample of such anetwork is shown in Figure 14.andP;  This network has three layers, although it ispossible and sometimes useful to have more.andP;  Activations flow from the inputlayer through a hidden layer, then on to the output layer.andP;  Each unit in onelayer is connected in the forward direction to every unit in the next layer.andO;As usual, the knowledge of the network is encoded in the weights onconnections between units.andP;  In contrast to the parallel relaxation methodused by Hopfield nets, backpropagation networks perform a simplercomputation.andP;  Because activations flow in only one direction, there is noneed for an iterative relaxation process.andP;  The activation levels of the unitsin the output layer determine the output of the network.andM;The existence of hidden units allows the network to develop complex featuredetectors, or internal representations.andP;  Figure 15 shows the application of athree-layer network to the problem of recognizing digits.andP;  Thetwo-dimensional grid containing the numeral &quot;7&quot; forms the input layer.andP;  Asingle hidden unit might be strongly activated by a horizontal line in theinput, or perhaps a diagonal.andP;  The important thing to note is that thebehavior of these hidden units is automatically learned, not preprogrammed.andO;In Figure 15, the input grid appears to be laid out in two dimensions, butthe fully connected network is unaware of this 2-D structure.andP;  Because thisstructure can be important, many networks permit their hidden units tomaintain only local connections to the input layer (e.g., a different 4-by-4sub-grid for each hidden unit).andM;The hope in attacking problems like handwritten character recognition is thatthe neural network will not only learn to classify the inputs it is trainedon, but will generalize and be able to classify inputs that it has not yetseen.andP;  We will return to generalization in the next section.andM;It seems reasonable at this point to express the following:  &quot;All neural netsseem to be able to do is classification.andP;  Hard AI problems like planning,natural language parsing, and theorem proving are not simply classificationtasks, so how do connectionist models address these problems?&quot;andP;  Most of theproblems we will see in this article are indeed classification problems,because these are the problems that neural networks are best suited to handleat present.andP;  A major limitation of current network formalisms is their way ofdealing with phenomena that involve time.andP;  This limitation is lifted to somedegree in work on recurrent networks (e.g.andP;  [14]), but for now, we willconcentrate on classification problems.andM;LEt us now return to back-propagation networks.andP;  The unit in abackpropagation network requires a slightly different activation functionfrom the perceptron.andP;  Both functions are shown in Figure 16.andP;  Abackpropagation unit still sums up its weighted inputs, but unlike theperceptron, it produces a real value between 0 and 1 as output, based on asigmoid (or S-shaped) function.andP;  LEt sum be the weighted sum of the inputs toa unit.andP;  The equation for the unit's output is given by:andM;output = 1/1+[e.sup.-sum]andM;Like a perceptron, a back-propagation network typically starts out with arandom set of weights.andP;  The network adjusts its weights each time it sees aninput/output pair.andP;  Each pair requires two stages: a forward pass and abackward pass.andP;  The forward pass involves presenting a sample input to thenetwork and letting activations flow until they reach the output layer.andO;During the backward pass, the network's actual output is compared to thetarget output, and error estimates are computed for the output units.andP;  Theweights connected to the output units can be adjusted in order to reducethose errors.andP;  WE can then use the error estimates of the output units toderive error estimates for the units in the hidden layers.andP;  Finally, errorsare propagated back to the connections stemming from the input units.andM;Unlike the perceptron learning algorithm of the last section, thebackpropagation algorithm usually updates its weights incrementally, afterseeing each input/output pair.andP;  After it has seen all of the input/outputpairs (and adjusted its weights that many times), we say that one epoch hasbeen completed.andP;  Training a backpropagation network usually requires manyepochs.andM;Refer back to Figure 14 for the basic structure on which the followingalgorithm is based.andM;Algorithm: BackpropagationandM;Given: a Set of input/output vector pairs.andM;Compute: A set of weights for a three-layer network that maps inputs ontocorresponding outputs.andM;1.andP;  Let A be the number of units in the input layer, as determined by thelength of the training input vectors.andP;  LEt C be the number of units in theoutput layer.andP;  Now choose B, the number of units in the hidden layer.andP;  (3) As shown in Figure 14, the input and hidden layers each have an extra unitused for thresholding; therefore, the units in these layers will sometimes beindexed by the ranges (0.andP;  .  .A) and (0.andP;  .  .B).andP;  We denote the activationlevels of the units in the input layers by [x.sub.j], in the hidden layer by[h.sub.j], and in the output layer by [o.sub.j].andP;  Weights connecting theinput layer to the hidden layer are denoted by [w1.sub.ij], where subscript iindexes the input units, and j indexes the hidden units.andP;  Likewise, weightsconnecting the hidden layer to the output layer are denoted by [w2.sub.ij]with i indexing to hidden units and j indexing output units.andM;2.andP;  Initialize the weights in the network.andP;  Each weight should be setrandomly to number between -0.1 and 0.1.andM;[w1.sub.ij] = random(-0.1,0.1) for all i = 0 .andP;  .  .  A, j = 1 .andP;  .  .  BandM;[w2.sub.ij] = random(-0.1,0.1) for all i = 0 .andP;  .  .  B, j = 1 .andP;  .  .  CandM;3.andP;  Initialize the activations of the thresholding units.andP;  These should neverchange their values.andM;[x.sub.0] = 1.0 [h.sub.0] = 1.0andM;4.andP;  Choose an input/output pair.andP;  Suppose the input vector is [x.sub.i] andthe target output vector is [y.sub.i].andP;  Assign activation levels to the inputunits.andM;5.andP;  Propagate the activations from the units in the input layer to the unitsin the hidden layer, using the activation function of Figure 16:andM;[Mathematical Expression Omitted]andM;for all j = 1 .andP;  .  .  BandM;Note that i ranges from 0 to A.andP;  [w1.sub.oj] is the thresholding weight forhidden unit j (its propensity to fire irrespective of its inputs).andP;  [x.sub.0]is always.andP;  1.0.andM;6.andP;  Propagate the activations from the units in the hidden layer to the unitsin the output layer.andM;[Mathematical Expression Omitted]andM;for all j = 1 .andP;  .  .  CandM;Again, the thresholding weight [w2.sub.0j] for output j plays a Role in theweighted summation.andP;  [h.sub.0] is always 1.0.andM;7.andP;  Compute the errors (4) of the units in the output layer, denoted]]delta]2.sub.j].andP;  Errors are based on the network's actual output([o.sub.j]) and the target output ([y.sub.j).andM;[[delta]2.sub.j] = [o.sub.j](1 - [o.sub.j])([y.sub.j] - [0.sub.j] for all j =1 .andP;  .  .  CandM;8.andP;  Compute the errors of the units in the hidden layer, denoted[[delta]1.sub.j].andM;[Mathematical Expression Omitted]andM;for all j = 1 .andP;  .  .  BandM;9.andP;  Adjust the weights between the hidden layer and output layer.andP;  (5)  Thelearning rate is denoted [eta]; its function is the same as in perceptronlearning.andP;  A reasonable value of [eta] is 0.35.andM;[[Delta]w2.sub.ij] = [eta] .andP;  [[delta]2.sub.j] .andP;  [h.sub.i] for all i = 0 .andO;.  .  B, j = 1 .andP;  .  .  CandM;10.andP;  Adjust the weights between the input layer and the hidden layer.andM;[[Delta]w1.sub.ij] = [eta] .andP;  [[delta]1.sub.j] .andP;  [x.sub.i] for all i = 0 .andO;.  .  A, j = 1 .andP;  .  .  BandM;11.andP;  Go to step 4 and repeat.andP;  When all of the input/output pairs have beenpresented to the network, one epoch has been completed.andP;  Repeat steps 4 to 10for as many epochs as desired.andM;The aglorithm generalizes straightforwardly to networks of more than threelayers.andP;  (6)  For each extra hidden layer, insert a forward propagation stepbetween steps 6 and 7; an error computation step between steps 8 and 9; and aweight adjustment step between steps 10 and 11.andP;  Error computation for hiddenunits should use the equation in step 8, but with i ranging over the units inthe next layer, not necessarily the output layer.andM;The speed of learning can be increased by modifying the weight modificationsteps 9 and 10 to include a momentum term [alpha].andP;  The weight updateformulas become:andM;[[Delta]w2.sub.ij](t + 1) = [eta] .andP;  [[delta]2.sub.j] .andP;  [h.sub.i] +[[alpha][Delta]w2.sub.ij](t)andM;[[Delta]w1.sub.ij](t + 1) = [eta] .andP;  [[delta]1.sub.1] .andP;  [x.sub.i] +[[alpha][Delta]w1.sub.ij](t)andM;where [h.sub.j.,] [x.sub.j.,] [[delta]2.sub.j] are measured at time t + 1.andO;[[Delta]w.sub.ij.](t) is the change the weight saw during the previousforward-backward pass.andP;  If [alpha] is set to 0.9 or so, learning speed isimproved.andP;  (7)andM;Recall that the activation function has a sigmoid shape.andP;  Since infiniteweights would be required for the actual outputs of the network to reach 0.0and 1.0, binary target output (the [y.sub.j.]'s of steps 4 and 7) are usuallygiven as 0.1 and 0.9 instead.andP;  The sigmoid is required by backpropagationbecause the derivation of the weight update rule requires that the activationfunction be both continuous and differentiable.andM;The derivation of the weight update rule is more complex than the derivationof the fixed-increment update rule for perceptrons, but the idea is much thesame.andP;  There is an error function that defines a surface over weight space,and the weights are modified in the direction of the gradient of the surface.andO;See [25-27] for details.andP;  Interestingly, the error surface for multilayernets is more complex than the error surface for perceptrons.andP;  One notabledifference is the existence of local minima.andP;  REcall the bowl-shaped space weused to explain perceptron learning (Figure 10).andP;  As we modified weights, wemoved in the direction of the bottom of the bowl; eventually, we reached it.andO;A backpropagation network, however, may slide down the error surface into aset of weights that does not solve the problem it is being trained on.andP;  Ifthat set of weights is at a local minimum, the network will never reach theoptimal set of weights.andP;  Thus, we have no analogue of the PErceptronConvergence Theorem for backpropagation networks.andM;There are several methods of combating the problem of local minima.andP;  Themomentum factor [alpha], which tends to keep the weight changes moving in thesame direction, allows the algorithm to skip over small minima.andP;  Simulatedannealing, to be discussed later, is also useful.andP;  Finally, adjusting theshape of a unit's activation function can have an effect on the network'ssusceptibility to local minima.andM;Fortunately, backpropagation networks rarely slip into local minima.andP;  Itturns out that, especially in larger networks, the high-dimensional weightspace provides plenty of degrees of freedom for the algorithm.andP;  The lack of aconvergence theorem is not a problem in practice.andP;  However, this pleasantfeature of backpropagation was not discovered until recently, when digitalcomputers became fast enough to support large-scale simulations of neuralnetworks.andP;  The backpropagation algorithm was actually derived independentlyby a number of researchers in the past, but it was discarded as many timesbecause of the potential problems with local minima.andP;  In the days before fastdigital computers, researchers could only judge their idea by provingtheorems about them, and they had no idea that local minima would turn out tobe rare in practice.andP;  The modern form of backpropagation is often credited to[16, 22, 25, 31].andM;Backpropagation networks are not without real problems, however--the mostserious being the slow speed of learning.andP;  Even simple tasks requireextensive training periods.andP;  The XOR problem, for example, involves only fiveunits and nine weights, but it can require many passes through the fourtraining cases before the weights converge, especially if the learningparameters are not carefully tuned.andP;  Also, simple backpropagation does notscale up very well.andP;  The number of training examples required is superlinearin the size of the network.andM;Since backpropagation is inherently a parallel, distributed algorithm, theidea of improving speed by building special-purpose backpropagation hardwareis attractive.andP;  However, fast new variations of backpropagation and otherlearning algorithms appear frequently in the literature, e.g., [7].andP;  By thetime an algorithm is transformed into hardware and embedded in a computersystem, the algorithm is likely to be obsolete.andM;GeneralizationandM;If all possible inputs and outputs are shown to a backpropagation network, itwill (probably, eventually) find a set of weights that maps the inputs ontothe outputs.andP;  For many AI problems, however, it is impossible to give allpossible inputs.andP;  Consider face recognition and character recognition.andP;  Thereare an infinite number of orientations and expressions to a face, and aninfinite number of fonts and sizes for a character, yet humans learn toclassify these objects easily from only a few examples.andP;  We would hope thatour networks would do the same.andP;  And in fact, backpropagation shows promiseas a generalization mechanism.andP;  If we work in a domain where similar inputsget mapped onto similar outputs, backpropagation will interpolate when giveninputs it has never seen before.andM;There are some pitfalls, however.andP;  Figure 17 shows the common generalizationeffect during a long training period.andP;  During the first part of the training,performance on the training set improves as the network adjusts its weightthrough backpropagation.andP;  Performance on the test set (examples that thenetwork is not allowed to learn on) also improves, although it is never quiteas good as the training set.andP;  After a while, network performance reaches aplateau as the weights shift around, looking for a path to furtherimprovement.andP;  Ultimately, such a path is found, and performance on thetraining set improves again.andP;  But performance on the test set gets worse.andO;Why?andP;  The network has begun to memorize the individual input/output pairsrather than settling for weights that generally describe the mapping for allcases.andP;  With thousands of real-valued weights at its disposal,backpropagation is theoretically capable of storing entire training sets;with enough hidden units, the algorithm could learn to assign a hidden unitto every distinct input pattern in the training set.andP;  It is a testament tothe power to backpropagation that this actually happens in practice.andM;Of course it is undesirable for backpropagation to have that much power.andO;There are several ways to prevent it from resorting to a table-lookup scheme.andO;One way is to stop training when a plateau has been reached, on theassumption that any other improvement will come through cheating.andP;  Anotherway is to add deliberately small amounts of noise to the training inputs.andO;The noise should be enough to prevent memorization, but is should not begreat enough to confuse the classifier.andP;  A third way to help generalizationis to reduce the number of hidden units in the newtork, creating a bottleneckbetween the input and output layers.andP;  Confronted with a bottleneck, thenetwork will be forced to come up with compact internal representations ofits inputs.andM;Finally, there is the issue of exceptions.andP;  In many domains, there aregeneral rules, but also exceptions to the rules.andP;  For example, we cangenerally make the past tense of an English verb by adding &quot;-ed&quot; to it, butthis is not true of verbs like &quot;sing,&quot; &quot;think,&quot; and &quot;eat.&quot;andP;  When we show anetwork many present/past tense pairs, we would like it to generalize inspite of the exceptions--but not to generalize so far that the exceptions arelost.andP;  Backpropagation performs fairly well in this regard, as do simpleperceptrons, as reported in [26].andM;Boltzmann MachinesandM;A Boltzmann machine is a variation on the idea of a Hopfield network.andP;  Recallthat pairs of units in a Hopfield net are connected by symmetric weights.andO;Units update their states asynchronously by looking at their localconnections to other units.andM;In addition to serving as content-addressable memories, Hopfield networks cansolve a wide variety of constraint satisfaction problems.andP;  Each unit can beviewed as a hypothesis.andP;  Mutually supporting hypotheses are connected withpositive weights, and incompatible hypotheses are connected with negativeweights.andM;A major limitation of Hopfield network is that they settle into local minima.andO;In constraint satisfaction tasks we need to find the globally optimal stateof the network.andP;  This state corresponds to an interpretation that satisfiesas many interacting constraints as possible.andP;  Unfortunately, Hopfieldnetworks cannot find global solutions because they settle into stable statesvia a completely distributed algorithm.andP;  If a network reaches a stable statelike state A in Figure 4, that means no single unit is willing to change itsstate in order to move uphill; thus the network will never reach globallyoptimal state B.andP;  If several units decided to change state simultaneously,the network might be able to scale the hill and slip into state B.andP;  We need away to push networks into globally optimal states while maintaining ourdistributed approach.andM;Boltzmann machines solve this problem by employing a search technique calledsimulated annealing [15].andP;  Space limitations preclude a full discussion ofBoltzmann machines; for details, see [11].andP;  Briefly, units in a Boltzmannmachine update their individual binary states using stochastic rather thandeterministic rules.andP;  At first, units switch on and off randomly, but as thenetwork &quot;cools down,&quot; they approximate a Hopfield network.andP;  If the coolingprocedure is slow enough, a Boltzmann machine is guaranteed to avoid localminima.andP;  As in backpropagation networks, the weights of a Boltzmann machineare usually acquired via a learning algorithm.andM;Unsupervised LearningandM;Some networks, e.g., [3], do not receive target output values from a teacher,but instead only receive a real-valued signal indicating punishment orreward.andP;  These networks adjust their behavior to avoid future punishment.andM;What if a neural network is given no feedback for its outputs, not even areinforcement signal?andP;  Can the network learn anything useful?andP;  Theunintuitive answer is: yes.andP;  This form of learning is called unsupervisedlearning because no teacher is required.andP;  Given a set of input data, thenetwork is allowed to play with it to try to discover regularities andrelationships between the different parts of the input.andM;Learning is often made possible through some notion of which features in theinput set are important.andP;  But often we do not know in advance which featuresare important, and asking a learning system to deal with raw input data canbe computationally expensive.andP;  Unsupervised learning can be used as a&quot;feature discovery&quot; module that precedes supervised learning.andM;Consider the data in Figure 18.andP;  The group of 10 animals, each described byits own set of features, breaks down naturally into three groups: mammals,reptiles, and birds.andP;  We would like to build a network that can learn whichgroup a particular animal belongs to, and to generalize so that it canidentify animals it has not yet seen.andP;  We can easily accomplish this with asix-input, three-output backpropagation network.andP;  We simply present thenetwork with an input, observe its output, and update its weights based onthe errors it makes.andP;  Since without a teacher, however, the error cannot becomputed, we must seek other methods.andM;Our first problem is to ensure that only one of the three outpuyt unitsbecomes active for any given input.andP;  One solution to this problem is to letthe network settle, find the output unit with the highest level ofactivation, set that unit to 1, and set all other output units to 0.andP;  Inother words, the output unit with the highest activation is the only one weconsider to be active.andP;  A more neural-like solution is to have the outputunits fight among themselves for control of an input vector.andP;  The scheme isshown in Figure 19.andP;  The input units are directly connected to the outputunits, as in the perception, but the output units are also connected to eachother, via prewired negative, or inhibitory, connections.andP;  The output unitwith the most activation along its input lines initially will most stronglydampen its competitors.andP;  As a result, the competitors will become weaker,losing their power of inhibition over the stronger output unit.andP;  The strongerunit then becomes even stronger, and its inhibiting effect on the otheroutput units becomes overwhelming.andP;  Soon the other output units are allcompletely inactive.andP;  This type of mutual inhibition is calledwinner-take-all behavior.andP;  One popular unsurpervised learning scheme based onthis behavior is known as competitive learning.andM;In competitive learning, output units fight for control over portions of theinput space.andP;  A simple competitive learning algorithm is the following:andM;1.andP;  Present an input vector.andM;2.andP;  Calculate the initial activation for each output unit.andM;3.andP;  Let the output units fight until only one is active.andM;4.andP;  Increase the weights on connections between the active output unit andactive input units.andP;  This makes it more likely that the output unit will beactive next time the pattern is repeated.andM;A problem with this algorithm is that one output unit may learn to be activeall the time--it may claim all of the space of inputs for itself.andP;  Forexample, if all the weights on a unit's input lines are large, in will tendto bully the other output units into submission.andP;  Learning will only furtherincrease those weights.andM;The solution, originally due to Rosenblatt (and described in [27]), is toration the weights.andP;  The sum of the weights on a unit's input lines islimited to 1.andP;  Increasing the weight of one connection requires that wedecrease the weight of some other connection.andP;  Here is the learningalgorithm:andM;Algorithm: CompetitiveandM;LearningandM;Given: A network consisting of n binary-valued input units directly connectedto any number of output units.andM;Produce: A set of weights such that the output unit become active accordingto some natural division of the inputs.andM;1.andP;  Present an input vector, denoted ([x.sub.1], [x.sub.2] .andP;  .  . andO;[x.sub.n]).andM;2.andP;  Calculate the initial activation for each output unit by computing aweighted sum of its inputs.andP;  [8]andM;3.andP;  Let the output units fight until only one is active.andP;  [9]andM;4.andP;  Adjust the weights on the input lines that lead to the single activeoutput unit:andM;[Mathematical Expression Omitted]andM;where [w.sub.j] is the weight on the connection from input unit j to theactive output unit, [x.sub.j] is the value of the jth input bit, m is thenumber of input units that are active in the input vector that was chosen instep [1], and n is the learning rate (some small constant).andP;  It can be shownthat if the weights on the connections feeding into an output unit total 1before the weight change, they will still total 1 afterwards.andM;5.andP;  Repeat steps 1-4 for all input patterns, for many epochs.andM;The weight update rule in step 4 makes the output unit more prone to firewhen it sees the same input again.andP;  If the same input is presented over andover, the output unit will eventually adjust its weights for maximumactivation on that input.andP;  Because input vectors arrive in a mixed fashion,however, output units never settle on a perfect set of weights.andP;  The hope isthat each wil find a natural group of input vectors and gravitate toward it,that is, toward high activations when presented with those inputs.andP;  Thealgorithm halts wehn the weight changes become very small.andM;The competitive learning algorithm works well in many cases, but it has someproblems.andP;  Sometimes, one output unit will always win, despite the existenceof more than one cluster of input vectors.andP;  If two clusters are closetogether, one output unit may learn weights that give it a high level ofactivation when presented with an input from either cluster.andP;  In other words,it may oscillate between the two clusters.andP;  Normally, anotehr output unitwill win occasionally, and move to claim one of the two clsuters.andP;  However,if the other output units are completely unexcitable by the input vectors,they may never win the competition.andP;  One solution, called leaky learning, isto change the weights belonging to relatively inactive output units as wellas the most active one.andP;  The weight update rule for losing output units isthe same as in the algorithm above, except that they move their weights witha much smaller n (learning rate).andP;  An alternative solutin is to adjust thesensitivity of an output unit through the use of a bias, or adjustablethreshold.andP;  Recall that this bias mechanism was used in perceptrons, andcorresponded to the propensity of a unit to fire irrespective of its inputs.andO;Output units that seldom win inthe competitive learning process can be givenlarger biasses.andP;  In effect, they are given control over a larger portion ofthe input space.andP;  In this way, units that consistently lose are eventuallygiven a chance to win and adjust their weights in the direction of aparticular cluster.andM;Applications of NeuralandM;NetworksandM;The study of neural networks has yielded a number of techniques that havebeen used to approach difficult problems with some success.andP;  For example,Figure 20 shows how a backpropagation network can be trained to discriminateamong different vowel sounds, given a pair of frequencies taken from a speechwaveform.andP;  A good deal of connectionist research is also directed toward theproblem of machine vision.andP;  Neural networks provide a framework forintegrating the numerous constraint sources necessary for vision, in a highlyparallel fashion [2].andP;  Connectionist systems have been applied in many otherareas, including speech generation [28], combinatorial problems [13], gameplaying [29], signal processing [10], image compression [5], and roadfollowing [23].andM;Since all of these systems rely heavily on automatic learning, we can thinkof them as exercises in &quot;extensional programming&quot; [5].andP;  There exists somecomplex relationship between input and output, and we program thatrelationship into the computer by showing its examples from the real world.andO;Contrast this with traditional, &quot;intensional programming,&quot; in which we writerules or specialized algorithms without reference to any particular examples.andO;In the former case, we hope that the network generalizes to handle new casescorrectly; in the latter case, we hope that the algorithm is general enoughto handle whatever cases it receives.andP;  Extensional programming is a powerfultechnique because it drastically cuts down on knowledge acquisition time, amajor bottleneck in the construction of AI systems.andP;  However, currentlearning methods are not adequate for the extensional programming of verycomplex tasks, such as the translation of English sentences into Japanese.andM;Connectionist AI andandM;Symbolic AIandM;The connectionist approach to AI is quite different from the traditionalsymbolic approach.andP;  Both approaches are joined at the problem, as both try toaddress difficult issues in search, knowledge representation, and learning.andO;Let us list some of the methods they employ:andM;ConnectionistandM;* Search--Parallel relaxation.andM;* Knowledge Representation--Large number of real-valued connection strengths(Structures often stored as distributed patterns of activation).andM;* Learning--Backpropagation, Boltzmann machines, reinforcement learning,unsupervised learning.andM;SymbolicandM;* Search--State space traversal.andM;* Knowledge Representation--Predicate logic, semantic networks, frames,scripts.andM;* Learning--Macro-operators, version spaces, explanation-based learning,discovery.andM;The approaches have different strengths and weaknesses.andP;  One major allure ofconnectionist systems is that they employ knowledge representations that seemto be more learnable than their symbolic counterparts.andP;  Nearly allconnectionist systems have a strong learning component.andP;  However, neuralnetwork learning algorithms usually involve a large number of trainingexamples and long training periods, compared to their symbolic cousins.andO;Also, after a network has learned to perform a difficult task, its knowledgeis usually quite opaque--an impenetrable mass of connection weights.andP;  Gettingthe network to explain its reasoning, then, is difficult.andP;  Of course, thismay not be a bad thing.andP;  Humans, for example, appear to have little access tothe procedures they use for many tasks like speech recognition and vision.andO;It is no accident that the most promising uses for neural networks are inthese areas of low-level perception.andM;It is difficult to see how connectionist systems will tackle difficultproblems that symbolic, state-space search addresses (e.g., chess,theorem-proving, and planning).andP;  Parallel relaxation search, however, doeshave some advantages over symbolic search.andP;  First of all, it maps naturallyonto highly parallel hardware.andP;  When such hardware becomes widely available,parallel relaxation methods will be extremely efficient.andP;  More importantly,parallel relaxation search may prove very efficient because it can make useof states that have no analogues in symbolic search.andP;  If we freeze a networkwhile it is still settling, we may not be able to make sense out of thepattern of activity, but eventually, a consistent solution state falls out ofthe relaxation process.andP;  In contrast, a symbolic system can only expand newsearch nodes that correspond to valid, possible states of the world.andM;A good deal of connectionist research concerns itself with modeling humanmental processes.andP;  Neural networks seem to display many psychologically andbiologically plausible features such as content-addressable memory, faulttolerance, distributed representations, automatic generalization.andP;  Can weintegrate these desirable poperties into symbolic AI systems?andP;  Certainly,high-level theories of cognition can incorporate such features as newpsychological primitives.andP;  Practically speaking, we may want to useconnectionist architectures for low-level tasks such as vision, speechrecognition, and memory, feeding results from these modules into symbolic AIprograms.andP;  Another ideas is to take a symbolic notion, and implement it in aconnectionist framework.andP;  A connectionist production system is described in[30] and a connectionist semantic network is described in [6].andP;  Ultimately,connectionists would like to see symbolic structures emerge naturally fromcomplex interactions among simple units, in the same way that wetness emergesfrom the combination of hydrogen and oxygen, although it is an intrinsicproperty of neither.andM;Most of the promising advantages of connectionist systems described in thisarticle are just that: promising.andP;  A great deal of work remains to be done toturn these promises into results.andP;  Only time will tell how influentialconnectionist models will be in the evolution of AI research.andP;  In any case,connectionists can at least point to the brain's existence as proof thatneural networks, in some form, are capable of exhibiting intelligentbehavior.andM;(1) The stable state in which all units are inactive can only be reached ifit is also the initial state.andM;(2) Gradient descent is the same thing as hill climbing, modulo a change insign.andP;  Hill climbing is one of the weak methods often used in symbolic AI.andM;(3) Successful large-scale networks have used input-hidden-output topologieslike 960-9-45 [23], 203-80-26 [28], and 459-24-24-1 [29].andP;  A larger hiddenlayer results in a more powerful network, but too much power may beundesirable, as we will see later.andM;(4) The error formula is related to the derivative of the activationfunction.andP;  The mathematical derivation behind the backpropagation learningalgorithm is beyond the scope of this article.andM;(5) Again, we omit the details of the derivation.andP;  The basic idea is thateach hidden unit tries to minimize the errors of output units to which itconnects.andM;(6) A network with one hidden layer can compute any function that a networkwith many hidden layers can compute: with an exponential number of hiddenunits, one unit could be assigned to every possible input pattern.andP;  However,learning is sometimes faster with multiple hidden layers, especially if theinput is high nonlinar (i.e., hard to separate with a series of straightlines).andM;(7) Empirically, best results have come from letting [alpha] be zero for thefirst few training passes, then increasing it to 0.9 for the rest oftraining.andP;  This process first gives the algorithm some time to find a goodgeneral direction, then moves it in that direction with some extra speed.andM;(8) There is no reason to pass the weighted sum through a sigmoid function,as we did with backpropagation, because we only calculate activation levelsfor the purpose of singling out the most highly activated output unit.andM;(9) As mentioned earlier, any method for determining the most highlyactivated output unit is sufficient.andP;  Simulators written in a serialprogramming language may dispense with the neural circuitry and simplycompare activations levels to find the maximum.andM;ReferencesandM;[1] Ashby, W. R. Design for a Brain.andP;  Wiley, New York, 1952.andM;[2] Ballard, D. H. Parameter nets.andP;  Artif.andP;  Intell.andP;  22, 3 (1984), 235-267.andM;[3] Barto, A. G. Learning by statistical cooperation of self-interestedneuron-like computing elements.andP;  Human Neurobiology 4, 4 (1985).andM;[4] Block, H. D.andP;  The perceptron: A model for brain functioning.andP;  Rev.andP;  Mod.andO;Phy.andP;  34, 1 (1962), 123-135.andM;[5] Cottrell, G. W., Munro P., and Zipser D.andP;  Learning internalrepresentations from gray-scale images: An example of extensionalprogramming.andP;  In Proceedings of the Ninth Annual Conference of the CognitiveScience Society (1987), pp.andP;  461-473.andM;[6] Derthick, M. Mudane Reasoning by Parallel Constraint Satisfaction.andP;  Ph.Ddissertation, Carnegie Mellon University, Pittsburgh, PA., 1988.andM;[7] Fahlman, S. E. Faster-learning variations on back-propagation: Anempirical study.andP;  In Proceedings of the 1988 Connectionist Models SummerSchool.andP;  Morgan Kaufmann Publishers, San Mateo, Calif.andP;  1988, pp.andP;  38-51.andM;[8] Fahlman, S. E. and Hinton, G. E. Connectionist architectures forartificial intelligence.andP;  IEEE Comput.andP;  20, 1 (1987), 100-109.andM;[9] Feldman, J. A. and Ballard, D. H. Connectionist models and theirproperties.andP;  Cogn.andP;  Sci.andP;  6, 3 (1985), 205-254.andM;[10] Gorman, R. and Sejnowski, T. J. Analysis of hidden units in a layerednetwork to classify sonar targets.andP;  Neutral Networks 1, i (1988), 75-89.andM;[11] Hinton, G. E. and Sejnowski, T. J. 1986.andP;  Learning and relearning inBoltzmann Machines.andP;  In Parallel Distributed.andP;  D. E. Rumelhart, J. L.andO;McClelland, and the PDP Research Group Eds., MIT Press, Cambridge, Mass., pp.andO;282-317.andM;[12] Hopfield, J.J.andP;  Neural networks and physical systems with emergentcollective computational abilities.andP;  In Proceedings of the National Academyof Sciences USA 79, 8 (1982), pp.andP;  2554-2558.andM;[13] Hopfield, J. J. and Tank, D. W. 'Neural computation of decisions inoptimization problems.andP;  Biol.andP;  Cybern.andP;  52, 3 (1985), 141-152.andM;[14] Jordan, M. I. Supervised learning and systems with excess degrees offreedom.andP;  In Proceedings of the 1988 Connectionist Models Summer School.andO;Morgan Kaufmann Publishers, San Mateo, Calif., 1988, pp.andP;  62-75.andM;[15] Kirkpatrick, S., Gelatt, Jr., C. D., and Vecchi, M. P.andP;  Optimization bysimulated annealing.andP;  Science 220, 4598 (1983).andM;[16] LeCun, Y. Une procedure d'apprentissage pour reseau a seauilassynmetrique (a learning procedure for asymmetric threshold networks).andP;  InProceedings of Cognitiva 85, (Paris, 1985), pp.andP;  599-604.andM;[17] Lippmann, R. P. Review of research on neural nets for speech.andP;  NeuralComput.andP;  1, 1 (1989).andM;[18] McCulloch, W. S. and Pitts, W. A. logical calculus of the ideas immanentin neural nets.andP;  Bulletin of Math.andP;  Biophys.andP;  5 (1943), 115-137.andM;[19] Minsky, M.andP;  Neural Nets and the Brain-Model Problem.andP;  Ph.D dissertation,Princeton University, Princeton, New Jersey, 1954.andM;[20] Minsky, M. and Papert, S.andP;  Perceptions.andP;  Mit Press, Cambridge, Mass1969.andP;  Expanded also published in edition, MIT Press, 1988.andM;[21] Minsky, M. and Selfridge, O. G.andP;  Learning in neural nets.andP;  InProceedings of the Fourth London Sympossium on Information Theory (August 29to September 2, London).andP;  Academic Press, New York 1961.andM;[22] Parker, D. B.andP;  Learning Logic.andP;  Tech.andP;  Rep.andP;  TR-47, Mit Center forComputational Research in Economics and Management Science, 1985.andM;[23] Pomerleau, D. ALVINN: An autonomous land vehicle in a neural network.andO;In Advances in Neural Information Processing Systems 1, D. Touretzky, Ed.andO;Morgan Kaufmann, San Mateo, Calif., pp.andP;  305-313.andM;[24] Rosenblatt, F.andP;  Principles of Neurodynamics: Perceptrons and the Theoryof Brain Mechanisms.andP;  Spartan Books, Washington, D.C.andP;  1962.andM;[25] Rumelhart, D. E., Hinton, G. E. and Williams, R. J.andP;  Learning internalrepresentations by error propagation.andP;  In Parallel Distributed Processing, D.andO;E. Rumelhart, J. L. McClelland, and the PDP Research Group Eds.andP;  MIT Press,Cambridge, Mass., pp.andP;  318-362.andM;[26] Rumelhart, D. E. and McClelland, J. L. 1986.andP;  One learning the pasttenses of English verbs.andP;  In Parallel Distributed Processings, D. E.andO;Rumelhart, J. L. McClelland, and the PDP Research Group Eds.andP;  MIT Press,Cambridge, Mass.andP;  pp.andP;  216-271.andM;[27] Rumelhart, D. E. and Zipser, D.andP;  Feature discovery by competitivelearning.andP;  In Parallel Distributed Processing, D. E. Rumelhart, J. L.andO;McClelland, and the PDP Research Group Eds.andP;  MIT Press, Cambridge, Mass., pp.andO;151-193.andM;[28] Sejnowski, T. J. and Rosenberg, C. R.andP;  Parallel networks that learn topronounce English text, Complex Systems 1, 1 (1987), 145-168.andM;[29] Tesauro, G. and Sejnowski, T. J.andP;  A parallel network that learns to playbackgammon, Artif.andP;  Intell.andP;  39, (1989).andM;[30] Touretzky, D. and Hinton, G. E.andP;  A distributed connectionist productionsystem.andP;  Cog.andP;  Sci.andP;  12, 3 (1988), 423-466.andM;[31] Werbos, P. J.andP;  Beyond Regression: New Tools for Prediction and Analysisin the Behavioral Sciences.andP;  Ph.D.andP;  dissertation, Harvard University,Cambridge, Mass., 1974.andM;KEVIN KNIGHT is a Ph.D.andP;  candidate in computer science at Carnegie MellonUniversity.andP;  He is also a regular consultant to the Artificial IntelligenceLaboratory at MCC in Austin, Texas.andP;  His research interests include naturallanguage processing, unification, and neural networks.andO;</TEXT></DOC>