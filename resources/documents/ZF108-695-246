<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-695-246  </DOCNO><DOCID>08 695 246.andM;</DOCID><JOURNAL>IBM Systems Journal  June 1990 v29 n2 p287(10)* Full Text COPYRIGHT International Business Machines Corp. 1990.andM;</JOURNAL><TITLE>Segmenting discrete data representing continuous speech input.andO;</TITLE><AUTHOR>Faulk, R.D.; Gustavson, F. Goertzel.andM;</AUTHOR><SUMMARY>A probabilistic method for segmenting continuous speech intolexical units is described.andP;  The algorithm assumes initialconversion of the continuous speech signal to a discreterepresentation over some suitable alphabet.andP;  The problem ofdetermining such alphabets is not considered.andP;  Experiments usedkeyed input in English, French, German, and Russian.andP;  Wehypothesize that the low error rates obtained in the experimentscan also be achieved with data representing actual speech.andP;  Thepaper discusses an area of linguistic science, and outlines amethod for investigating it.andP;  (Reprinted by permission of thepublisher.)andM;</SUMMARY><DESCRIPT>Topic:     Natural Language InterfacesProbabilistic AutomataAlgorithm AnalysisLinguistics.andO;Feature:   illustrationtablechart.andO;Caption:   Summary of experiment results. (table)Grammatical product definitions. (chart)A segmentation example. (chart)andM;</DESCRIPT><TEXT>Segmenting discrete data representing continuous speech input A probabilisticmethod for segmenting continous speech into lexical units is described.andP;  Thealgorithm assumes initial conversion of the continuous speech signal to adiscrete representation over some suitable alphabet.andP;  The problem ofdetermining such alphabets is not considered.andP;  Experiments used keyed inputin English, French, German, and Russian.andP;  We hypothesize that the low errorrates obtained in the experiments can also be achieved with data representingactual speech.andP;  The paper discusses an rea of linguistic science, andoutlines a method for investigating it.andM;The achievement of natural-language communication between humans andcomputers, involving such capabilities as speech recognition and languagetranslation, must be preceded by some probabilistic understanding of howlinguistic skills are acquired in the presence of limited data and withoutbenefit of a priori knowledge of structure.andP;  Historically, this problem firstarose for linguists wishing to construct grammars of unanalyzed languagesfrom audio tapes of native speech.andP;  The linguist first had to transcribe therecorded speech data into a continuous stream of phonetic characters.andP;  Thenext step was to divide, or segment, the resulting character strings intomeaningful units such as words, roots, stems, and endings to further specifythe grammar.andP;  It is this second step in the process of grammar constructionthat is addressed in this paper, partly because of its traditional interestfor linguistics, but primarily because we believe it is crucial in thedevelopment of natural-language communication with computers.andM;Informal rules for segmentation were proposed in 1954 by the linguist ZelligHarris [1]--before the advent of modern computer science.andP;  At that time itwas not yet practical to raise questions about random sampling from naturallanguages and how much data might be required to obtain reliable results fromalgorithmic computation.andP;  This paper considers these questions and offerssome suggestions toward their solution.andM;Recent research has advanced the theory and application of grammaticalformalism in computing science.andP;  However, current linguistic theory does notexplain, nor even consider relevant, such phenomena as the intuitive abilityof linguists to segment speech data and that of illiterate bilinguals totranslate without explicit knowledge of grammar.andP;  The investigation of suchphenomena is an appropriate and challenging objective of linguistic research.andO;The fundamental problme of such research is to elucidate in probabilisticterms the properties of language data that make grammar constructionpossible.andP;  This knowledge can then serve as the basis for automaticconstruction and revision of local grammars based on the increasing amountsof ambient data available in modern information systems.andP;  Such a capabilitywould have numerous practical applications, including natural-languagecommunication with computers.andP;  One of the authors has discussed aprobabilistic method of language translation motivated by this approach.andO;[2,3]andM;Specifically, this paper describes an algorithm for segmenting continuousspeech utterances into lexical units.andP;  The algorithm assumes the initialconversion of the continuous speech signal to a discrete representation oversome suitable alphabet, but is independent of the choice of alphabet.andP;  Thedetermination of optimal alphabets for this purpose is a task for the type ofresearch described above.andP;  Thus, the words alphabet and character are usedhere for convenience, realizing that in practice, input data elements mightcorrespond to units of speech such as phones, allophones, phonemes, etc.andP;  Themethod requires initial training data, but no dictionary or other externalinformation about the input, and has yielded single-digit error rates inexperiments with keyed input in English, French, German, and Russian fromwhich all delimiting information (i.e., space, punctuation, andcapitalization) was removed.andP;  We hypothesize that comparable results can beobtained with data representing speech.andM;The method is essentially a generalization of the informal rules proposed byHarris and may be described as follows:andM;Given a string of input characters to segment, a variety index is computedfor each character of the input string, using frequency data obtained fromthe training set.andP;  The resulting computed values are smaller at the lastposition of lexical segments than at other positions of the input string.andO;Segmentation is accomplished by comparing the variety indices with athreshold.andM;For example, in one experiment the input data contained the sequenceandM;&quot;...thewomensblackcat...&quot;,andM;and the computed variety indices were as shown in Figure 1.andP;  Applying athreshold of .086 to these values produced the segmentation indicated in thefigure.andM;The principle involved is illustrated in the children's word-spelling gamecalled &quot;ghost.&quot;andP;  In ghost, a player announces the first letter of some word.andO;For most languages, this may in effect be any letter of the appropriatealphabet.andP;  The next player adds a second letter--thus specifying the firsttwo letters of some word--and so on, until one player cannot avoid giving aletter that completes the spelling of a word of four or more letters.andP;  Thatplayer earns a penalty and begins the next round.andP;  A variant of the gameallows letters to be added to the left or to the right of the current string.andO;A player may bluff, but scores a double penalty if challenged and unable tospecify a word beginning with (or containing) the proposed sequence.andP;  Theprogressive reduction in the number of available alternatives (i.e., thevariety of choices) during a round of ghost illustrates the process,described by Harris, [1] which is the conceptual basis for computing varietyindices.andM;Variety indices are thus probabilistic estimators of the number of possiblecharacters at successive positions of a natural-language input stream.andP;  Theunderlying assumption is that lexical segment boundaries are characterized bylow values of this quantity.andM;In the next sections of this paper, we discuss the data used to test themethod and the actual calculation and interpretation of variety indices.andO;Later sections describe the experimental data and methodologicalconsiderations, and some experiments and their results.andP;  Finally, someconclusions are offered.andM;Training dataandM;The term training data as used here refers to a corpus of lexically completespeech utterances represented in terms of some alphabet.andP;  Utterances may bewords, phrases, or sentences.andP;  It is not assumed, however, that theutterances conform to any syntactic rules of formation, but only that theyare composed of recurring lexical elements.andP;  For computational purposes theutterances in a corpus (C) are concatenated to form a single string (S) oflength L, containing L(L+1)/I substrings.andP;  For example, C might consist ofthe concatenation of the product of a list of spoken words.andP;  Such a corpusmight then be used to segment any utterance composed of words in the givenvocabulary as in Experiments 4 and 5, described later.andM;The computation of variety indices is defined in terms of the frequencies inS of the substrings of S.andP;  For this reason, it is useful to construct a tableof substring frequencies.andP;  The table need only contain entries for substringsof S with frequencies greater than 1 and for those of minimum length withfrequency equal to 1.andP;  For example, if the substrings ab and abc occur onlyonce in S, then only ab need be entered in the table.andP;  Every substring in Soccurs with a frequency equal to or greater than 1, with shorter substringsgenerally having higher frequencies.andM;A table can be constructed by generating, sorting, and counting thesubstrings of S.andP;  In practice, it is not necessary to generate all of thesubstrings of S, but only those of length less than or equal to somespecified maximum.andP;  Once generated, the information in the table is conduciveto the use of rapid lookup strategies.andM;Calculation and interpretation of variety indicesandM;The following discussion assumes the existence of a substring frequency tableT derived from some given corpus of utterances C.andP;  A new utterance orcharacter string (I) is to be segmented.andP;  I is not necessarily contained in Cbut presumably is drawn from the same language.andP;  The term bi-string refers toany ordered pair andless;[s.sub.1],[s.sub.2]andgt; of substrings of I such that both[s.sub.1] and [s.sub.2] either begin or end at the same position in I anddiffer in length by exactly one character, where [s.sub.1] denotes theshorter member of the pair.andP;  A bistring is left-aligned or right-aligned,depending on whether [s.sub.1] and [s.sub.2] begin or end in the sameposition of I.andP;  We refer to left-aligned and right-aligned bi-strings simplyas left and right bi-strings, respectively.andM;A bi-string ratio r is defined as a function of T and an arbitrary bi-stringof I:andM;r = f(T,andless;[s.sub.1],[s.sub.2]andgt;) = [f.sub.2]/[f.sub.1] found in T),andM;orandM;r = 1 ([s.sub.1] not found in T),andM;where [f.sub.1] and [f.sub.2] denote the frequencies in S of [s.sub.1] and[s.sub.2].andM;Values of r range over the unit interval.andP;  ThusandM;0 [is less than] r [is less than or equal to],andM;since the frequency of [s.sub.2], the longer member of andless;[s.sub.1],[s.sub.2]andgt;,can never exceed that of [s.sub.1], the shorter member.andP;  The value of rdetermined by andless;[s.sub.1],[s.sub.2]andgt; is referred to as a bi-string ratio, andis said to be proper if and only if [f.sub.1] [is greater than] 1.andP;  SeeFigure 2 for an illustration of the above terminology.andM;Let n=1, ...andP;  , N, where N denotes the length of I, and let I be treated ascircular, i.e., the last character of I is followed by the first.andP;  Values ofn correspond to characters in I.andP;  Then for each value of n, n determines setsof left bi-strings and sets of right bi-strings, which in turn determine setsof proper left and right bi-string ratios, respectively.andP;  (See Figure 2.)andM;A variety index v associated with character position n of i is defined as theproduct of the mean of the proper left bi-string ratios and the mean of theproper right bi-string ratios determined by n and the selected sets ofbi-strings.andP;  Trial runs were made using different sets of left and rightbi-strings as the basis for computation.andP;  In some cases, comparable goodresults were obtained.andP;  This paper presents the best results obtained using afixed set of bi-strings.andM;WE offer the number e as an interpretation of what is measured by thecalculated variety indices.andP;  If K is the size of the alphabet representingthe utterances in C, then e is defined asandM;e = int[(v)(K - 1)] + 1.andM;Since 0 [is less than] v [is less than or equal to] 1,andM;1 [is less than or equal to] e [is less than or equal to] K.andM;In other words, e is the estimated number of characters that might haveoccurred with variety index v at position n of the input string.andP;  As statedin the introduction, the method assumes that low values of this quantitycharacterize segment boundaries.andM;Because calculation of the values of v for the N positions of I can eachproceed independently, the proposed method has strong potential forimplementation in a parallel processing environment.andM;Experimental data and methodologicalandM;considerationsandM;Data for the experiments were obtained by random sampling from grammaticalproduct languages.andP;  A grammatical product language (GPL) is a set ofsentences defined by one or more arrays of lists such that the sequenceobtained by randomly selecting one element from each list in a specifiedarray is always a sentence of the language.andP;  Such an array of lists is calleda grammatical product definition (GPD), and the set of sentences it defines,a grammatical product (GP).andP;  To exhibit any given sentence as a GP member,one need only include one or more of the sentence elements in a list ofpossible atlernatives.andP;  the resulting sequence of list is a GPD.andP;  In general,natural languages can be viewed and exhibited in terms of GPDs as unions ofdisjoint grammatical products.andM;A GPL may be any desired subset of a natural language, or an arbitrarylanguage of interest for experimental purposes.andP;  As a particular case, a GPmay be defined as a product of a single list, where the n-place product of agiven list is understood to be the set of ordered n-tuples that can be formedfrom elements of the list.andP;  Figure 3 defines the GPLs from which data for thepresent five experiments were obtained by random sampling.andP;  For the firstthree experiments, sentences were generated by choosing one element randomlyfrom each list in a GPD.andP;  For the last two, the language was considered to beany sequence of elements in the given vocabulary.andP;  In terrs of grammaticalproducts, such languages are unions of disjoint list products.andP;  For thepurposes of the present experiments, the elments in the sequences obtained bythe above method were concatenated to form continuous character strings.andM;The concept of grammatical products effectively implements the operation ofrandom sampling from languages, as distinguished from random sampling fromdiscursive data such as newspaper text, literary text, or speech.andP;  Thisdistinction is fundamental for the purposes of the present type of research.andO;The precise formulation and testing of linguistic hypotheses and thefacilitating of the design and testing of programs intended for large-scalelinguistic computations are made possible using GPL data.andP;  GPL data areeasier to obtain than statistically equivalent discursive data and allowexact formulation of desired program behavior and performance evaluation.andO;Examples of hypotheses are (1) the on illustrated in this paper where lexicalboundaries are characterized by low values of the variety index, and (2) inthe case of language translation, that similar sentences have similartranslations.andP;  [2,3]andM;Next discussed is the concept of sampling units relative to random samples ofsentences drawn from a grammatical product.andP;  If r is the number of sentencesin a random sample drawn from the GP, then U, the number of sampling units inthe sample, is defined as follows:andM;U = r/mandM;where m = the product of the two longest list sizes in GPD.andM;As defined above, m is the size of the minimal subset or subsets in GP suchthat every possible pair of list elements that can occur together in asentence of GP occurs in at least one sentence of the subset.andP;  (If GP isdefined as the product of a single list, then m is simply the square of thelist size.)andM;The size of a grammatical product language is the sum of the sizes of thegrammatical products included in it.andP;  The size (N) of a single grammaticalproduct is the product of the list sizes in the associated GPD.andP;  Nif GP isdefined as th eproduct of simple list of L elements, then N is simply[L.sup.k] where K is the length of sentences of GP, and the size of theentire language is the sum of the powers of L from 1 to an arbitrarilyspecified maximum sentence length.)andP;  Table 1 documents the language sizes andthe number of sampling units involved in the present experiments.andP;  Note thatit is the number of sampling units, rather than the language size, thatcorrelates positively with the error rates.andP;  The calculation of error ratesis explained below.andM;ExperimentsandM;The experiments were performed using an IBM XT personal computer.andP;  Theprograms were written in BASIC and are available upon request, along with ademonstration version of an experiment in the form of an educational exhibit.andO;The segmentation program obtains input strings from the training data, or,optionally, the strings may be entered manually by the user.andP;  Spaces suppliedby the user do not enter into the calculation of variety indices, but if theinput string does contain blanks, the program computes a threshold value thatminimizes the total number of insertion and omission erros.andP;  This thresholdis used to measure the success rate.andP;  In either case, the experimenter isallowed to enter and see immediately the effect of different threshold valueson the output strings.andP;  Figures 1, 4, and 5 show some typical results of thesegmentation process.andP;  In each figure, the input string is shown above one ormore of the computed variety indices, threshold, and error counts.andP;  Thevariety indices are compared to the threshold and when an index is less thanthe threshold, a blank is inserted after the corresponding character todesignate a segment boundary.andP;  Before contains the input string as enterd bythe experimenter, with (optional) spaces indicating the expected locations ofsegment boundaries.andP;  (The error counts are meaningful only when this optionis exercised).andP;  The segmented output string appears as After.andM;Figure 3 presents the GPDs for the GPLs in the experiments.andP;  Experiment 1defines a GPL consisting of one 60-sentence GP, presented here primarily forthe sake of illustration, though it is worth noting that a considerablevariety of structure can be captured even in a small GPD.andP;  Experiment 2defines the GPL of an experiment first performed (but not reported) withcomparable experiments in German and Russian in 1976.andP;  This GPL is the unionof two grammatical products containing 186 624 and 36 000 sentences for atotal of 222 624, of which 492 contain the lexically ambiguous sequence(themeatthemeat&quot;.andP;  Experiment 3 is taken verbatim from the grammaticalproduct definition referred to as a &quot;word wheel&quot; in Reference 4.andP;  Experiments4 and 5 illustrate GPLs defined as unions of list products.andM;ResultsandM;Table 1 summarizes the results of the experiments.andP;  We observe that varietyindices computed by the proposed method can be as much as an order ofmagnitude smaller at segment boundaries than at other input positions (seeFigure 1).andP;  The results also demonstrate the ability of the proposed methodto discover possibly unsuspected lexical structure (see Figure 5).andP;  Theresults of Experiments 4 and 5 confirm the expectation of a strongcorrelation between observed error rates and the number of experimentalsampling units, as defined in the section on experimental data.andM;ConclusionsandM;While the volume of computations required by the present algorithm is large,it has strong potential for implementation in parallel processingenvironments because the calculation of each variety index is independent ofthe others.andP;  It is also possible to store the resulting lexical segments andtheir frequencies for use in the more expeditious proceeding of subsequentinput.andP;  This paper identifies an unexplored area of linguistic science,outlines a method for its investigation, and demonstrates the feasibility ofthe proposed method of speech segmentation as a practical solution.andM;AcknowledgmentsandM;We are grateful to Winfred P. Lehmann of the University of Texas for pointingout the relevance of Harris' work on morpheme recognition, and for hiscomments on our work over the yearsandgt; to Kenneth Powell of IBM CorporateMarketing Education (now retired), without whose vision, interest, andencouragement this paper would not have been writtenandgt; and to Jay Friedman ofthe IBM Systems Journal staff, for his invaluable assistance.andM;Cited referencesandM;[1.] Z. S. Harris, &quot;From Phoneme to Morpheme,&quot; Language 31, No.andP;  2, 190-222(1954).andM;[2.] R. Faulk, The Phenomenon of Interlingual Correspondence: A QuantativeFormulation of the Translation Problem for Natural Languages, Research ReportRC-1501, IBM Thomas J.andP;  Watson Research Center, Yorktown Heights, NY 10598(1965).andM;[3.] R. Faulk, &quot;An Inductive Approach to Language Translation,&quot;Communications of the ACM 7, No.andP;  11, 647-653 (1964).andM;[4.] C.K.andP;  Ogden, Basic English, Harcourt, Brace and Company, New York, NY(1934), p. 305.andM;Ramon D. Faulk  16 North Broadway, apt.andP;  3H, White Plains, New York 10601.andO;Mr. Faulk obtained his B.A.andP;  degree in philosophy and French in 1959 at theUniversity of Texas in Austin.andP;  While doing graduate work in linguistics hebegan developing a probabilistic approach to machine translation.andP;  In 1961 hejoined the IBM Thomas J. Watson Research Center In Yorktown, New York, tocontinue that effort, the results of which appeared in 1964 and 1965.andP;  In1969 he joined the IBM Field Engineering Headquarters in White Plains, NewYork, as a staff programmer, while privately continuing his research incomputer linguistics.andP;  A preliminary series of experiments in 1976 yieldedresults leading to those reported here.andP;  Mr. Faulk was a recent member of theTranslation Project in the Speech Recognition Department of the ResearchCenter In Yorktown.andM;Fran Goertzel Gustavson  Information Systems Department, Pace University,Bedford Road, Pleasantville, New York 10570.andP;  Dr. Gustavson obtained her A.B.andO;degree in 1963 in mathematics and French at Vassar College, an M.S.andP;  inmathematics from New York University in 1967, and a Ph.D.andP;  in computerscience from Polytechnic Institute of New York in 1979.andP;  From 1964 to 1969she was a research staff member at the IBM Thomas J. Watson Research Centerin Yorktown, New York.andP;  She joined Pace University in 1971 and is currentlyProfessor of Information Systems and Chairperson of the Information SystemsDepartment.andO;</TEXT></DOC>