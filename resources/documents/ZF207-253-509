<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-253-509  </DOCNO><DOCID>07 253 509.andM;</DOCID><JOURNAL>Dr. Dobb's Journal of Software Tools  Jan 1989 v14 n1 p32(13)* Full Text COPYRIGHT Mandamp;T Publishing Inc. 1989.andM;</JOURNAL><TITLE>Neural nets and noise filtering.andO;</TITLE><AUTHOR>Klimasauskas, Casey.andM;</AUTHOR><SUMMARY>Back-propagation-type neural networks offer powerful architecturesfor noise filtering.andP;  Such networks are particularly suited fordeveloping adaptive filtering techniques that can be tuned toisolate varying ranges of signal detail.andP;  One technique useful forencoding and compression of input data is dimensionalityreduction, which can reduce a large number of inputs to a smallnumber of outputs.andP;  Dimensional reduction implements well on aneural network for noise filtering where there is a highcorrelation between adjacent time samples of the signal.andP;  Neuralnetworks are also for the transformation of linear combinations ofinputs through the use of nonlinear functions, facilitating moreflexible control in the analysis of noisy data.andP;  Several examplesof noise filtering are implemented on NeuralWare's NeuralWorksProfessional II neural network development environment.andM;</SUMMARY><DESCRIPT>Topic:     Neural NetworksNoiseFilteringApplicationsCost Benefit AnalysisUtilization.andO;Feature:   illustrationchart.andO;Caption:   An input signal is sampled at several points. (chart)A scatter diagram of several x-inputs and the resulting y-outputs.andO;(chart)Noise filtering with five hidden units. (chart)andM;</DESCRIPT><TEXT>Neural Nets and Noise FilteringandM;Neural networks are an information-processing technology inspired by studiesof the brain and nervous system.andP;  Despite (or, perhaps, because of) theirorigins, certain types of neural networks have a strong founding inmathematics.andP;  One network type in particular, back-propagation, is a powerfuladaptive technique for approximating relationships between several continuousvalued inputs and one (or more) continuous valued output.andP;  This articlediscusses applications of back-propagation to filtering out noise, or,conversely, identifying fundamental underlying signals.andP;  An example,discussed later, is in isolating EKG signals taken from a noisy environment.andM;This article addresses the problem of developing a limited number of &quot;featuredetectors&quot; that can account for the maximum amount of a signal (in aleast-mean-mean-square sense).andP;  In doing this, the noise is assumed to berandom and of a higher frequency than the underlying signal.andP;  The approachtaken here is to find a way of encoding or compressing the input data andthen reexpanding it.andP;  The compression process eliminates portions of theinput data, which represent small or nonrecurring featurs.andP;  By selecting thenumber of &quot;encodes,&quot; you can vary the amount of detail retained in thetransformation.andP;  Figure 1, page 31, illustrates this process.andP;  Key to solvingthis problem is the selection of a good set of feature detectors for theparticular signal to be to filtered.andM;Notice that in Figure 1 the 11 inputs have been reduced at the output of theencoder stage to 3 outputs.andP;  If you were to represent each input as well asthe output of each encoder as a 32-bit floating-point number, it would take352 bits (11 x 32) to represent the input to the system.andP;  It would take 96bits (3 x 32) to represent the outut of the system.andP;  From this perspective,you have compressed the input data by a factor of 96/352.andP;  This form of datacompression is also known as dimensionality reduction.andP;  You have reduced thenumber of dimensions from 11 to 3.andP;  For this system to work well, you must besure that the following two interrelated criteria are met: 1.andP;  There must besome relationship between input variables; and 2.andP;  You must use an encodingsystem that can represent the relationship.andM;It may not be apparent at first that these two criteria are related, butconsider the situation in which the input sampels are from aconstant-amplitude sine wave.andP;  In this case, you could encode the dataexactly with a single &quot;sine-wave&quot; detector in which the output is related tothe position in the cycle.andP;  On the other hand, trying to use another type ofdetector function may require several detectors to develop a &quot;piece wise&quot;approximatiion of the function.andP;  If there is no relationship between theinput variables, dimensionality reduction often results in a series of fixedoutputs representing the &quot;average&quot; value of a combination of inputs.andP;  In thecase of noise filtering (where the inputs are a series of time-sequentialsamples), there is usually a high degree of correlation (or relationship)between adjacent time samples.andP;  Dimensionality reduction works very well inthis situation.andM;With a little thgouht, it becomes apparent that if the signal you are workingwith has a noise component added to it, the contribution of the noise to thesignal will tend to be random or nonstationary, with respect to other largerfeatures.andP;  As such, the noise will most likely be one of the featureseliminated in the encoding process.andP;  If the noise component is of arelatively low frequency and accounts for a majority of the energy in theoutput signal (for example, 60-cycle hum), this process will detect thenoise.andP;  Subtracting the output of the detector from the input signal removesthe noise from the signal.andM;To approach the problem of feature detection, first consider a simplerproblem of mapping from a single continuous input to a single continuousoutput.andP;  Assume that the input and output are related by some continuousdifferentiable function.andP;  Figure 2, this page, shows a plot of input values&quot;X&quot; and output values &quot;Y&quot;.andM;A first attempt to approximate the relationship uses a linear relationship.andO;This, too, is shown in Figure 2.andP;  The slope of this line can be determinedusing statistical techniques (linear regression) or by using adaptivesignal-processing techniques (Bernard Widrow).andP;  Rather than using a straightline, you could have used a quadratic polynomial.andP;  Figure 3, this page, showshow a quadratic polynomial might fit the data shown.andP;  Both of thesetechniques can be generalized to two, three, or more inputs.andP;  Notice thatregardless of the number of inputs, each feature detector has exactly oneoutput.andM;For both of the cases just described, the decoded output is assumed to beidentical to the output of the encoders.andP;  Consider a more complex situationin which you use a &quot;sigmoidal&quot; feature detector.andP;  Figure 4, page 34, shows anexample of a sigmoid function.andP;  You might use this function to create twoencoders as shown in Figures 4b and 4c.andP;  Now, if you use a decoder that takesthe difference of the output of the two encoders, you have the function shownin Figure 4d.andP;  Using this method, you have been able to synthesize a &quot;bump.&quot;andO;By varying the ai0term, you shift the transition point.andP;  Varying the ai1parameter changes the slope at the transition.andP;  In this way, you could havecreated a &quot;bump&quot; of any size, any location, and independently vary the slopeof each of its sides.andP;  By adding more &quot;bumps&quot; together, you can createarbitrarily complex functional relationships between one or more inputs and asingle output.andM;Though bumps can be used to create arbitrarily complex functions, this maynot always be the best method.andP;  At times, it may be better to use since orother types of functions in the encoders.andP;  Using &quot;since&quot; encoders isequivalent to Fourier decomposition of the inputs.andP;  Unlike the standardFourier transform, this technique accounts for both the phase and amplitudeof the sampled input.andM;At this point, you might observe that what has been discussed sounds likeadaptive signal-processing, but with two stages instead of one.andP;  And you areright to an extent.andP;  An adaptive unit is identical to the &quot;linear&quot; encodershown in Figure 2.andP;  If you were to take two linear encoders and try combiningtheir outputs together in a linear combination, you can easily show that thisis identical to a single linear element.andP;  The novel characteristics of theneutral network approach is that the output of the weighted sum (linearcombination of the inputs) is transformed using a nonlinear function such asa sigmoid or sine function.andP;  These nonlinearities make the creation ofmultilevel systems possible and are responsible for several of the resultingcharacteristics.andM;Why use this approach?andP;  The two basic reasons are as follows.andP;  First, it iscapable of retaining a level of detail greater than other techniques.andP;  Ratherthan simply ignoring small features, it can allow them to pass if they are ofa stationary recurring nature.andP;  Second, the amount of detail retained by thefiltering process can be varied by varying the number of elements in thehidden layer.andP;  From these perspectives, it is a technique providingadditional flexibility and control when the engineer analyzes noisy data.andM;The approach described thus far is to develop a limited number of featuredetectors that encode a series of samples of an input signal.andP;  The output ofthese encoders is used to reconstruct all or a portion of the input sample.andO;The use of a limited number of feature detectors ensures that some of theinformation (preferably the noise) will be removed in the reconstruction.andM;Putting the Theory Into PracticeandM;Until now, the problem of how to compute the various coefficients used by afeature detector or encoder has been ignored.andP;  The process is a form ofregression.andP;  The word &quot;regression&quot; comes from a study done in the early1900s.andP;  In this study, the researchers attempted to find relationshipsbetween the height of parents and that of their children.andP;  They developed astatistical technique for fitting a straight line through the data (linearregression).andP;  The results of the study showed that tall parents had shorterchildren and that short parents had relatively taller children--their heightswere regressively correlated.andP;  The term regression analysis applies to anytechnique for fitting a curve to data.andM;The technique used to set the coefficients in a network is described as a setof differential equations that modify the coefficients in such a way as toreduce the mean-square-error of the output for the training set.andP;  These arederived by defining the error for a single pass through a training set(several presentations of input and expected, or desired, output) as the sumof the squares of the difference of the actual and expected outputs.andP;  This iscalled the square error.andM;The derivative of each weight (or coefficient) in the network is computed asa function of the error.andP;  For most cases, these differential equations do nothave a closed-form solution.andP;  As such, the solution (or a solution) isderived by using numerical methods.andP;  The numerical techniques computer thedirection to change the coefficients (gradient) and then change them slightlyin that direction.andP;  The mathematics behind this are explained in detail inChapter 8 of Parallel Distributed Processing by Rumelhart and McClelland (MITPress 1986).andM;Listing one, page 96, presents a back-propagation network program that cansolve the &quot;exclusive OR&quot; problem descirbed in the Rummelhart-McClelland book.andO;This program could also be extended to solve the noise filtering problem.andP;  Ifyou want to extend the program in Listing One to solve the &quot;noise filtering&quot;problem, you must do three things:andM;1.andP;  Increase the number of processing elements in the input and output layersto 19 each and the number of processing elements in the hidden layer to 5, 7,10, or 19.andM;2.andP;  Delete all of the existing connections and fully connect the input layerto the hidden layer and the hidden layer to the output layer.andP;  Likewise fullyconnect the bias to both the hidden and output layers.andM;3.andP;  Write a module to load a series of noise data into memory and presentthem to the network.andM;For the purposes of studying the noise filtering problem, I used a commercialneural network development environment (NeuralWorks Professional II byNeuralWare) for all of the examples that follow because neural networkdevelopment systems provide the infrastructure needed to solve the problem.andO;These systems provide the network building tools that make it easy to changethe network design, interfaces to assist in setting up the problem, tools forinvestigating what is happening within the network itself, and a host ofother &quot;bookkeeping&quot; functions.andP;  A good development environment also enablesyou to focus on the problem.andP;  Though most neural network types appearrelatively straightforward from the standpoint of mathematics, manyprogrammers find it difficult to debug this kind of numerical-oriented code.andO;If a handcoded network fails to converge, the problem may lie either in the&quot;code&quot; or the network parameters.andP;  A neural network development environmentsolves the first of these problems and makes the other problems easier totest.andM;Figures 5a, page 42, and 5b, 5c, and 5d, page 44, show screen dumps of fourtests of training a network to filter out noise.andP;  The examples use 5, 7, 10,and 19 elements in the hidden layer.andP;  As you would expect from the previousdiscussion, the more processing elements in the hidden layer, the more detailis preserved from the encoding process to decoding process.andM;The data displayed in the lower window of each of the examples is the rawinput data.andP;  This is actually EKG data taken in a nosy environment (the noiseis caused primarily by fluorescent lights).andP;  The filtered output data isdisplayed in the upper window.andP;  The networks used here each had 19 inputs and19 outputs.andP;  The &quot;filtered output&quot; in the upper window is the output of thecenter processing element of the output layer.andP;  Experimentally, the outputsof the other processing elements in the output layer do a similar job oftracking the input.andM;So far, we have looked at the problem from the perspective of doingdimensionality reduction on 19 inputs to produce 19 outputs.andP;  The way wetrained the network was to randomly select a block of 19 sequential samplesfrom the raw data, and to use this as both the input and desired output ofthe network.andP;  This was typically repeated 300,000 to 600,000 times for eachof the networks shown (overnight on a NEC Powermate III).andP;  Each time a blockof inputs was applied to the network, the weights were slightly adjusted tomake the actual network output closer to the desired output.andP;  Only the middleoutput element is used as the &quot;filtered&quot; output.andM;You might wonder what would happen if you were to have only one outputelement that was trained to reproduce the value of the middle input.andP;  Figure6, this page, shows the results of such a network with five elements in thehidden layer.andM;Closely examining the weights in the network, it becomes clear that theoutput is affected almost exclusively by the center input.andP;  All other inputsare ignored.andP;  Neural network techniques are sometimes smarter than you mightexpect!andP;  The use of additional processing elements in the output layer forcesthe feature detectors to come up with a more &quot;general&quot; way of encoding theinputs.andM;During the recall process, the raw data is shifted through a 19-element longshift register and the output computed with that set of inputs.andP;  The data isshifted one position and the process repeated.andM;The training process is reasonably repeatable with the data shown.andP;  We havetried it on a variety of other signals (manually constructed) with someinteresting results.andP;  Depending on the type of signal used, the processingelements in the hidden layer should use a &quot;sine&quot; function rather than asigmoid.andP;  As mentioned previously, this causes the network to do a form ofFourier decomposition and reconstruction.andM;ConclusionandM;As a general technique, neural network-based techniques for noise filteringoffer an interesting and potentially powerful approach.andP;  one of theparticularly interesting characteristics of neural nets is the capability todevelop an adaptive filtering technique that can be tuned to preserve varyingdegrees of detail.andP;  Significant experimentation remains to develop thesetechniques to the point where they are well characterized and understood.andO;</TEXT></DOC>