<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-412-870  </DOCNO><DOCID>07 412 870.andM;</DOCID><JOURNAL>Communications of the ACM  July 1989 v32 n7 p802(9)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Virtual video editing in interactive multimedia applications.andO;(Special Section) (technical)</TITLE><AUTHOR>Mackay, Wendy E.; Davenport, Glorianna.andM;</AUTHOR><SUMMARY>Multimedia tools and applications under development at MIT areexamined.andP;  Principles and techniques of user-controlled videoediting have been integrated into four multimedia environments:documentary film makers order video segments; educational softwaredesigners create interactive learning environments; human factorsresearchers use tools that help them record, analyze and presentdata; and users of on-line communication systems share and modifyinteractive video data.andP;  People tend to be influenced bybroadcast-based television, so that notions of interactivity oftenare limited.andP;  It is shown that different applications makedifferent demands on interactivity.andP;  The work referred to here isbeing done at MIT's Media Laboratory, which connects advancedscientific research with innovative communications media, andProject Athena, a $100-million experiment in education, also atMIT.andM;</SUMMARY><DESCRIPT>Topic:     Massachusetts Institute of TechnologyInteractive videoDigital VideoResearch and DevelopmentColleges and UniversitiesMassachusetts Institute of Technology. Media LaboratoryMassachusetts Institute of Technology. Project AthenaCommunicationMultimedia.andO;Feature:   illustrationtablephotograph.andO;Caption:   Multimedia tools and applications under development at MIT.andO;(table)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Virtual Video Editing in Interactive Multimedia Applications Earlyexperiments in interactive video included surrogate travel, training,electronic books, point-of-purchase sales, and arcade game scenarios.andO;Granularity, interruptability, and limited look ahead were quickly identifiedas generic attributes of the medium.andP;  Most early applications restricted theuser's interaction with the video to traveling along paths predetermined bythe author of the program.andP;  Recent work has favored a more constructivistapproach, increasing the level of interactivity by allowing users to build,annotate, and modify their own environments.andM;Today's multitasking workstations can digitize and display video in real-timein one or more windows on the screen.andP;  Users can quickly change their levelof interaction from passively watching a movie or the network news toactively controlling a remote camera and sending the output to colleagues atanother location.andP;  In this environment, video becomes an information stream,a data type that can be tagged and edited, analyzed and annotated.andM;This article explores how principles and techniques of user-controlled videoediting have been integrated into four multimedia environments.andP;  The goal ofthe authors is to explain in each case how the assumptions embedded inparticular applications have shaped a set of tools for buildingconstructivist environments, and to comment on how the evolution of acompressed digital video data format might affect these kinds of informationenvironments in the future.andM;ANALOG VIDEO EDITINGandM;One of the most salient aspects of interactive video applications is theability of the programmer or the viewer to reconfigure video playback,preferably in real time.andP;  The user must be able to order video sequences andthe system must be able to remember and display them, even if they are notphysically adjacent to each other.andP;  It is useful to briefly review theprocess of traditional analog video editing in order to understand both itsinfluence on computer-based video editing tools and why it is so important toprovide virtual editing capabilities for interactive multimedia applications.andM;A video professional uses one or more source videotape decks to select adesired video shot or segment, which is then recorded onto a destinationdeck.andP;  The defined in and out points of this segment represent thegranularity at which a movie or television program is assembled.andP;  At anygiven point in the process, the editor may work at the shot, sequence, orscene level.andP;  Edit controllers provide a variable speed shuttle knob thatallows the editor to easily position the videotape at the right frame whileconcentrating on the picture or sound.andP;  Desired edits are placed into a listand referenced by SMPTE time code numbers, which specify a location on avideotape.andP;  A few advanced systems also offer special features such as iconicrepresentation of shots, transcript follow, and digital sound stores.andM;The combined technology of an analog video signal and magnetic tape presentslimitations that plague editors.andP;  Video editing is a very slow process, farslower than the creative decision-making process of selecting the next shot.andO;The process is also totally linear.andP;  Some more advanced (and expensive) videoediting systems such as Editflex, Montage, and Editdroid allow editors topreview multiple edits from different videotapes or videodiscs beforeactually recording them.andP;  This concept of virtual viewing was firstimplemented for an entirely different purpose in the Aspen Interactive Videoproject of 1980, almost four years before Montage and Editdroid were born.andM;Some features of analog video editing tools have proven useful forcomputer-based video editing applications.andP;  However, the creation ofapplications that provide end users with editing control has introduced anumber of new requirements.andP;  Tool designers must decide who will have editingcontrol, when it will be available, and how quickly it must be adjusted tothe user's requirements.andP;  Some applications give an author or programmer soleediting control; the end user simply views and interacts with the results.andO;In others, the author may make the first cut but not create any particularinteractive scenario; the user can explore and annotate within the databaseas desired.andP;  Groups of users may work collaboratively, sharing the task ofcreation and editing equally.andP;  The computer may also become a participant andmay modify information presentation based on the content of the video as wellas on the user's behavior.andM;Different perspectives in visual design also affect editing decisions.andP;  Forexample, an English-speaking graphic designer who las out print in booksknows that the eye starts in the upper left-hand corner and travels from leftto right and downward in columns.andP;  A movie director organizes scenesaccording to rules designed for a large, wide image projected on a distantmovie screen.andP;  Subjects are rarely centered in the middle, but often movediagonally across the screen.andP;  Video producers learn that the center of thetelevision screen is the &quot;hot spot&quot; and that the eye will be drawn there.andP;  Ineach case, the information designer is visually literate, but takes forgranted conventions that are not recognized by the others.andP;  Conflicts arisefrom the clash in assumptions.andP;  Although new conventions will probably becomeestablished over time, visual designers will for now need tools to suggestreasonable layouts and users need to be able to override those suggestionsbased on current needs.andM;MULTIMEDIA TOOLS AND APPLICATIONSandM;Assumptions about the nature of video and of interactivity deeply affect theways in which people think about and use interactive video technology.andP;  Weare all greatly influenced by television, and may be tempted to place videointo that somewhat limited broadcast-based view.andP;  Notions of interactivitytend to extend about as far as the buttons on a VCR: stop, start, slowmotion, etc.andP;  However, different applications make different demands on thekinds and level of interactivity:andM;* Documentary film makers want to choose an optimal ordering of videosegments.andM;* Educational software designers want to create interactive learningenvironments for students to explore.andM;* Human factors researchers want to isolate events, identify patterns, andsummarize their data to illustrate theories and phenomena.andM;* Users of on-line communication systems want to share and modify messages,enabling the exchange of research data, educational software, and other kindsof information.andM;The authors have been involved in the development of tools and applicationsin each of these areas, and have struggled with building an integratedenvironment that includes digitized video in each.andP;  This work has beeninfluenced by other work, both at MIT and at other institutions, and thespecific tools and applications have influenced each other.andM;Table I summarizes four sets of multimedia tools and applications underdevelopment at MIT's Media Laboratory and Project Athena and successivelyidentifies video editing issues raised by each.andP;  Most of this work has beendeveloped on a distributed network of visual workstations at MIT.andP;  This tableis not intended to be exhaustive; rather, it is intended to illustrate thedifferent roles that digitized video can play and trace how theseapplications and tools have positively influenced each other.andM;INTERACTIVE DOCUMENT ARIESandM;Movie-making is highly interactive during the making, but quite intentionallyminimizes interaction on the part of the audience in order to allow viewersto enter a state of revery.andP;  To a filmmaker, editing means shaping acinematic narrative.andP;  In the case of narrative films, predefined shootingconstraints, usually detailed in a storyboard, result in multiple takes ofthe same action; the editor selects the best takes and adjusts the length ofshots and scenes to enhance dramatic pacing.andP;  Editor's logs are an integralaid in the process.andP;  Music and special sound effects are added to enhance thecinematic experience.andM;Good documentary on the other hand tries to engage the viewer in anexploration.andP;  The story is sculpted first by the filmmaker on the fly duringshooting.andP;  As all shots are unique, editing involves further sculpting byselecting which shots reflect the most interesting and coherent aspects ofthe story.andP;  Editors must balance what is available against exactly what hasalready been included and selecting those shots that will make the sequenceat hand as powerful as possible.andP;  A major adjustment in one sequence mayrequire the filmmaker to make dozens of other adjustments.andM;A City in Transition: New Orleans, 1983-86andM;The premise of the case study, &quot;A City in Transition: New Orleans, 1983-86,&quot;was that cinema in combination with text could provide insights into thepeople, their power and the process through which they affect urban change.andO;Produced by Glorianna Davenport with cinematography by Richard Leacock, the2-hour 40-minute film was edited for both linear and interactive viewing.andO;The interactive version of this project was developed as a curriculumresource for students in urban planning and political science.andP;  Rather thancreating a thoroughly scripted interactive experience, faculty who use thisvideodisc set design a problem that motivates the student as explorer.andP;  Theeditor's log is replaced with a database that contains text annotation andancillary documentation which are critical for indepth analysis andinterpretation.andM;An Interactive Video Viewing and Editing ToolandM;A generic interactive viewing and editing tool was developed to let viewersbrowse, search, select, and annotate specific portions of this or othervideodisc movies.andP;  Each movie has a master database of shots, sequences, andscenes; a master &quot;postage stamp&quot; icon is associated with each entry.andP;  Authorsor users can query existing databases for related scenes or ancillarydocumentation at any point during the session or they can build new databasesand cross-reference them to the main database.andP;  The Galatea videodisc server,designed b Daniel Applebaum at MIT, permits remote access to the video.andP;  Thecurrent prototypes include four to six videodisc players and a full audioback-away crosspoint switcher; for some applications this is sufficient toachieve seamless playback of the video.andM;A viewer can watch the movie at his or her own pace by controlling aspring-loaded mouse on a shuttle bar or pressing a play button.andP;  The user canpause at any point to select and mark a segment.andP;  Segments can be saved bystoring a representative postage stamp icon on a palette; the icon can laterbe moved to an active edit strip where it is sequenced for playback.andP;  Once ashot has been defined, the user can annotate it in a number of associateddatabases.andP;  A graphical representation of the shot or edit strip allowsadvanced users to mark audio and video separately (Figure 1).andM;The traditional concept of edit-list management is used to save and updatelists made by users as they fill a palette or make an edit strip.andP;  Users cangive each new list a name and a master icon--a verbal and visual memory aid;these are then used to place sequenced lists into longer edits or intomultimedia documents.andP;  The database design significantly expands theinformation available to both the computer and the editor about a particularshot.andM;Future DirectionsandM;The goal of this experiment is to represent shots and abstract story modelsin a way that allows the computer to make choices about what the viewer wouldlike to see next and create a cohesive story.andP;  The kind of information neededto make complex editing decisions may be roughly divided into two categories:(1) content (who, what, when, where, why); and (2) aesthetics (cameraposition relative to object position in a time continuum).andM;Much of the information that is now entered into a database manually could beencoded during shooting or extracted from the pictures using advanced signalprocessing techniques.andP;  Digital video will also allow the computer togenerate new views of a scene from spatially encoded video data.andP;  Finally, itwill become easier to mix computer graphics with real images, which will bothencourage the creation of new constructivist environments and make all videosuspect.andM;INTERACTIVE LEARNING ENVIRONMENTSandM;The use of digitized video in education spans a range of educationalphilosophies from goal-oriented tutorials to open-ended explorations.andP;  Theunderlying philosophy tends to dictate the level of interactivity and videoediting control given to authors and users of any program.andP;  Programmedinstruction and its successors are interactive in the sense that a studentmust respond to the information presented; it is not possible to be a passiveobserver.andP;  However, only the author has flexible video editing capabilities;the student is expected to work within the structures provided, following thedesignated paths to reach criterion levels of mastery of the information.andO;Hypermedia provides the user with a wider range of opportunities to explore,by following links within a network of information nodes.andP;  An even richerform of interactivity allows users to actively construct their environments,not just follow or even explore.andP;  The constructivist approach providesstudents with some of the same kinds of editing and annotation tools asauthors of applications.andM;The Navigation Learning EnvironmentandM;In 1983, Wendy Mackay, working with members of the Educational ServicesResearch and Development Group at Digital Equipment Corporation, began a setof research projects in multimedia educational software.andP;  The goals were tocompare different instructional strategies, improve the software developmentprocess and address the technical problems of creating multimediaobject-oriented databases and learning environments.andP;  Coastal navigation waschosen as a test application to push the limits of the technology.andP;  Not onlydoes it require real-time handling of a complex set of real images, symbols,text, and graphics, but it has also been presented to students using a widerange of educational philosophies, ranging from structured military trainingto open-ended experiential learning at Outward Bound.andM;The heart of the navigation information database is a videodisc containingover 20 discrete types of information, including nautical charts, aerialviews, tide tables, navigation instruments, graphs and other referencematerials.andP;  The videodisc also contains over 10,000 still photographs takensystematically from a boat in Penobscot Bay, Maine, to enable a form ofsurrogate travel similar to the MIT Aspen project mentioned earlier.andM;The synchronization of images in three-dimensional space was essential to thevisualization of this application.andP;  The project leader, Matt Hodges, broughtthe ideas and videodiscs to the Visual Computing Group at Project Athena.andO;This project became one of the inspirations for the development of AthenaMuse, a collaborative effort with Russ Sasnett and Mark Ackerman.andM;Athena MuseandM;At Project Athena, the required spatial dimensions for the Navigation diskwere generalized to include temporal and other dimensions.andP;  In particular,several foreign language videodiscs funded by the Annenburg Foundation werebeing produced under the direction of Janet Murray.andP;  An important goal was toprovide cultural context in addition to practice with grammar and vocabulary.andO;Students were presented with interactive scenarios featuring native speakers.andO;In order to respond correctly, students needed to understand the speakers.andO;Thus, they needed subtitles synchronized to the video and the ability tocontrol them together.andM;The concept of user-controllable dimensions was created as a general solutionto the control of spatially organized material (as in the Navigation project)and temporally organized material (as in the foreign language projects).andO;Athena Muse packages text, graphics, and video information together andallows them to be linked in a directed graph format or operated independently(Figure 2).andP;  Different media can be linked to any number of dimensions whichcan then be controlled by the student or end user.andM;When reimplemented in Athena Muse, the Navigation Learning Environment usedseven dimensions to simulate the movement of a boat.andP;  Two dimensionsrepresent the boat's position on the water and two more represent the boat'sheading and speed.andP;  A fifth tracks the user's viewing angle and the sixth andseventh manage a simulated compass that can be positioned anywhere on thescreen.andP;  The user can move freely within the environment and use the toolsavailable to a sailor to check location (charts, compass, looking in alldirections around the boat) to set a course.andP;  Other aspects of a simulationcan be added, such as other boats, weather conditions, uncharted rocks, etc.andO;Here, the user does not change the underlying structure of the information,since it is based on constraints in the real world, but he or she can movefreely, ask questions, and save information in the form of notes andannotations for future use.andM;Future DirectionsandM;Direct recording of compressed digital video data may provide moresophisticated ways to embed visual data into simulations of real andartificial environments.andP;  Issues of synchronization will change, as somekinds of data are encoded as part of the signal.andP;  For example, subtitles orsign language for the hearing impaired will be an integral part of theoriginal video, obviating the need for external synchronization.andP;  Other kindsof data will continue to require externally-controlled synchronization, tohandle the changing relationships among data in different applications.andO;Users should have more sophisticated methods of controlling these associateddata types, either in real time or under program control.andM;Cross-referencing of visual information across databases will also be easier.andO;For example, a photograph of an island, taken from a known location in thebay, must currently be linked by hand to the other forms of information(charts, aerial views, software simulations) from which it might also beviewed.andP;  Digitally encoded representations of these images will make iteasier to calculate these relationships.andM;Digital encoding would also allow views to be created that were not actuallyphotographed.andP;  For example, the eight still images representing a 360-degreeview from the water could be converted into a moving video sequence in whichthe user appears to be slowly turning in a circle.andP;  If enough visualinformation has been encoded, it may also be possible to create the illusionof moving in any direction across the water.andP;  Digital representation willalso make it easier to provide smooth zooming in and out to view closeups ofparticular objects.andM;Given advances in limited domain natural language recognition and naturallanguage parsing, it may be possible to automatically encode the audioportion of a video sequence.andP;  Future versions of the foreign languageprojects would then allow students to engage in more sophisticated dialogs.andM;VIDEO DATA ANALYSISandM;Video has become an increasingly prevalent form of data for social scientistsand other researchers, requiring both quantitative and qualitative analysis.andO;The term video data can have several meanings.andP;  To a programmer, video datais an information coding scheme, like ASCII text or bitmapped graphics.andP;  To aresearcher, who uses video in field studies or to record experiments, videodata is the content, rather than the format, of the video.andM;The requirements researchers place upon video editing go far beyond thecapabilities of traditional analog editing equipment.andP;  They want to flexiblyannotate video and redisplay video segments on the fly.andP;  They often need tosynchronize video with other kinds of data, such as tracks of eye movementsor keystroke logs.andP;  They want methods for exploring their data at differentlevels of granularity, identifying patterns through recombination orcompression and summarizing it for other researchers.andM;A clear priority for many researchers is to reduce the total amount ofviewing time required for a particular type of analysis.andP;  Researchers who usemultiple cameras have an even more difficult problem; they must eithersynchronize multiple video streams and view them together or significantlyincrease viewing times.andP;  Some researchers must also share control of theirdata, maintaining the ability to create and modify individual annotationswithout affecting the source data.andP;  Just as with numerical data, differentresearchers should be able to perform different kinds of analyses on the samedata and produce different kinds of results.andM;The ability to integrate video and computers has spurred the development ofnew tools to help researchers record, analyze and present the latter form ofvideo data.andP;  (A number of these tools are described in.)andP;  Wendy Mackaycreated a tool, EVA, written in Athena Muse to help analyze video data froman experiment on intelligent tutoring.andP;  The goal was to allow the researcherto annotate video data in meaningful ways and use existing computer-basedtools to help analyze the data.andM;EVA: An Experimental Video AnnotatorandM;EVA, or Experimental Video Annotator, allows researchers to create their ownlabels and annotation symbols prior to a session and permits live annotationof video during an experiment.andP;  In a typical session, the researcher beginsby creating software buttons to tag particular events, such as successfulinteractions or serious misunderstandings (Figure 3).andP;  During a session, asubject sits in front of a workstation and begins to use a new softwarepackage.andP;  A video camera is directed at the subject's face and a log of thekeystrokes may be saved.andP;  The researcher sits at the visual workstation andlive video from the camera appears in a window on the screen.andP;  Another windowdisplays the subject's screen and an additional window is available fortaking notes.andP;  The researcher has several controls available throughout thesession.andP;  One is a general time stamp button, which the researcher presseswhenever an interesting but unanticipated event occurs.andP;  the rest are buttonsthat the researcher created prior to the session.andM;Annotation during the session saves time later when the researcher is readyto review and analyze the data.andP;  The researcher can quickly review thehighlights of the previous session and mark them for more detailed analysis.andO;The original tags may be modified and new ones created as needed.andP;  Tags canbe symbolic descriptions of events, recorded patterns of keystrokes, visualimages (single-frame snapshots from the video), patterns of text (from atranscription of the audio track), or clock times or frame numbers.andP;  Tagsthat refer to the events in different processes, such as the text of theaudio and the corresponding video images, can be synchronized and addressedtogether.andM;Note that annotation of live events, while useful, requires intenseconcentration.andP;  The mechanics of taking notes may cause the researcher tomiss important events, and events will often be tagged several seconds afterthey occur.andP;  Subsequent passes are almost always necessary to create preciseannotations of events.andP;  While EVA does not address all of the generalproblems of protocol analysis, it does provide the researcher with moremeaningful methods for analyzing video data.andM;Future DirectionsandM;Digital video offers possibilities for new kinds of data analysis, both inthe discovery of patterns across video segments and in the understanding ofevents within a segment.andP;  Video data can be compressed and summarized inorder to provide a shortened version of what occurred, to tell a story abouttypical events, to highlight unusual events or to present collections ofinteresting observations.andP;  The use of highlights can either conciselysummarize a session or completely misrepresent it.andP;  Just as with quantitativedata, it is important to balance presentation of the unusual data points(outliers) with typical data points.andP;  In statistics, the field of exploratorydata analysis provides rigorous methods for exploring and seeking outpatterns in quantitative data.andP;  These techniques may be applied profitablyhere.andM;An application that requires graphic overlays over sections of video needs amethod of identifying the video frame or frames, a method for storing theoverlay and method for storing the combination of the two.andP;  Other kinds ofannotation could permit later redisplay of the video segments under programcontrol.andP;  For example, linkages with text scripts would enable a program topresent all instances in which the subject said &quot;aha!&quot;andP;  Researchers could userules for accessing video segments with particular fields; for example, allsegments might have a time stamp and a flag to indicate whether or not thesubject was talking.andP;  Then a rule might be: If (time andgt; 11:00) and (voice on)then display the segment.andM;MULTIMEDIA COMMUNICATIONandM;Electronic mail and telephones provide two separate forms of long-distancecommunication, both involving the exchange of information.andP;  Early attempts toincorporate video into long distance communication were based on a model ofthe telephone, simply adding video to the audio channel.andP;  A differentstrategy is to incorporate video into on-line message systems, such aselectronic mail, on-line consulting systems, and bulletin boards.andP;  The latterapplications enable users to exchange information asynchronously, rather thansetting up a synchronous link.andP;  If video is included, the video annotationtechniques described earlier can be used to help recipients identifyinteresting messages and handle their mail more efficiently.andM;Scientists are likely candidates for using multimedia communication systems,because they face an increasing need to compare their data from photographsand video sources with that obtained by other scientists.andP;  The NeuroanatomyResearch Database, developed by Steven Wertheim under a faculty grant fromProject Athena, provides a set of multimedia messages and a set ofcommunication requirements that can be used to define the functionality of amultimedia message system.andM;The Neuroanatomy Research DatabaseandM;The Neuroanatomy Research Database began with a database of terms about theanatomy of the brain, including over 1200 text definitions of anatomicalparts, video of a brain dissection, slides of brain cross sections, and athree-dimensional model of the brain reconstructed from these brain sections(Figure 4).andP;  Video portions of the database could be located remotely andaccessed by any video workstation through the Galatea video network server.andO;A single image or video sequence can be incorporated into as many electronicpages as appropriate, without duplicating the images.andP;  Future work willextend the database to enable scientists to include images from their ownexperiments and compare them with existing images.andM;Pgymalion: A Multimedia Message SystemandM;Pygmalion is a multimedia message system based on a three-dimensional modelfor on-line communication systems developed by Wendy Mackay and Win Treese.andO;The model allows users to control message exchanges along three dimensions:synchronous or asynchronous, stream or database and control by sender or thereceiver.andP;  All messages, including text, graphics and video are stored in adatabase at a central post office and a pointer to the message is sent to theuser.andP;  The user has the impression that individual messages are local, butthey are actually stored whether it is most convenient in the network.andO;Current technology requires the video network to be separate from thecomputer network, but conceptually, they can be treated in the same way.andM;The first test of Pygmalion will be at the SIGGRAPH '89 Conference, where apreliminary version will be made available to conference attendees.andP;  Over30,000 people will be able to send and receive multimedia messages from adistributed network of workstations.andP;  The software is being developed byindividuals from MIT's Project Athena, the Media Lab, MIT's Sloan School ofManagement, and Digital's Cambridge Research Laboratory.andM;An important aspect of managing information overload at the conference is toprovide a balance between control over access to messages between senders andreceivers.andP;  Senders who are willing to individually identify the recipientsof a message can send messages directly, as in electronic mail.andP;  Senders whowant to reach a large audience must annotate their messages and send them toa central database, as in an electronic bulletin board or on-lineconferencing system.andP;  Recipients then receive personally addressed messagesand can browse the database for messages of interest.andP;  Attendees will be ableto record and edit messages containing video as well as text and graphics.andO;An important feature is that all the messages can be edited by both thesender and the receiver of the message, which allows for shared control ofthe information.andM;Future DirectionsandM;A multimedia message system designed for a one-week event such as SIGGRAPH'89, must address different design constraints than a system designed fordaily use.andP;  However, since large scale implementations of the latter are notcurrently technically feasible, implementation of the former may provideuseful information about the design considerations for such systems.andO;Pygmalion will allow us to explore balance of control issues between sendersand receivers of messages, the effectiveness of different annotation andretrieval schemes, particularly for multimedia messages, and the managementof scarce video resources.andM;Video takes a lot of room.andP;  A laser videodisc, which holds 54,000 frames, hasonly 30 minutes of video per side.andP;  Write-once disks are available, but theper-disk cost is high.andP;  The speed of access to the video is important, andhas implications for how the video is laid down on the disk.andP;  The Pygmalionsystem stores only one copy of each message and mails a pointer, rather thanthe actual video, text, and graphics.andP;  (Note that this model works at Athena,which has 850 workstations in a distributed network, but does not address theproblems of sending multi-media messages over larger distances.)andM;Sharing presents a problem in that one person's annotation may not make senseto another person.andP;  Also, message senders must perform extra work to annotatemessages, which decreases the likelihood that it will be done.andP;  Messagetemplates, with fields to be filled in, are one approach.andP;  Another is toprovide the option of asking the sender for annotations before the message issent.andP;  What if two people try to manipulate video at the same time?andP;  Itdepends, of course, on whether they're cooperating or competing, and whatthey're trying to accomplish.andP;  It also depends on whether they are makingdecisions about a fixed order of video segments, e.g., looking at differentviews of the same visual database or providing different kinds ofannotations.andP;  In all of these cases, the participants are working together toconstruct a shared information environment.andM;CONCLUSIONSandM;Computer control of analog video has been commercially available tobroadcasters for almost two decades and in personal computing environmentsfor almost a decade.andP;  The requirements for interactivity and flexiblemanipulation of video and associated data are increasing and emphasize thelimits of current technology.andP;  Analog video storage on optical videodiscs isexpensive and videotape does not provide sufficiently accurate or precisecontrol.andP;  Cooperative work applications, as in shared authorship ofmultimedia documents, and video prototyping, as in interactive design ofmultimedia software, are promising new application areas that will extend therequirements of video as an information medium.andM;New techniques for compressing video and encoding content and otherinformation into the signal promise to both decrease the costs and increasethe possibilities for future applications.andP;  It is important that assumptionsabout current and future video-based applications be taken into account whenchoosing among compression tradeoffs.andP;  Some applications require real-timedecompression and can afford to wait for a long compression process.andP;  Othersmay require the reverse.andP;  Different algorithms cause delays at differentpoints in the compression/decompression cycle.andP;  Thus, some applications thatrequire high quality images and long video segments may not find a 30 secondpreliminary delay problematic.andP;  Other applications may require very rapidchanges among short segments, in which delays of several seconds may not beacceptable.andP;  The importance of image quality varies across applications, andthe advent of HDTV may further change the requirements.andP;  All new technologiespresent tradeoffs between costs and features.andP;  Before deciding on thosetradeoffs, it is important for manufacturers of digital video equipment tounderstand the ways in which it will be used.andP;  The tools and applicationspresented in this article are biased toward a constructivist point of view,which maximizes user control over the media and the environment.andP;  Digitalvideo offers the potential for significantly enhancing them all and expandingour views of interactivity.andM;Acknowledgments.andP;  The authors would like to thank Geoffrey Bock, AndyLippman, Lester Ludgwig, and Deborah Tatar for early comments on thisarticle.andP;  The work described here occurred in a stimulating environment atMIT and has been influenced by many creative people.andP;  We would like to thankHal Birkeland, Hans Peter Brondmo, Jeff Johnson, Patrick Purcell, and BenRubin at the Media Lab; Ben Davis, Matt Hodges, Evelyn Schlusselberg, WinTreese, and Steve Wertheim from Project Athena; and Mark Ackerman, DanApplebaum, Brian Gardner, Brian Michon, and Russ Sasnett who have beenassociated with both the Media Lab and Project Athena.andM;The authors would also like to thank the participants and reviewers for theACM/SIGCHI Workshop on Video as a Research and Design Tool, particularlyAustin Henderson, Deborah Tatar, Raymonde Guindon, Marilyn Mantei, and LucySuchman.andO;</TEXT></DOC>