<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-330-720  </DOCNO><DOCID>08 330 720.andM;</DOCID><JOURNAL>Communications of the ACM  April 1990 v33 n4 p449(11)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Efficient decoding of prefix codes. (data compression algorithms)</TITLE><AUTHOR>Hirschberg, Daniel S.; Lelewer, Debra A.andM;</AUTHOR><SUMMARY>The effectiveness of data compression depends on applicationcharacteristics as well as the efficiency of the algorithm.andP;  Adata structure that lets a decoder with severely constrainedmemory contain Huffman-coded files sent by a powerful encoder ispresented.andP;  Four distinct methods of decoding prefix codes inlimited memory space are divided into two categories based ondata-structure choice.andP;  The first category is based on an implicitrepresentation of the Huffman code tree structure, while thesecond uses a 'canonical' code concept under which the Huffmanalgorithm is needed only to compute the length of codewords to bemapped to dictionary entries.andP;  Actual codewords may be specifiedin different ways to increase efficiency.andP;  The choice of avariation on the canonical code system depends on the parametersof a particular application.andM;</SUMMARY><DESCRIPT>Topic:     Data CompressionAlgorithmsHuffman CodePerformance Improvement.andO;Feature:   illustrationtablechart.andO;Caption:   An example dictionary. (table)A Huffman tree for the example dictionary. (chart)The canonical Huffman code tree for the example dictionary ofFigure 2. (chart)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Efficient Decoding of Prefix Codes Data compression is an important andmuch-studied problem.andP;  Compressing data to be stored or transmitted canresult in significant improvements in the use of computing resources.andP;  Thedegree of improvement that can be achieved depends not only on the selectionof a data compression method, but also on the characteristics of theparticular application.andP;  That is, no single data compression algorithm willbe superior in every application.andP;  The very meaning of &quot;superior&quot; isapplication dependent.andP;  While the goal of data compression is to represent amessage as succinctly as possible, a particular application may modify thatgoal by placing additional requirements on the performance of the datacompression system.andP;  In other words, the application may define parameterswhich guide the selection of the data compression method.andP;  For example, theseparameters include knowledge about the type of data to be compressed andconstraints on memory usage and execution speed.andM;The work we describe here is based on a specific data compression applicationin which:andM;* textual data is to be transmitted and received over a communication line,andM;* decoding must be performed on-line, andandM;* the amount of memory available during the decode operation is very limited.andM;The encoder in our data compression system is allowed substantialcomputational resources.andP;  It can expend significant time and space to find acompact representation of the source text.andP;  Once the representation isconstructed, it will be transmitted to the decoder.andP;  The decoder may beviewed as a special-purpose translator with very limited space.andP;  This spacelimitation provides an interesting challege.andM;The application on which this research is based involves a minimal memoryconstraint.andP;  While technology is providing increasing amounts of memory atlow cost, minimizing the use of memory will always be a goal in massproduction applications of data compression.andM;We employ a dictionary compression technique, that is, an algorithm whichcompresses a source text by replacing strings of characters in the source bypointers to a dictionary.andP;  The dictionary is a collection of n strings ofvarying lengths.andP;  Long dictionary entries have higher potential forcompression than short ones in that we replace a large number of characterswith a single codeword.andP;  We must, however, also take into account thefrequency with which a dictionary entry occurs in the source text.andP;  We wantto assign short codewords to frequently occurring strings; if a string occursonly rarely its codeword may be too long to provide good compression eventhough the string being replaced is itself quite long.andP;  The degree ofcompression to be achieved by a dictionary compression system is largelydependent on the choice of the dictionary; however, it is also necessary torepresent the pointers efficiently.andP;  We choose to represent pointers byprefix codes based on the relative frequencies of the dictionary entries theyrepresent.andP;  The Huffman code is the most widely-known prefix code and isminimal in that it provides the best compression of any prefix code appliedto a fixed dictionary [5].andP;  Arithmetic codes, which are not prefix codes, canprovide better compression than the Huffman code when applied to the samedictionary [11].andP;  This improved compression is possible because arithmeticcodes are not constrained to map an integer number of bits to each dictionaryentry.andP;  The additional compression they provide is generally a few percent.andM;An offsetting advantage of Huffman codes is that they are more robust.andP;  Whilean error in a single bit will prevent the bits that follow from beingcorrectly decoded by an arithmetic decoder, Huffman codes tend toresynchronize quickly, thus localizing damage [6].andP;  A more importantconsideration in terms of the present application is that arithmetic codinguses the frequencies of the dictionary entries during decoding.andP;  Our methodsdo not require the table of frequencies, and as a result we are able todecode with a much smaller space requirement.andP;  For these reasons we elect touse Huffman coding for our application.andM;The compressed version of the source text consists of a representation of:andM;(1) the encoding dictionary,andM;(2) its prefix code, andandM;(3) the sequence of codes that can be expanded to recover the originial text.andM;Most of the compression is achieved by choosing an appropriate dictionary.andO;The computation of the corresponding prefix code is straightforward.andP;  Themethod of representing the dictionary and the prefix code, however, alsoaffects the resulting compression ratio (for moderate-sized files, therepresentation choice can have a significant impact on the compressionratio).andP;  The encoder in our application must construct a representation whichis compact and which our space-limited decoder can translate efficiently.andO;The way in which the encoder represents the dictionary and the prefix code isthe focus of our work.andP;  We partition the encoding dictionary into two parts:andM;(1a) a stream of characters andandM;(1b) information that permits parsing this stream into individual dictionaryentries (e.g., the lengths of the entries or their starting positions).andM;All of our methods prepend the stream of characters to the encoded text andstore the characters as part of the decode data structure.andP;  It is in the waythat (1b) and (2) are represented that the methods differ.andP;  We will comparedthe decode space efficiencies of our methods and the amount of representationoverhead they incur.andP;  We define representation overhead to be the number ofbytes in the compressed text that are used to represent items (1b) and (2).andO;We allow the decoder some limited set-up time to receive the coderepresentation (items 1 and 2) and store the information needed forperforming translation.andP;  Except for the lag due set-up, the decoder mustoperate on-line.andP;  That is, the time required for decoding must beproportional to the size of the expanded source.andM;In order for our methods to be presented in the most general form, we definethe variables listed in Figure 1.andP;  It should be noted that N [is greater thanor equal to] lg n bits, (1) that M [is less than or equal to] V, that B [isless than or equal to] V, and that A [is greater than or equal to] C since wemust be able to access any dictionary entry with an address.andP;  Figure 2presents a small example dictionary which we use to illustrate our methods.andM;PREVIOUS METHODSandM;A number of papers have appeared on the subject of implementations of Huffmanencoding and decoding.andP;  These implementations apply to any prefix code.andP;  Themore recent of these papers [2, 8] concentrate on fast implementations andthe reduction of processing time by avoiding manipulation of individual bits.andO;A price is paid, however, for the reduced time requirements in the form ofincreased memory requirements.andM;Sieminski's method requires 64K bytes to store the decode tables for a simplesituation in which the dictionary contains only 127 individual characters.andO;The size of the decode tables grows exponentially if dictionary entrieslonger than one character are used [8].andP;  The method of [2] requiresO(n.sup.2] extra space where n is the number of dictionary entries.andP;  Whileprocessing time is of concern, our primary criterion is the efficient use ofinternal memory during decoding.andP;  Thus, these methods are inappropriate forour purposes.andM;Hankamer [4] describes a modified Huffman procedure with reduced memoryrequirements.andP;  The reduced memory requirements are attained by reducing thesize of the dictionary and computing a suboptimal Huffman code.andP;  Hankamer'smethod assumes fixed-length dictionary entries, and there is no obviousextension to variable-length entries.andP;  This fact, coupled with the loss ofoptimality, renders the method inappropriate for our needs.andP;  Tanaka [10 givesa finite automaton-based Huffman decoding algorithm.andP;  His method assumessingle character dictionary entries.andP;  A straightforward modification to allowfor variable-length entries is similar to our Method A1 (which follows) interms of execution speed, but requires approximately 67 percent more memory.andM;METHOD AandM;Our first solution to the problem of decoding in restricted memory uses theHuffman code tree to represent the dictionary.andP;  We do not use, however, theobvious linked implementation in which each internal node contains poiners toits left and right subtrees; the space requirements of this implementationare prohibitive.andP;  Instead, Method A1 employs an implicit representation ofthe tree structure.andP;  Method A2 is a variation of Method A1 which providesimproved storage utilization.andM;Method A1andM;Method A1 uses a total of nC + (n - 1)A space in addition to the spacerequired for the n dictionary entries (the space for a dictionary entry isthe space required to store the characters that make up the entry).andP;  The coderepresentation and the dictionary are stored as a single structure.andP;  Theprefix code is represented by the corresponding binary tree stored inpreorder form.andP;  Preorder storage is defined recursively: the root node isstored first, followed by its left subtree stored in preorder form, and thenits right subtree in preorder form.andP;  In our storage scheme, a leaf nodecontains a flag bit (set to one, distinguishing between internal nodes andleaves), the length of the corresponding dictionary entry, and the entryitself.andP;  For each internal node we store two items, a flag bit (set to zero)and an address.andP;  The address component of an internal node is the address ofits right subtree.andP;  The left subtree for an internal node is storedimmediately by following the node itself.andP;  A tree with n leaves contains n -1 internal nodes.andP;  Thus, the total storage in addition to the dictionaryentries is nC for the leaf nodes and (n - 1)A for the internal nodes,assuming that there is a spare bit in the address and length fields.andP;  In ourapplication, for which the typical values given in Figure 1 apply, thestorage requirement is 3n - 2 bytes.andP;  Figure 3 shows a Huffman tree for theexample dictionary.andP;  The codeword for each dictionary entry appears under theentry.andP;  We use the convesntion that left branches are labeled &quot;0&quot; and rightbranches &quot;1.&quot;andP;  Figure 4 gives the corresponding decode data structure.andP;  Werepresent tree nodes as tuples of the form (0, address) or (1, length,entry).andP;  The address values are based on allowing 2 bytes for an address (A =2) and 1 byte for each character and each string length (C = 1).andP;  We assumethat the first bit of an address or length field stores the flag bit.andM;The storage scheme just described allows for simple decoding.andP;  For eachcodeword we begin at the first position of the decode table, and we decodeone bit at a time.andP;  On a 0 bit we move from an internal node to its leftchild by advancing over the address field.andP;  On a 1 bit we use the addressfield to move to the right subtree of the current internal node.andP;  We continueto decode bits until a flag value of 1 is encountered, indicating a leafnode.andP;  At this point we have detected the end of a codeword and located thecorresponding dictionary entry.andP;  The dictionary entry is appended to thedecoded output, and we return to the first position of the decode table readyto decode the next codeword.andP;  The following operations are performed for eachcodeword in the encoded source.andP;  We use address (k) to represent the addresscomponent of an internal node k, flag (k) to represent the flag component ofany node k, and length (k) to represent the length component of aandM;The encoder transmits the tree to the decoder in the form we have described.andO;Thus, the representation overhead associated with Method A1 is nC + (n - 1)A,and the lag time consists of the time necessary to receive and store thetree.andM;Method A2andM;The storage requirement of Method A1 can be improved in some cases because ofthe length values need not be stored in the decode data structure.andP;  The keyobservation that allows elimination of the string lengths is that the lengthof an entry can be found by subtracting its starting address from thestarting address of its preorder successor.andP;  The starting address of any leafnode's preorder successor can be found easily, trivially, in fact, if theleaf, x, is a left child of its parent.andP;  In this case, the preorder successorof x is its sibling, and the address of the sibling is stored in x's parentnode.andP;  In the other case, when x is a right child, we can walk from x to itspreorder successor as follows: we walk up &quot;1&quot; branches until we reach a nodethat is not a right child; at this point, we walk up a single &quot;0&quot; branch andthen down a &quot;1&quot; (right branch.andP;  In other words, the preorder successor of xis the right child of the lowest internal node from which we follow a &quot;0&quot;(left) branch to x.andP;  This characterization is also valid when x is a leftchild, since x's parent is the lowest internal node from which we follow aleft branch to x; and x's preorder successor is the right child of this(parent) node.andP;  The only node for which this characterization is not valid isthe final node in the preorder listing.andP;  This node lies on a path from theroot consisting of only right branches, and it has no preorder successor.andP;  Sothat we can decode this final node, we store the address of its (nonexistent)preorder successor in address 0 of the decode data structure, ahead of thepreorder representation of the decode tree.andP;  Thus we store n addresses inMethod A2 instead of the n - 1 addresses used in Method A1.andM;In the Method A1 data structure, address values are coupled with flag bits torepresent internal nodes, and length values are coupled with flag bits andcombined with character strings to represent leaf nodes.andP;  The coupling isaccomplished by using the leading bit of the address or length value forstoring the flag.andP;  In eliminating the length value from a leaf node, we arepresented with the problem of how to store the flag bit.andP;  The best solutionto this problem is to couple the flag bit with the leading character of thedictionary entry.andP;  In order for this to be possible, we must be able to storecharacters in b - 1 bits (where be is the number of bits per byte).andP;  Thisassumption may be reasonable on machines with 8-bit bytes where theapplication involves storing or transmitting text.andP;  The printable characterstypical of many text files can be represented in seven bits.andP;  Under thisassumption, the storage requirement of Method A2 becomes nA, as compared with(n - 1)A + nC for Method A1.andP;  Using the typical values given in Figure 1, wehave 2n bytes for Method A2, as compared with 3n - 2 bytes for method A1.andM;If the assumption of a spare bit in character storage is not valid,eliminating the lengths may not provide an improvement in storageutilization.andP;  Since high-level languages have the byte as the atomic unit ofaddressable memory, we are forced to store the flag in a byte when neitherthe length field nor the character field can accommodate it.andP;  If stringlengths can be stored in a single byte (C = 1) with a spare bit, we gainnothing by storing a one-byte flag instead of a one-byte (flag, length) pair.andO;In fact, the storage requirement for Method A2 would be nA + n bytes ascompared with (n - 1)A + n bytes for Method A1.andM;In a case where lengths require more than 1 byte of storage ([is greater thanor equal to] 2), however, the one-byte flag would be an improvement over theC-byte (flag, length) pair.andP;  I this case, Method A1 requires (n - 1)A + nCbytes of storage, and Method A2 requires only nA + n.andP;  In addition, the useof the (flag, lenght) coupling depends on the assumption that lengths can bestored in a way that provides a spare bit for the flag.andP;  If this assumptionis not valid, storing the flag alone will provide a space improvement overstoring the (flag, length) paid in C + 1 bytes.andP;  In summary, the eliminationof the length values from the Method A1 data structure is not guaranteed toprovide improved storage utilization, but does so under fairly generalconditions.andP;  In pact, Method A2 will be superior to Method A1 unlesscharacters require all b bits in a byte and string lengths require at most b- 1 bits.andP;  And in this case Method A2 can lose by at most A + 1 bytes!andM;In Figure 5 we give the Method A2 data structure for the example dictionaryof Figure 2 under the assumption that each character contains a spare bitthat can be used for the flag value.andP;  We assume that address fields alsocontain the spare bit and that A = 2.andP;  We represent internal nodes as (flag,address) pairs and leaf nodes as (flag, entry) pairs.andM;Using the Method A2 data structure to decode is very similar to using theMethod A1 structure.andP;  The only difference is that, in addition to the addressof the dictionary entry being decoded, we are also looking for the address ofits preorder successor.andP;  The following instructions are performed for eachcodeword.andP;  We use address(k) and flag(k) as in Method A1; p represents thecurrent candidate for the address of the preorder successor.andP;  We use thenotation contents(0) to retrieve the successor of the last node in thepreorder listing from memory location 0.andP;  Decode speed is very similar tothat of Method A1; the only extra time is due to storing an address in p foreach 0 bit.andM;p [left arrow] contents(0) k [left arrow] 2 repeat receive bit if bit = 0then p [left arrow] address(k) k [left arrow] k + A else k [left arrow]address(k) flag-value [left arrow] flag(k)andM;until flag-value = 1 append contents of memory locations k. .andP;  .p - 1 to thedecoded outputandM;The encoder transmits the tree to the decoder in the form we have described.andO;Thus, assuming a spare bit in character bytes, the representation overheadfor Method A2 in nA and the set-up time consists of the time necessary toreceive and store the tree.andP;  Both representation overhead and set-up time aresmaller for Method A2 than for Method A1.andP;  Figures 10 and 11 present spaceand time comparisons of our methods.andP;  The data for Method A1 presumes thespare bit in the address and length bytes, and for Method A2 the spare bit incharacter bytes is assumed.andM;METHOD BandM;The second method we discuss is based on the concept of a canonical Huffmancode defined by Schwartz and Kallick [7] and by Connell [3].andP;  We describethis concept first and then our implementation of it.andP;  The essence of thecanonical code concept is that Huffman's algorithm is needed only to computethe lengths of the codewords to be mapped to the dictionary entries.andP;  Oncelengths are determined, actual codewords may be specified in many ways; theonly necessary condition is that they satisfy the prefix property.andP;  This istrue for prefix codes in general.andP;  Intuitively, the canonical code may beviewed as one that builds the prefix code tree from left to right inincreasing order of depth (i.e., codeword length) with the convention thateach leaf is placed at the first position (from left to right) available toit.andP;  The example dictionary has codeword length sequence [2, 2, 3, 3, 3, 4,4].andP;  In constructing the canonical code, the first codeword of length two isplaced at the left edge of level two of the tree.andP;  Using the convention thatleft branches are labeled with 0 and right branches with 1, the firstcodeword is 00.andP;  The second codeword of length two is the sibling of thefirst, 01.andP;  The first codeword of length three is placed at the firstavailable position on level three of the tree.andP;  Level three is filled fromleft to right by placing codewords 100, 101, and 110.andP;  The length-fourcodewords, 1110 and 1111, complete the tree.andP;  The canonical code tree for theexample dictionary is given in Figure 6.andP;  The codeword for each dictionaryentry appears under the entry.andM;The canonical code possesses some nice mathematical properties.andP;  Thecodewords of a given length are consecutive binary numbers.andP;  The firstcodeword of length l, [c.sub.l], is related to the last codeword of length l- 1, [d.sub.l] - 1], by the equation [c.sub.l] = 2([d.sub.l - 1] + 1).andP;  Inother words, the first codeword of length l is obtained from the lastcodeword of length l - 1 by adding 1 to the binary number represented by[d.sub.l - 1] and shifting that binary number left once.andP;  In the case wheresome lengths are unused, as in [1, 3, 3, 3, 4], the codewords of length 3 areconsecutive binary numbers as are the codewords of length 4.andP;  The functionthat computes the first length 3 codeword from the length 1 codeword is2(2([d.sub.1] + 1)); that is, to move down two levels in the tree from level1 to level 3, two shifts are required.andP;  For the length sequence [1, 3, 3, 3,4, 4], the canonical code is {0, 100, 101, 110, 1110, 1111}.andP;  Every canonicalcode has a string of zeros as its first (shortest) codeword.andP;  We say that acanonical code has the numerical sequence property.andM;How does the numerical sequence property contribute to reducing memoryrequirements?andP;  First, the canonical code eliminates the need for the encoderto transmit to the decoder an explicit representation of the tree; the lengthsequence is sufficient to define the tree.andP;  We represent the length sequenceas a list consisting of (1) min, the length of the shortest codeword, (2)max, the length of the longest codeword, and (3) the number of codewords ofeach length.andP;  The first example we used, is thus, represented by 2, 4, 2, 3,2 and the second by 1, 4, 1, 0, 3, 2.andP;  In m ost cases this representation ismore compact than a list of the lengths of all of the codewords.andP;  If theencoder uses the length list to define the code, the size of therepresentation is 2B + LM where L = max - min + 1; M represents the number ofbytes required to store the maximum number of codewords of any given lengthand B the number of bytes required to store the length of a codeword.andP;  Wewill show that the data structure needed by the decoder can be constructedefficiently given the length list.andM;In addition to providing a compact representation of the code, the numericalsequence property may be used to index into the data dictionary.andP;  This isdone through the use of two small tables, limit and base.andP;  Each of thesetables is indexed from min to max.andP;  The limit table is used in decoding todetect the end of a codeword.andP;  The entry limit[i] contains the value of thelargest codeword of length at most i.andP;  The numerical sequence propertyguarantees that the numerical value of a codeword of length i is greater thanthe value of any shorter codeword.andP;  Thus, if the binary value of a string ofi bits is greater than limit[i] the string is not a codeword but a prefix ofa codeword.andP;  The decoder reads min bits from the coded text.andP;  If the binaryvalue of this bit string is less than or equal to limit[min] the bit stringrepresents a codeword.andP;  If the value of the first min bits is greater thanlimit[min] the decoder reads another bit, updates the value of the bitstring, and compares that value to limit 1[min + 1].andP;  This process continuesuntil the value of the bit string of length i is less than or equal tolimit[i] for some i.andP;  At this point we have recognized a codeword.andP;  Once theend of a codeword is detected the base table may be used to locate thecorresponding dictionary entry.andP;  The base table as defined in [3] maps acodeword value onto the relative position of the corresponding dictionaryentry in a list of dictionary entries.andM;The information provided by the limit and base tables is sufficient to allowdecoding if the entries of the data dictionary are all of the same length;however, for variable-lentgh entries we need the address of the appropriateentry, not an index.andP;  We present two solutions to this problem.andP;  We commentthat tables limit and base as defined by Connell [3] are redundant withrespect to one another.andP;  That is, the information contained in the base tablecan be extracted from the limit table entries.andP;  The base table, however, canbe represented in very little space and contributes substantially to theclarity of exposition of our methods.andP;  Eliminating the base table alsoresults in slower decoding; therefore we maintain the base table.andM;Method B1andM;Method B1 adapts Connell's base table method to allow for variable-lengthdictionary entries by introducing an address table indexed from 1 to n + 1.andO;The value of address 1[k] is the address of the first character of the[k.sup.th] dictionary entry.andP;  The entries are stored in a string table thatis organized in the following way: entries are stored in nondecreasing orderby codeword length and the block of entries with codeword length i stored inorder of decreasing codeword value.andP;  In terms of the prefix tree, we storethe dictionary in modified level order, that is, in increasing order by leveland in order from right to left on each level (of course we are storing onlythe leaves of the prefix tree).andP;  The base table provides pointers into theaddress table; that is, base[i] contains x such that address[x] is thestarting address of the block of dictionary entries with codeword length i.andO;When a codeword c of length i is recognized, limit[i] -- [value(c).sup.2]provides an offset in the list of codewords of length i.andP;  Thus, p = base[i] +limit[i] -- value(c) is the subscript in the address table at which thebeginning of the corresponding dictionary entry is stored.andM;The length of the entry is given by address[p + 1] -- address[p].andP;  Theaddress and length of the entry are all we need to append the entry to theoutput of the decoder.andP;  The storage requirement at decode time consists of LVfor the limit table (limit contains codeword values), LN for the base table(base contains subscripts from 1 to n + 1), and (n + 1)A for the addresstable.andP;  In most cases we expect LV + LN + (n + 1)A to be an improvement overthe nC + (n - 1)A requirement of Method A1.andP;  In practice L is generally O(lgn) while a typical value of L is 13.andP;  Therefore, Method A1 requires 3n - 2bytes and Method B1 2n + 54 in a typical application.andP;  The storagerequirement of Method B1 will always be greater than the 2n requirement ofMethod A2; thus, Method B1 provides no improvement in an application in whichcharacter bytes contain an unused bit.andP;  In terms of translation time, MethodB1 is expected to be a little bit but not significantly slower than the AMethods.andM;The encoder transmits the length list, the strings, and their lengths as apreface to the encoded text.andP;  Thus, the representation overhead is 2B + LM +nC.andP;  The representation is transmitted in the following form: first, min andmax; then for each codeword length i nfrom min to max), [n.sub.i] (length,str) pairs.andP;  Each [n.sub.i] represents the number of dictionary entries withcodeword length i and each (length, str) pair gives the number of charactersin a dictionary entry followed by the character string itself.andP;  The entrieswith codeword length i are listed in order of decreasing codeword value.andP;  Thedecoder performs the following calculations to set up the decode datastructure.andP;  In addition to the time required to receive the data, the decoderperforms [Theta](n) operations in setting up the address table and [Theta](L)operations in constructing tables limit and base.andM;s [left arrow] 1 a [left arrow] 1 receive min, max for i [left arrow] min tomax do receive [n.sub.i] if i min then base [min] [left arrow] 1 limit [min][left arrow] [n.sub.min] - 1 else base [i] [left arrow] base [i - 1' +[n.sub.i - 1] limit [i] [left arrow] 2(limit[i - 1' + 1) + [n.sub.i] - 1 forj [left arrow] 1 to [n.sub.i] do receive length, str store str in string [s.andO;.  .s + length - 1] address [a] [left arrow] s a [left arrow] a + 1 s [leftarrow] s + length endfor endfor address [a] [left arrow] sandM;Figure 7 gives the Method B1 data structure for the example dictionary.andP;  Theaddresses represent byte addresses of dictionary entries; we assume that thestarting address is 1 and that each character of an entry occupies 1 byte.andO;Figures 10 and 11 provide space and time comparisons of Method A1, Method A2,and Method B1.andM;Method B2andM;We now present a modification of Method B1 which can provide spaceutilization superior to that of Method A2.andP;  Method B2 is actually acollection of methods, parameterized by a variable k.andP;  The time-spacecompromise that best fits the requirements of a particular application can beselected by fixing an appropriate value of k.andP;  The improvement in Method B2over Method B1 is achieved by storing fewer than n address values.andP;  The valueof the parameter k determines what fraction of the n address values arestored.andP;  Method B2 uses the limit and base tables exactly as in Method B1.andO;The dictionary is represented by three tables.andP;  The first table, string,contains the dictionary entries stored as in Method B1 (i.e., in modifiedlevel order).andP;  The second table, address, is indexed from 1 to * n/k * andstores the address of every [k.sup.th] dictionary entry, with address[j]containing the address of entry jk.andP;  The third table, len, is indexed from 1to n - * n/k * and contains string lengths.andP;  Thus the space requirements ofMethod B2 are: LV + LN for the limit and base tables, * n/k * A for theaddress table, and (n - * n/k *)C for the len table.andM;The limit table is used to recognize codewords as in Method B1.andP;  The basetable again yields an index into the list of dictionary entries; if base[i] =x then the [x.sup.th] dictionary entry is the first entry (in modified levelorder) with codeword of length i.andP;  When a codeword c of length i has beendecoded, we use p = base[i] + limit[i] - value(c) - 1 to find thecorresponding dictionary entry.andP;  If p mod k = 0, the address of the firstcharacter of the entry is stored in address [p/k].andP;  If p mod k [is not equalto] (k - 1), the length of entry p is stored in len[p - * p/k * + 1].andP;  Thus,when p mod k = 0, both the address and the length of the correspondingdictionary entry are stored in the decode data structure.andP;  When p mod k [isnot equal to] 0, address[ * p/k * ] is a pointer to the block of k entrieswhich includes the one we seek.andP;  We walk along this block until we find theentry corresponding to c.andP;  This walk can be viewed as a sequence of jumpsthat use the len values to jump over entries.andP;  The number of jumps is givenby p mod k; the maximum number of jumps is k - 1.andP;  If p mod k [is not equalto] (k - 1), the length of the entry is stored in the len table; otherwise,the length of the entry is computed from the starting address of itssuccessor in the modified level order listing (i.e., address [ * p/k * + 1[).andO;The following calculations provide the starting address start and the lengthcorresponding to any index p.andM;p [left arrow] base [i] + limit [i] - value(c) - 1 q [left arrow] [ p/k] if q= 0 then start [left arrow] 1 else start [left arrow] address [q] r [leftarrow] p mod k t [left arrow] p - q for i [left arrow] 1 to r do start [leftarrow] start + len [t - i + 1] endfor if r [is not equal to] k - 1 thenlength [left arrow] [t + 1] else length [left arrow] [q + 1] - startandM;As in Method B1, the encoder transmits the length list, the strings, andtheir lengths.andP;  Thus the representation overhead is 2B + LM + nC.andP;  Tableslimit and base are built exactly as in Method B1.andP;  The following codeincludes the computations for tables len and address.andP;  The set-up time isagain [Theta](n) + [Theta] (L).andM;s [left arrow] 1 a [left arrow] 1 l [left arrow] 1 count [left arrow] 0receive min, max for i [left arrow] min to max do receive [n.sub.1] if i =min then base [min] [left arrow] 1 limit [min] [left arrow] [n.sub.min] - 1elsa base [i] [left arrow] [i - 1] + [n.sub.i - 1] limit [i] [left arrow]2(limit) [i - 1] + [n.sub.i] - 1 for j [left arrow] to [n.sub.i] do receivelength, str store str in string[s].andP;  .  .s + length - 1] if count mod k [isnot equal to] k - 1 then len[1] [left arrow] length l [left arrow] 1 + 1 ifcount [is not equal to] 0 and count mod k = 0 then address [a] [left arrow] sa [left arrow] a + 1 count [left arrow] count + 1 s [left arrow] s + lengthendfor endfor if count mod k = 0 then address[a] [left arrow] sandM;Figure 8 gives the Method B2 data structure for the example of Figure 2 withk = 2.andP;  A comparison with the other methods is provided in Figures 10 and 11.andO;We note that if k = 1 the storage requirement for Method B2 reduces to therequirement for Method B1.andM;We provide a second example for Method B2 in Figure 9.andP;  The data structurefor an example with a larger dictionary and k = 3 is given.andP;  The reader canuse the limit table values to verify that the codewords for {wxyz, the, qu,rst, abcd, ps, lm, out, rt} are {0, 110, 101, 100, 11110, 11101, 11100,111111, 111110}.andM;The parameter k determines the decode speed of Method B2 as well as itsstorage requirement.andP;  The maximum number of jumps determines the worst casetime for appending one dictionary entry to the output.andP;  The maximum number ofjumps is k - 1.andP;  It is important to recognize that the time-space tradeoffprovided by Method B2 is nonlinear.andP;  When k = 1, Method B2 stores naddresses; when k = n, Method B2 stores 1 address and n - 1 lengths.andO;Assuming A = 2 and C = 1, the choice k = 1 requires 2n bytes of storage, andthe choice k = n requires n + 1 bytes.andP;  When k = 2, the storage requirementis 1.5n bytes, essentially midway between the requirement for k = 1 and thatfor k = n.andP;  The choice of k = 2, however, may result in decode speed muchcloser to that provided by k = 1 than that provided by k = n.andP;  The extradecode time required by Method B2 (as compared to Method B1) is proportionalto the number of jumps.andP;  When k = n, only one address is stored.andP;  Thus, thefirst codeword (in modified level order) can be decoded with no jumps, thesecond requires 1 jump, and in general the [j.sup.th] requires j - 1 jumps;the maximum number of jumps required to decode a single codeword is n - 1.andO;Employing Method B2 with k = 2 reduces the maximum number of jumps to justone.andP;  IF we compare the use of k = n with the use of k = 1, we see that bydoubling the space requirement we eliminate the need to jump since everyaddress is stored; however, we can reduce the maximum number of jumps to oneat a cost of only 50 percent extra space.andP;  In general, a space increase of1/k of the k = n requirement (which stores only a single address and all nstring (lengths) imposes a ceiling of k = 1 on the number of jumps.andP;  Inpractice a k value of about 4 or 5 is reasonable.andM;We present a summary of the performance of our methods in Figures 10 and 11.andO;The typical values are those given in Figure 1 with the addition of k = 5.andO;In the second column of Figure 11, labeled, &quot;Receiving Time for CodeDescription,&quot; we give the number of bytes transmitted for the codedescription; clearly the time required to receive the data is proportional toits size.andP;  In column three of Figure 11, [c.sub.1] and [c.sub.2] representsmall constants.andP;  We note, that while the A Methods require no additionalset-up time, their code descriptions are almost guaranteed to be longer thanthose of the B Methods, so that the larger receiving time requirement offsetsthe savings in set-up time.andM;ADDITIONAL IMPLEMENTATIONandM;CONSIDERATIONSandM;Reducing Transmission TimeandM;We consider several issues associated with the representation of:andM;(1a) the stream of characters;andM;(1b) information needed to reconstruct the dictionary from the characterstream; andandM;(2) the prefix code.andM;We have focused on the way the representation is stored in the decoder andthe way it is used to decode the message.andP;  We now make some observations onthe way in which it is transmitted.andM;We have assumed that the characters of the dictionary are stored onecharacter per byte in our decode data structures.andP;  It is not necessary torespect byte boundaries in transmitting the stream of characters.andP;  The streamof characters may be represented in 7- or 8-bit ASCII; however, if thedictionary is very large, it may be significant more efficient to employ avariable-length coding technique.andP;  The canonical Huffman code can be used atvery low cost for encoding single characters; only tables limit and base andan array of characters in modified level order are required for decoding.andM;In Figures 10 and 11 we include nC bytes in the representation overhead forthe lengths of the dictionary entries.andP;  We observe, first, that it is notnecessary that an integer number of bytes be used to transmit a stringlength.andP;  In addition, if the lengths of the entries vary across a wide range,we can do much better than nC bytes by using a variable-length representationof the integers such as the Fibonacci codes described by Apostolico andFraenkel [1].andP;  If dictionary-entry lengths vary from [l.sub.1] to [l.sub.2],a fixed-length representation requires lg [l.sub.2] bits for each length.andO;The variable-length codes represent small lengths in fewer than lg [l.sub.2]bits, but large length values require more bits.andP;  The variable-length code isjustified, then, if dictionary entries are short on average.andP;  For Methods B1and B2, in which the prefix code is represented by a length list, samevariable-length coding can be applied to codeword lengths.andP;  Codeword lengthsare expected to be short; it is likely that most of them can be representedin less than 1 byte.andP;  The Fibonacci codes are simple to encode and decode inplace and are well-suited for representing integers.andM;Reducing Decode TimeandM;Another implementation detail worthy of mention is one that can reduce decodetime for Method B2.andP;  Just as the canonical Huffman code can be viewed as arefinement of standard Huffman coding (in that it selects a particular codetree among multiple optimal trees), we present a further refinement of thecanonical code, which we call the B2-optimal canonical code.andP;  We note, first,that while the canonical code specifies a code tree, it leaves open thequestion of how to assign the [n.sub.i] codewords of length i to the[n.sub.i] dictionary entries.andP;  We specify this assignment in a way that willminimize the average number of jumps (thus a B2-optimal canonical code is onethat minimizes decode time).andM;The B2-optimal code depends on the parameter k and on the interplay between kand the number of codewords of each length.andP;  Figure 8 shows that decoding anyof wxyz, qu, abcd, or lm requires no jumps and that decoding either the, rst,or ps requires one jump.andP;  The B2-optimal code reverses the positions of wxyzand the so that the entry with higher frequency can be decoded without jumps.andO;Two of the level-three entries can be decoded without jumps; these should bethe two with highest frequencies.andP;  Therefore, {qu} is placed at the middleposition of level three, and the positions of {abcd} and {rst} are arbitrary.andO;Since {ps} and {lm} have equal frequency their relative positions on levelfour are arbitrary.andP;  The B2-optimal tree typically reduces the average numberof jumps in decoding a source text by 25-30 percent.andP;  There are nodisadvantages to the use of the B2-optimal tree for decoding; the only costis the time it takes the encoder to construct the optimal tree rather than anarbitrary canonical tree, and this cost is small.andM;SUMMARYandM;Four methods of decoding prefix codes in limited space have been presented.andO;The methods are partitioned into two categories based on the data structuringstrategy employed.andP;  Method A2 is almost always superior to Method A1;however, the choice among Methods A2, B1, and B2 is less obvious.andP;  Parametersof a particular application will influence this decision.andP;  Tables comparingtime and space requirements of the four methods expose the relevantparameters.andP;  The methods we describe define only the decoding phase of a datacompression system.andP;  The choice of a fixed encoding dictionary is the mostcritical factor in determining the performance of a system based on ourmethods.andP;  While the representation of the code contributes to the compressionratio attained, most of the compression is achieved by selecting a dictionarythat is well-suited to the data being compressed.andP;  The size of the dictionary(i.e., number of strings, n) determines the exact time and space requirementsof each method.andP;  With an advantageous choice of dictionary, our methods canattain compression performance comparable to state-of-the-art techniques suchas the Unix utility compress.andP;  Defining a dictionary that guarantees goodcompression is, however, a difficult task.andP;  When the use of main memory is aconcern and sufficient preprocessing time to construct the dictionary isavailable, our methods provide a solution to the problem of decoding inlimited space without sacrificing compression performance.andP;  The methods aredescribed in sufficient detail to allow practitioners to implement themeasily.andM;(1) lg denotes the base 2 logarithmandM;(2) value(c) is the binary value of codeword candM;REFERENCESandM;[1] Apostolico, A., and Fraenkel, A. S. Robust transmission of unboundedstrings using Fibonacci representations.andP;  IEEE Trans.andP;  Inf.andP;  Theory 33, 2(Mar.andP;  1987), 238-245.andM;[2] Choueka, Y., et al.andP;  Huffman coding without bit-manipulation.andP;  Tech.andO;Rep.andP;  CS86-05, Weizmann Institute of Science, Dept.andP;  of Applied Mathematics,Rehovot, Israel, 1986.andM;[3] Connell, J. B. A. Huffman-Shannon-Fano code.andP;  Proc.andP;  IEEE 61, 7 (July1973), 1046-1047.andM;[4] Hankamer, M. A. modified Huffman procedure with reduced memoryrequirements.andP;  IEEE Trans.andP;  Commun.andP;  27, 6 (June 1979), 930-932.andM;[5] Huffman, D. A. A. method for the construction of minimum-redundancycodes.andP;  Proc.andP;  IRE 40, 9 (Sept.andP;  1952), 1098-1101.andM;[6] Lelewer, D. A., and Hirschberg, D. S. Data compression.andP;  ACM Comput.andO;Surv.andP;  19, 3 (Sept.andP;  1987), 261-296.andM;[7] Schwartz, E. S., and Kallick, B. Generating a canonical prefix encoding.andO;Commun.andP;  ACM 7, 3 (Mar.andP;  1964), 166-169.andM;[8] Sieminski, A. Fast decoding of the Huffman codes.andP;  Inf.andP;  Process.andP;  Lett.andO;26, 5 (May 1988), 237-241.andM;[9] Storer, J. A. Data Compression Methods and Theory.andP;  Computer SciencePress, Rockville, Md., 1988.andM;[10] Tanaka, H. Data structure of Huffman codes and its application toefficient encoding and decoding.andP;  IEEE Trans.andP;  Inf.andP;  Theory 33, 1 (Jan.andO;1987), 154-156.andM;[11] Witten, I. H., Neal, R. M., and Cleary, J. G. Arithmetic coding for datacompression.andP;  Commun.andP;  ACM 30, 6 (June 1987), 520-540.andM;DANIEL S. HIRSCHBERG is currently a professor and associate chair of GraduateStudies in the Department of Information and Computer Science at theUniversity of California at Irvine.andP;  Current research interests are in designand analysis of combinatorial algorithm and data structures.andM;DEBRA A. LELEWER is currently an assistant professor in the Computer ScienceDepartment at California Polytechnic University at Pomona.andP;  Current researchinterests include the design and analysis of algorithm and data structure.andO;Authors' Present Address: Dept.andP;  of Information and Computer Science,University of California at Irvine, Irvine, CA 92717.andO;</TEXT></DOC>