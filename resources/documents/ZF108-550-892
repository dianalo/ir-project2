<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-550-892  </DOCNO><DOCID>08 550 892.andM;</DOCID><JOURNAL>UNIX Review  June 1990 v8 n6 p58(5)* Full Text COPYRIGHT Miller Freeman Publications 1990.andM;</JOURNAL><TITLE>Distributed applications with vision. (designing distributedsystems)</TITLE><AUTHOR>Wayner, Peter.andM;</AUTHOR><SUMMARY>Many difficult implementation questions must be addressed beforeusers see any benefits from distributed computing.andP;  Where and howto store data is a major concern; mainframes are ideal forcentralized jobs, while local area networks offer greaterflexibility in decentralized applications.andP;  LANs are costly andcan become overloaded.andP;  Security is a serious concern as well.andO;The Xerox Palo Alto Research Center is working on a load-balancingsystem for grafting distributed control onto the UNIX kernel.andO;Cornell University's Isis Project simplifies the implementation ofdistributed algorithms.andP;  Networked environments are notoriouslyinsecure, and multiple layers of password protection substantiallyincrease operating overhead.andP;  MIT's Kerberos Project approachestablishes use identities with DES encryption at first log-on andmaintains a list of users and secret 'keys' on a centralizedauthentication servers.andP;  Centralization and consistency are keyproblems in distributed file systems.andP;  A central database iseasier to maintain than a distributed one but suffers frombottlenecks; distributing files over multiple servers requireseach workstation to maintain more data.andP;  The University ofWisconsin's Condor load-balancing system maintains queues ofnecessary jobs.andP;  Cornell is developing Meta, an Isis-based systemfor addressing real-time industrial control problems.andM;</SUMMARY><DESCRIPT>Topic:     Hospital Information SystemsFuture of ComputingDistributed ProcessingUNIXSoftware MigrationSystem SelectionSystem Conversion.andM;</DESCRIPT><TEXT>DISTRIBUTED APPLICATIONS WITH VISIONandM;Imagine you've just walked into a hospital of the future.andP;  Everything fromthe heart monitors in the Intensive Care Unit (ICU) to the sterile robotsoperating the vacuum cleaners in the hallways is computerized.andP;  Across thelobby, a doctor approaches a computer terminal and opens a file on a patientwho is resting comfortably in bed on the fourth floor.andP;  The physicianreceives data from the patient's medical history, as well as current readingsfrom bedside monitoring systems upstairs.andM;Determining that the patient's medication needs to be changed, the doctorhits a few keys, and new prescriptions appear on the attending nurse'sterminal and in the patient's file.andP;  Meanwhile, the accounting systemsilently tracks this activity and issues the proper bills.andP;  While thisdoctor-comptuer interaction takes place, researchers downstairs set up daemonprocesses that check the hospital roll for patients with a particular classof symptoms and then evaluate the effects of the treatments beingadministered.andP;  Were you to walk up to the reception area and see a diagram ofthe building's computing resources, you'd note that all the activitydescribed above transpired in an environment with machines that range fromdiagnostic equipment with embedded controllers to large compute servers.andM;The tremendous advantages of this distributed system should be obvious, butbefore we can reap the benefits of such a system, a host of difficultquestions regarding its implementation will have to be addressed.andP;  Onenotable problem is that each machine in the hospital--from the CT scanner tothe digital thermometer--might have data formats and needs completelydifferent from the other systems.andP;  The network must be flexible enough tochange and expand without interruption.andP;  How should such a networked systembe constructed?andM;One of the first problems to be solved is where and how data could be storedon such a system.andP;  Several large mainframes might be efficient for runningcentralized jobs, but they could quickly become bottlenecks in a system thathad to perform multiple local operations.andM;One other important question to be confronted is which protocol would be mostadvantageous to such a network.andP;  Ethernets are easily expanded, but they canquickly become overloaded.andP;  Token rings are expensive and fragile.andP;  Ourfuturistic system would have to be monitored and tuned for performance, andprecautions would need to be taken to prevent--or at least localize--systemfailure.andP;  The system administrator would need to ensure that such a criticalsystem was not burdened to the breaking point.andM;The system must also be secure.andP;  Patients don't want their medical historiesavailable to everyone, but if an overly secure system prevented a physicianfrom gaining access to a patient's file, that patient might die.andP;  How couldthe configuration prevent drug thieves from &quot;breaking in&quot; and generating fakeprescriptions?andP;  Computer viruses running rampant on a hospital computer wouldcertainly provide fodder for local newspapers, but to say that patients'health would be jeopardized would be to mildly understate the case.andP;  And as adistributed system grows, security problems become substantially moredifficult.andM;The crucial question for anyone building a distributed system is not how sucha system can be constructed, but what kind of system is to be built.andO;Achieving fault tolerance, providing for network expansion, supportingprocess migration, ensuring security, maintaining functional distributedfilesystems, and monitoring load balance are not small challenges, certainly,and each must eventually be considered.andP;  These concerns, however, are notnearly as important initially as is the establishment of a clear concept forthe final working system.andM;Computer networks are becoming ubiquitous; before computing standards are setand programming code grows too old to change, we have to consider that wewant our networks to be able to do.andP;  Once we've formed a solid picture ofwhat the completely flexible, accessible system will look like, then--andonly then--can we begin casting this mixture of whim, desire, and necessityinto UNIX code.andM;Wish Lists.andP;  Each of these problems is being explored through a number ofdifferent projects currently under way in the computing research community.andO;One real example of a distributed control system grafted on top of the UNIXkernel is a load-balancing system called Spawn, built at the Xerox Palo AltoResearch Center (PARC) by Carl Waldspurger, Tad Hogg, Bernardo Huberman,Jeffrey Kephard, and Scott Stornetta.andP;  Spawn allocates cycles for processesrunning on a distributed network of machines by conducting &quot;auctions&quot; usingimaginary money.andP;  If a machine on the network has a light load, it informsthe network it is selling time to th e highest bidder.andP;  High-priorityprocesses receive more &quot;money&quot; from the system administrator to bid for timethan do low-priority processes, and thus win more auctions and more time.andM;The Spawn researchers chose to implement the system in ordinary user-modeUNIX because UNIX runs successfully on a number of different brands ofcomputers.andP;  They used Sun Microsystems' Remote Procedure Call (RPC)initially, but it didn't offer them support sufficient to their particularpurposes: they were intent on watching how processes moved about on thenetwork when controlled by a free-market-like load-balancing system.andO;Unfortunately, UNIX processes called by RPCs took too long to start up.andO;Tests of the system were limited to easily parallelized programs that couldbe split into several parts, each of which required little interprocesscommunication.andP;  Moreover, the remote-procedure-call mechanism was notadequate for process migration, and the developers were forced to build theirown router.andM;Means of Abstraction.andP;  The problems the Spawn team encountered are commonones for developers trying to implement distributed systems.andP;  They found thatthe remote procedure call didn't handle enough of the chores, and they wereforced to do the extra work themselves.andP;  The most common abstraction forsharing work between two machines is RPC because it is a straight-forward andnatural extension of traditional programming.andP;  The gritty details of passingdata between machines and maintaining connections are handled transparently,and a program that issues RPCs runs much the same as it would on a singleprocessor.andP;  The RPC protocol encourages the programmer to think of adistributed system comprising many different machines as a single, dedicatedmachine.andP;  This approach makes it easier to port old, uniprocessor software todistributed systems, because it requires relatively little rethinking orrewriting of old algorithms.andM;Rethinking can be very beneficial, however, because distributed systems rundifferently from uniprocessor machines.andP;  The communication time betweenprocessors can add substantial overhead.andP;  Moreover, traditional algorithmsrarely adopt a metaphor of cooperation to divide work that needs to be done.andM;That Isis Project at Cornell University, run by Ken Birman, was establishedto provide a toolkit that makes implementing these distributed algorithmseasier.andP;  A group of concurrent tasks is the basic structure of the system.andO;Once the programmer hooks several processes together into such a group, theIsis system provides high-level tools for managing replicated data, splittingcomputation among machines, and synchronizing and co-ordinating activities.andM;When processes running on a distributed system with Isis start or stop, Isisprovides tools that enable the software to reconfigure the systemdynamically.andP;  Machines that recover after crashes are brought up to date onthe state of other processes.andP;  Isis keeps the machines in a given processgroup virtually synchronous.andM;The programmer who used Isis can begin to think of a program as manydifferent process working together to accomplish a task, without worryingabout the details of communication and concurrency.andP;  A central threat ofcontrol that parcels out work isn't necessary, but the protocol can easilysupport one.andP;  The result is a system that is less centralized and moreresistant to failure.andM;Security in a Distributed System.andP;  Linking computers together in adistributed network has many advantages, but security is not one of them.andP;  Anetworked environment not only multiplies opportunities for snooping, butalso increases the chances for large-scale viral terrorism.andP;  Topology is atthe heart of many of the security problems of distributed networks.andP;  Mostnetworks are not fully connected because of the high cost of running wiresbetween pairs of machines; individual computers and fileservers musttherefore be able to respond as if the messages they receive are actuallyfrom the entities the sources claim to be.andP;  If a packet says, &quot;I'm root, sendme may files&quot;, determining whether this is the truth or a lie is impossiblefor conventional systems.andM;Many standard protocols--such as Sun's NFS--do not defend against spoofingand infiltration because the benefits are usually not worth the extracomputing load that security requirements impose.andP;  For instance, if readingeach block of a file requires a password, the system load would double.andO;Moreover, an intruder could listen to the packet coming by and pick up thepassword.andP;  Interactive password systems based on Zero-Knowledge Proof systemscan establish identity without revealing the data to a casual observer--butthese require more than several iterations of message exchanges.andP;  If theblocks of data being examined are small, bouncing these interactive proofpackets between the machines would slow the system considerably.andM;Researchers laboring on the Kerberos Project at MIT have developed oneapproach for UNIX systems that effectively solves many security problemswithout significantly retarding system performance.andP;  When a user first logsin, his or her identity is authenticated by a DES-based encryption system.andP;  Acentralized authentication server maintains a list of users and their secretkeys.andP;  When a user wants to access a fileserver, the local computer firstqueries the authentication server for a temporary key, which will be used toencrypt all the traffic between the fileserver and local computer.andP;  Theauthentication server sends this back encrypted in the local computer'ssecret key.andP;  Only the local computer can decrypt this message.andP;  The temporarykey is used to encrypt all data moving between the fileserver and the user.andO;The key has only a short life span; when it expires the workstation mustrerun the authentication protocol.andP;  This is done often enough to discourageeavesdroppers, but not often enough to slow down the system.andM;Distributed Filesystems.andP;  Files can be maintained on a distributed system inmany different forms.andP;  Many workstation networks are built around multiplefileservers.andP;  Operating systems such as SunOS, which uses NFS, can knittogether the various directories on a group of machines so that they appearto users as one large filesystem.andP;  More sophisticated systems would be ableto generate multiple replicas of filesystems on different servers, so thatfiles could move dynamically through the system to speed local computationand prevent bottlenecks.andM;TWo fundamental problems of file management are centralization andconsistency.andP;  Central databases are easier to maintain because only one copyof each file exists; but central databases can become bottlenecks when manydata-intensive processes rely on the same central server.andP;  Distributing filesover several servers reduces bottlenecks, but requires that each workstationmaintain enough information to find all the files.andP;  In addition, if adistributed filesystem made multiple copies to increase either performance orfault tolerance, the entire networked system would have to ensure that allcopies were consistent.andM;The Andrew Project at Carnegie Mellon University produced a filesystem thatuses distribution to increase performance.andP;  The structure of the central fileservice is very simple.andP;  Each workstation maintains hierarchical directoryinformation locally.andP;  This lowers network traffic and information demandbecause the server need deliver only real file data.andP;  Clients in the Andrewsystem relentlessly cache most active data.andM;The Deceit distributed filesystem, being developed at Cornell by Alex Siegel,is more sophisticated.andP;  Files in the system are dynamically relocated toplace them closer to the user and to lighten the load on the network.andP;  Thesystem relies on the standard NFS protocol for client-server communications.andO;Servers communicate with one another using the Isis system described above.andO;Files may be replicated to improve availability in the event of a crash orslow read performance.andM;Load Balancing.andP;  Redistributing the computing workload dynamically across anetwork of computers is one of the most seductive goals of distributedcomputing.andP;  Most of the time, CPU cycles are probably wasted by screen saversor text editors, since users do not constantly push their computers to peakperformance.andP;  If the load could be redistributed seamlessly, everyone'sperformance would be enhanced.andM;The Spawn system described earlier distributes the compute load by auctioningmachine time.andP;  The metaphor of free-market economics for balancing demandsfor computer time is not only historically resonant (mainframes &quot;charged&quot; fortime many years ago), but also intuitively satisfying in times when centrallycontrolled economies in the world at large are struggling in disarray.andP;  Theprocess of holding auctions is not much different, in practice, from using ajob-priority scale to allocate resources.andM;Condor, a load-balancing system developed at the University of Wisconsin,maintains a queue of necessary tasks which machines on the network consult toget their assignments.andP;  If a networked machine had to execute a large Linpackjob, it would send the job to the matrix-algebra queue, where the job wouldwait for the next free systolic array.andP;  The system maintains several copiesof each queue on various machines, to preserve fault tolerance and preventbottlenecks.andM;Dynamical Systems.andP;  One of the most intuitive applications for distributedsytems is solving real-time, real-world control problems such as thosepresented by factory automation.andP;  The distributed system controlling afactory-automation environment must maintain it such that an isolated failurewill not bring down the whole operation.andP;  Further, the system must be able toreceover from the failure automatically.andM;The Meta system being built at Cornell by Keith Marzullo and Mark Woods isdesigned to address real-time industrial-control problems.andP;  The system runson Isis tools, and relies on them to provide protocols for efficient,fault-tolerant interaction.andP;  At the center of the system, an enginecontinuously compares data sent by some number of peripheral sensors to aninternal set of rules.andP;  When an event occurs that matches one of these rules,Meta triggers the appropriate actions.andP;  The entire system is designed totolerate the failure of several nodes and recover when the machines arebrough on-line again.andM;Creating a fault-tolerant distributed system that is not only secure, butalso easily extended and flexible enough to change with time, is not easy.andO;What makes this most difficult is finding a flexible structre that knitsthese solutions together.andP;  Users are not concerned about transparency orlow-level monitors; they want a network that makes the most of theircomputers.andP;  The goal is to balance users' needs with the technology.andM;Returning to our hospital illustration, consider the practical concerns usersof such advanced technology might have.andP;  If a doctor were in a different wingof the hospital from where the patient was resting, the doctor might stillwant to read the patient's files.andP;  The system administrator would need todecide whether a central set of closely linked fileservers would meet thisneed better than a loosely joined network with migrating files.andP;  Meanwhile,the issues of security are still important.andP;  How should doctors verify theiridentities?andP;  How much encryption is enough?andP;  If the machines coordinating thebedside monitors in the ICU suddenly suffered a crash, how would the networkrespond?andM;If you are building a network, consider the possibilities, and think of everydesirable feature.andP;  According to  Mark Weiser of Xerox PARC, we are enteringan age of &quot;ubiquitous computing&quot;.andP;  The walls of our buildings will soon befilled with wires carrying information everywhere.andP;  With the proper overalldesign, systems of the future will be fault-resistant and fully functional.andO;Our dreams need to inform us of all the possibilities we could seematerialize, so the system can be flexible enogh to include them easily andseamlessly.andM;Peter Wayner is a graduate student at Cornell University's department ofcomputer science, and a consulting editor for Byte magazine.andP;  He is currentlyworking on a thesis documenting the development of a three-dimensional,model-based recognition system.andP;  Wayner has also worked for IBM at YorktownHeights and for Xerox at its Palo Alto Research Center.andO;</TEXT></DOC>