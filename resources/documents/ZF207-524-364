<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-524-364  </DOCNO><DOCID>07 524 364.andM;</DOCID><JOURNAL>Dr. Dobb's Journal of Software Tools  Sept 1989 v14 n9 p114(5)* Full Text COPYRIGHT Mandamp;T Publishing Inc. 1989.andM;</JOURNAL><TITLE>Nasal nets. (column)</TITLE><AUTHOR>Swaine, Michael.andM;</AUTHOR><SUMMARY>Neural networks are growing in importance, but suffer from speedproblems because the convergence of the learning algorithm isslow.andP;  Information clarifying the statements of researchers TomWaite and Hal Hardenbergh is presented.andP;  Multi-level,perceptron-based networks converge to solutions faster thancompeting systems.andP;  The performance of a neural network increasesdramatically as it learns, often speeding up by a factor of onemillion between training and practice.andP;  Efficient circle-drawingalgorithms are a current area of interest in neural-networkresearch because integrating neural nets with move conventionalexisting algorithms is important.andP;  Marvin Minsky's arguments aboutthe limitations of perceptron networks undermined funding supportfor neural network paradigms in the late 1960s, but neural netshave made a dramatic comeback in recent years.andM;</SUMMARY><DESCRIPT>Topic:     Neural NetworksArtificial IntelligenceApplications ProgrammingComputer EducationAlgorithms.andM;</DESCRIPT><TEXT>Nasal NetsandM;In a short story by Nikolay Gogol, an officer misplaces his nose, which issubsequently spotted here and there about Petersburg, masquerading as anofficer of higher rank.andP;  While the officer is trying to place anadvertisement offering a reward for his nose's return, the press officialssolicitously but injudiciously offers him snuff.andP;  Insulted, the officerretorts that he is missing precisely what is required to appreciate snuff,and besides he would never use that cheap brand.andM;The point is ...andP;  but first I think I need to backtrack a bit.andM;Slow LearnerandM;Hal Hardenbergh called to clarify a point in the interview with him that waspublished in the June issue.andP;  In that interview, I left the speed issueambiguous: It's not the execution speed of trained neural nets, but theconvergence of the learning algorithm, that is slow.andP;  One other correspondentknowlegeable in neural net techniques also caught the slip.andM;In Hardenbergh's words, &quot;While it's true that multi-level, perceptron-basedartificial neural networks are extremely slow, this is only true for thetraining phase.andP;  When you actually run them, they are much faster.andP;  This iseven true for toy problems.andP;  I have been playing with neural nets on my threeAtari Sts and have achieved speedups from training to practice of one millionto one.&quot;andM;Tom Waite, who is working on neutral nets with Hardenbergh at Vicom in SanJose, Calif., pointed out that the performance of the system as it learns isdramatic.andP;  Unlike adaptive filters, which just get better as they learn,neutral nets do a flip-flop to which it is hard not to attach anthropomorphiclabels.andP;  You see the system spontaneously &quot;gets the idea,&quot; and develops&quot;insight&quot; into the problem.andP;  &quot;It will be stuck in a rut and all of a suddengo wild and drop into the right answer,&quot; Waite said.andM;But that insight may take a while.andM;Slow or not, multi-level, perceptron-based artificial neural networksconverge to solutions more rapidly than some competitors.andP;  Hardenbergh said,&quot;the Boltzmann machine training method is generally regarded as more robustthan multi-level perceptrons, but it is also considered to be two orders ofmagnitude slower than back propagation, so it's not used for practical work.&quot;andO;Anderson and Rosenfeld, in their massive and important book Neuron-computing,say &quot;[Backprop] is ...andP;  the most popular learning algorithm for working withmulti-layer networks.andP;  It is considerably faster than the other learning,algorithm that is sucessful with multi-layer networks, the Boltzmannmachine.&quot;andM;So, if you response to the interview was that you were missing precisely whatwas required to appreciate the stuff (algorithms, code, hands-on examples),and besides you would never use such a slow technique, I hope I havecorrected the impression I gave about the speed of neural nets.andP;  As for thealgorithms, code, and hands-on examples, that will have to wait until I candevote a full column to it, because a backprop algorithm doesn't make anysense without a fairly complete description of the network to which it isapplied.andP;  At the end of this column, though, are references to a book and anarticle where backprop is well exemplified.andM;The Explosive Market for Neural NetsandM;Hardenbergh also brought up to date a reference in that article to a possibledevelopment in the commercial neural nets market.andP;  &quot;At the time of thepublication of your interview with me there were no commercial applicationsof multi-level artificial neural nets.andP;  Since the SAIC has signed amultimillion dollar contract to provide explosives detectors to airports.andM;&quot;The way their device works is that the explosives are subjected tobombardment from a gamma ray source and the molecules making up the explosivesubstance have characteristic signatures under gamma rays.andP;  This results in adiffraction pattern that identifies the explosive.andP;  And SAIC uses amulti-level, perceptron-based artificial neural network to detect thecharacteristic patterns.andM;&quot;I call a one-hundred-million-dollar contract real activity.&quot;andP;  So do we, Hal.andM;Closing the CircleandM;The management at Vicom is supportive of Waite and Hardenbergh's neural netexplorations because Vicom is in the image processing business, and theapplications of neural nets to image processing are evident.andP;  But there aremany powerful techniques already developed for processing images, dating backat least to Fourier's development, in 1807, of the Fourier transform, whichhas had applications Fourier could not have imagined, such as transformingmedical diagnosis by way of the CAT scan.andM;Waite and Hardenbergh see neural nets as needing to fit in with existing,more conventional algorithms, so they find themselves regularly thinking indifferent paradigms.andP;  Lately, Hardenbergh has been treading the well-trodpath of efficient algorithms for circle drawing.andM;&quot;The past three weekends I've been playing with plotting curves.andP;  Back when Iwas [working on another project] I plotted curves and pulled out threecurve-plotting algorithms, all in fact from DDJ.andP;  When I looked at themrecently, I found to my surprise that all of them used multiplication foreach point.andP;  Then I came across Bresenhamhs algorithm.&quot;andM;In fact, DDJ published an implementation and discussion of Bresenham'salgorithm in September, 1987.andP;  It was by Jim Blinn, a graphics expert at JPL.andO;I used Blinn's version of Bresenham's algorithm in my book on HyperTalk, toshow beginning HypeTalk programmers three ways to draw a circle, but ofcourse drawing circles in an interpreted language doesn't tell you a wholelot about algorithmic efficiency.andP;  I was interested in Hardenbergh's take onBresenham's algorithm, because 1.andP;  Hardenbergh is interested in algorithmicefficiency and usually has an interesting timing result or two to report, and2.andP;  I knew how he hated wading through academic explanations of technicalissues.andP;  I suspected that, if he hadn't seen the Blinn piece, he had onlyseen academic explications, and would have impatiently taken off on his own,possibly interesting tangent.andM;Sure enough, he told me, &quot;The Bresenham algorithm doesn't use multiplication,but Bresenham is an academician and so the article I read was full ofderivations and math.andP;  Normally, people doing this kind of work keep it as atrade secret, but Bresenham is an academic, trying to make PhD Browniepoints, so he just made it obscure.andM;&quot;So I put away the book and set about trying to write a good circle-drawingalgorithm myself.andP;  Then I found out that Vicom does not know how to do fastellipses, so I worked on ellipses.andP;  Now I have a circle algrorithm, analgorithm that does ellipses oriented to the taxes, and also a tilted ellipsealgorithm, all without multiplies.&quot;andM;At that point Hardenbergh went back to the books.andP;  &quot;I picked up the book inwhich I had read the Bresenham algorithm and found to my surprise that I hadnot reinvented Bresenham; my algorithm is just not Bresenham's.andP;  Mine issimpler.andP;  It is ridiculously simple.andP;  And the ellipse and tilted ellipsealgorithms are just extensions of the circle algorithm, so I suspect that myellipse algorithms are also different from Bresenham's.andP;  I tried to checkthis but could find no book there that contained the Bresenham algorithm [forellipses].&quot;andM;Hal didn't give me his algorithms, but he did give me a number to testagainst: one thousand radius-100 circles drawn in 5.84 seconds with a 0.02second loop overhead.andP;  The hardware was an Atari ST.andM;The 48th Parallel ProcessorandM;In the Bavarian countryside outside Munich, I met with Jurgen Fey, almostexactly on the 48th parallel, roughly the same latitude as Poltava, where theyoung student Gogol was mercilessly teased for his beak-like nose a centuryand a half earlier.andM;Jurgen turned up his nose when I told him that Hardenbergh had criticized theTransputer for its lack of registers (Jurgen has developed a Transputerboard).andP;  &quot;It's true that it's not a register machine.andP;  You can't do parallelprocessing on a parallel machine.andP;  I challenge him to find a register-basedmachine that supports parallel processing.&quot;andM;But Jurgen did agree that the Transputer's &quot;native&quot; language, occam, was onits way out.andP;  There are now, he explained, several more or less viablealternatives in the works, including &quot;.Lips, SC-Prolog, Parallel Prolog,Modula-2, C, C, C, C, C, and C, Ada, assembler, and Forth.&quot;andP;  Jurgen isworking on something compatible with Turbo Pascal 4.0.andM;Antisocial Behavior in theandM;Society of MindandM;What has been called &quot;the Minsky smoke bomb&quot; continues to smell.andM;In The Structure of Scientific Revolutions, Thomas Kuhn wrote: &quot;But paradigmdebates are not really about relative problem-solving ability, though forgood reasons they are usually couched in those terms.andP;  Instead, the issue iswhich paradigm should in the future guide research on problems many of whichneither competitor can yet claim to resolve completely .andP;  .  .  that decisionmust be based less on past achievement than on future promise.&quot;andM;And Kuhn wrote:&quot; .andP;  .  .  the defenders of traditional theory and procedurecan almost always point to problems that its new rival has not solved butthat for their view are no problems at all .andP;  .  .  if a new candidate forparadigm had to be judged from the start by hard-headed people who examinedonly relative problem-solving ability, the sciences would experience very fewmajor revolutions.&quot;andM;And: &quot;.andP;  .  .  The member of a mature scientific community is, like thetypical character of Orwell's 1984, the victim of a history rewritten by thepowers that be.&quot;andM;In Perceptrons, Marvin Minsky and Seymour Paprt presented an impeccablyreasoned argument showing certain limits on the results you could get out ofany single-layer perceptron network, along with something quite different: An&quot;intuitive judgment that the extension $(to multi-layer networks$) issterile.&quot;andM;Together, the reasoned argument and the unsupported (and mistaken) intuitivejudgment undermined support for this neural net paradigm that was contendingfor AI research funding with their favored Lisp-based work at MIT.andP;  In 1969,Minsky and Papert were the hard-headed establishment of artificialintelligence and neural nets was the new candidate paradigm that got writtenout of the history of AI work by their efforts.andP;  Two decades later, thedramatic comebacks of neural nets is one of those heartening success stories,like &quot;Nikolay Gogol, the odd, beak-nosed boy, had turned obscurity andfailure into triumph, and was well on his way to becoming one of the mostfamous authors in Russia&quot; (Dick Penner).andP;  Heartening once you get to thesuccess part, but discouraging through the years of obscruity and failure.andO;Were those years unnecessary?andM;Some interesting public documents recently came into my possession, andanyone interested in the history of invention in this area ought to take alook at them.andM;The problem that formed the crux of Minsky and Papert's documentation ofperceptron-based neural net research was linearly separable functions;basically, the exclusive-OR problem of classifying unlike objects into thesame category.andP;  For example, given a collection of red and green squares andcircles, you are to classify the red squares and green circles together inone group, and the red circles and green squares in the other group.andP;  Youcan't form the grouping with a straight line through the space whosedimensions are red-green and circle-square.andP;  But it's a perfectly logicalgrouping, expressed by the exclusive-OR expression &quot;red or circle (but notboth).&quot;andP;  The first group contains all objects satisfying the expression, thesecond group all objects not satisfying it.andP;  The expression perfectly assignsthe objects to the required groups.andP;  Single-layer perceptrons, and thusneural nets, Minsky and Papert said, couldn't do that.andM;Although Minsky and Papert clarified the full consequences of the problemwith single-layer perceptrons in a way that had never been done, this basicproblem had been perceived years earlier, and many people had given thoughtto its solution.andM;At the very least, the problem was under attack.andP;  Here is what one researcherwrote in 1963: &quot;Two layers were used .andP;  .  .  because one layer can only betrained successfully on linearly separable functions $(Minsky and Papert'spoint$).andP;  .  .  .  Having two layers of variable weights was our initialintention, but we do not yet know of an algorithm leading to a convergenttraining process.&quot;andP;  The writer went on to discuss his group's search for sucha convergent algorithm, as well as practical measures they had taken.andM;How close were neural net researchers to solving the problem?andP;  Had theproblem been solved, perhaps accidentally, nearly a decade before Minsky andPapert published their book?andP;  And was Minsky aware of the solution?andP;  I don'thave the answers, but the issue is apparently more complex than anyone hasyet admitted.andP;  I refer you to the work on graphical data processing done atSRI in the early 1960s (references are at the end of this article).andM;Recommended ReadingandM;Hl Hardenbergh has been buying all the books he can find on neural nets.andP;  Hecalled me to recommend strongly Neural Computing: Theory and Practice, byPhilip D. Wasserman, Van Nostrand Rinehart, 1989.andM;&quot;I now know enough about back propagation to judge that the chapter on backpropagation is good, so by extension I suspect that the rest is good.andP;  I'mparticularly interested in the chapter on stochastic methods.andP;  I know someonewho knows the author and who says he's more interested in the Cauchy machinethan in back propagation.andP;  He says the training is better with simulatedannealing.andP;  The Cauchy machine is another stochastic method based on theBoltzmann machine, so I'm currently studying the stochastic chapter inWasserman's book.&quot;andM;Hardenbergh considers the Wasserman book a good place to start learning aboutneural nets: The right level and the right size.andP;  &quot;It's not a large book.andO;The problem is that when you are starting to get into a new field you need tocover a lot of ground so you need a big book but you're not really ready tohandle a big book.&quot;andM;A much more elementary but very broad presentation of this area can be foundin Cognizers: Neural Networks and Machines that Think, by R. Colin Johnsonand Chappell Brown, John Wiley, 1988.andP;  Light reading.andM;Hardenbergh and Waite have published their own version of the backpropagation algorithm in the June issue of Programmer's Journal.andM;The SRI work mentioned had to do with the development of several pieces ofparallel processing hardware (including MINOS I and MINOS II) and boththeoretical and practical work on multi-level neural net algorithms.andP;  Thework is described in several reports under SRI project number 3192.andO;Quarterly Reports 4 and 5 are particularly interesting, showing MINOS I doingexclusive-OR classification.andP;  The quote is from the final report, Report No.andO;12, by A.E.andP;  Brain.andM;Many Russian writers have written stories about noses.andP;  Gogol's The Nose,however, may be the only one in which the curious Russian fascination withnoses has survived translation.andP;  It was published in The Diary of a Madmanand Other Stories, Nikolay Gogol, translated by Andrew R. MacAndrew, NewAmerican Library 1960; and was reprinted in Fiction of the Absurd: Pratfallsin the Void, edited by Dick Penner, New American Librry, 1980.andP;  The latterbook contains several other works that offer insights into the softwaredevelopment process, including a selection from Catch-22 and Camus' essay onsoftware engineering, The Myth of Sisyphus.andO;</TEXT></DOC>