<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-232-165  </DOCNO><DOCID>08 232 165.andM;</DOCID><JOURNAL>Systems Integration  Dec 1989 v22 n12 p31(2)* Full Text COPYRIGHT Cahners Publishing Co. 1989.andM;</JOURNAL><TITLE>New twists in error recovery. (DBMS/Report) (column)</TITLE><AUTHOR>Carnahan, Ron.andM;</AUTHOR><SUMMARY>Unify Corp utilizes four background processes for its data basemanagement system's (DBMS) error-recovery strategy.andP;  Companyofficials refer to the four as the cache daemon, the file-managerdaemon, the clean-up daemon and a log-archive daemon.andP;  Unify'sapproach is different from traditional approaches because itallows the data base administrator to create a backup of the database even while it is in use.andP;  Oracle Corp's DBMS 6.0 approachesthe error-recovery process on the preparation side of the problem.andO;It includes support for fast commits and group commits.andP;  Thefast-commit facility allows only the changes made to the data baseto be written to the log.andP;  The group-commit facility writes redo,undo and commit information for multiple transactions in a singlewrite to the log.andM;</SUMMARY><DESCRIPT>Company:   Unify Software Corp. (products)Oracle Corp. (products).andO;Ticker:    ORCL.andO;Product:   Oracle 6.0 (Data base management system) (product specifications).andO;Topic:     Data base management systemsError RecoverySystems SoftwarePerformance ImprovementSoftware Architecture.andM;</DESCRIPT><TEXT>New twists in error recoveryandM;Today's DBMS products sport some interesting standard features that reducethe trauma of planing against, and recovering from, system errors.andP;  Untilrecently, error-recovery strategies involved powerful disincentives likesacrificing producing performance to insure recoverability, losing systemsupport when correcting operating errors and, in some cases, stopping allprocessing at backup time.andM;Traditionally, commercial database products employ some form of a commonstrategy for error recovery: Database backups are made regularly, and atransaction log records changes made since the last backup.andP;  In the event offailure, the system administrator takes the borken system down, fixes theproblem, restores a backup and reapplies any transactions that were on thetransaction log but not on the old backup.andM;But writing each transaction to both the database and a log consumesresources in what are typically the slowest components of a system--the diskdrives, where data is moved in milliseconds, not nanoseconds as with the CPUand main memory.andP;  And further, the periodic database backups create aninconvenience for computing centers that run beyond the typical business day.andM;A four-daemons approachandM;Even today, most DBMS publishers rely on traditional transaction logging, orsome variation, as the cornerstone of their error-recovery strategy.andP;  UnifyCorp., Sacramento, Calif., uses four background processes in itsimplementation.andP;  This set of daemons is described by Unify's director ofdatabase development, Bill Osberg, as &quot;the cache daemon, the file-managerdaemon, the clean-up daemon and a log-archive daemon.&quot;andM;1.andP;  The cache daemon is responsible for periodically cycling through theshared memory buffer pool and writing to a physical log those pages thathaven't been referenced in a certain period of time.andM;2.andP;  The file-manager daemon maintains physical file-system integrity byperidically checkpointing the physical log back into the database.andP;  Osbergexplains: &quot;For physical file-system integrity, we write pages that have beenupdated out to a physical log.andP;  We do this to keep a certain percentage offree space in the cache so that when a user process has to read in a newpage, there's a slot to do it.andP;  After a while, the physical log will fill upand we have to synchhronize the physical log with the database; that's wherethe file manager comes in.andP;  It first flushes all the dirty pages from cacheand out to the physical log.andP;  Then it copies the pages in the physical logback into the database.&quot;andM;The file-manager daemon also writes a record to the transaction log justbefore a synch point.andP;  The logical recovery process uses those synch markerswhen restoring from a failure.andP;  The DBMS only has to go back to the lastcheck point or, as Osberg notes, &quot;You don't have to recover transactions allthe way back to the beginning of time.&quot;andM;3.andP;  The clean-up daemon is responsible for detecting when user processes havedied, for whatever reason, without cleaning up their database resources.andP;  Itperiodically wakes up, cycles through the list of active transactions andsends a signal to each user process.andP;  If the user process isn't there, theclean-up daemon finds out about it through the error-return call.andP;  SaysOsberg, &quot;This way we can tell whether that process is still alive or not.andP;  Ifit's not, the clean-up daemon will abort the process's transaction, back itout, and release any other database objects that the user process haslocked.&quot;andM;4.andP;  The log-archive daemon is used to guard against media failure by writingcommitted transactions off to another storage device, typically tape, thusproviding another level of protection against errors.andP;  The log-archive demonwakes up at every synch point and cycles through the transaction log, lookingfor transactions that have been committed.andP;  It writes these off to thelogging device, freeing up space in the logical log for more transactions.andM;System failure recoveryandM;Once prepared for failure, recovery is easy.andP;  Included in the start-uproutine of most DBMS products is a step that involves looking at the system'sphysical and logical logs to see if the system had been shut down in anabnormal fashion.andP;  In the Unify system, the DBMS first looks to the physicallog to see whether or not the system was in the process of doing a synch.andP;  Ifit was, the DBMS will write the pages that were in the physical log back intothe database, thus re-establishing file-system integrity.andM;Since synch points happen relatively infrequently, this is not the normalcase.andP;  More often, systems are not in the process of doing a synch when theyfail.andP;  Normally, the DBMS can simply discard the physical log, because filesystem integrity has not been at all compromised.andM;Next, the DBMS reads through the logical-transaction log.andP;  This read beginsat the end of the log and proceeds backwards until the system finds thesynch-point marker.andP;  Then the DBMS takes note of which transactions had beensuccessfully completed after the synch but before the crash.andP;  Thosetransactions are the ones that the DBMS will have to &quot;redo.&quot;andM;Finally, the DBMS must undo the incomplete transactions.andP;  Osberg contends:&quot;You won't have to undo transactions that started after the synch but didn'tcomplete, because the synch point brought the database back to a state wherenone of their updates are in the database.andP;  But, if a transaction had startedbefore the synch point, and hadn't finished by the time the crash happened,you'll have to undo those.&quot;andM;Twists on traditionandM;Unify's approach is somewhat different from some other traditional strategiesin that it allows the database administrator to make a backup of the databasewhile the database is in use.andP;  &quot;That,&quot; says Osberg, &quot;is because, when we'redoing normal operations (when we're not doing a synch), we're not actuallywriting the new pages to the database; we're writing them to the physicallog.andP;  So, the database is in a consistent physical state.andP;  That allows us tomake a backup of the database while it's in use.&quot;andM;Oracle Corp., Belmont Calif., also reports innovations to make theerror-recovery process easier.andP;  Oracle's innovations come in on thepreparation side of the problem.andP;  To overcome the database performanceproblems associated with writing transactions to both the database and thelog file, version 6.0 of the Oracle DBMS includes support for fast commitsand group commits.andM;Says Oracle senior product manager, Gordon Smith: &quot;Other database productsmight store redo and undo information off to disk by writing out entire pagesof data.andP;  If, for instance, a block of a database needs to be modified, theDBMS would write out that block, before the modification happens, to abefore-image file of some sort [an undo file].andP;  Then it would write the blockwith the changes to an after-image file of some sort.andP;  This creates atremendous amount of I/O every time you do update operations.&quot;andM;With the fast-commit facility in Oracle version 6.0, only the changes made tothe database are written to the log, not the entire page or block.andP;  Smithcontinues, &quot;You just write out that, in such and such a row, such and such avalue was changed.&quot;andM;With the group-commit facility, Oracle writes redo, undo and commitinformation for multiple transactions in a single write to the log file.andO;Essentially, if multiple transactions are ready to commit at the instant thatanother transaction is about to be written to disk, they all are written outtogether.andP;  &quot;The result,&quot; Smith says, &quot;is that, you find that, when Oracleruns in multiuser, and particularly in high-use, situations we will bewriting less than one I/O per transaction to actually get all the data ontothe log file.&quot;andM;Oracle has discovered a side benefit to its error-recovery architecture thatpermits Oracle to support what it calls &quot;multiversion concurrency control,&quot;meaning that query operations can operate concurrently with update-intensiveOLTP (on-line transaction processing), without blocking each other or withoutotherwise interfering with each other's performance.andP;  The product uses theundo information when reading data that another process has updated betweenthe time that the read process begins and ends.andM;Smith offers an illustration: &quot;Say the query that you are running scans atable to sum up all the values of some field in each row.andP;  The query beginsreading each of the rows.andP;  An update operation comes along and wants tochange one of the rows before his query gets to it.andP;  You can go ahead and letthat happen with Oracle.andP;  You don't have to worry about taking off locks forthe read operation before the update can occur.andM;&quot;When the query gets to the appropriate row,Oracle will notice that an updatehas happened since the query began.andP;  It will allow the query to read theprevious value of that row by going to the undo information.andP;  There's nolocking; we do not take read locks out at all.andP;  That way, updates don't blockreads and reads don't block udpates.&quot;andM;Disk mirroringandM;Perhaps the logical extreme of any error-recovery strategy is the sort offault tolerance that Tandem Computers of Cupertino, Calif., has pioneered.andO;Tandem's systems not only provide a mechanism for recovering from systemerrors but can do so automatically so that, when a system error occurs, userscan continue processing without missing a byte.andP;  This fault tolerance isusually accomplished by duplicating system resources and operations.andM;According to Tandem product manager Rob Holbrook: &quot;To minimize the number oftimes the system fails because of a software error, we code our systemprocesses in process pairs.andP;  There's a primary process effectively doing thework.andP;  And there's a backup process that shadows the primary.andP;  It's ready tokick in if, for example, the CPU of the primary process fails.&quot;andM;Informix of Menlo Park, Calif., seems headed for this type of fault tolerancein a new version of its DBMS scheduled for shipping in the first quarter of1990.andP;  In this release, Informix will provide protection against disk failurethrough a disk-mirroring feature implemented in software.andM;Informix's disk-allocation scheme involves raw devices, where a raw device isa contiguous disk area allocated for the exclusive use of the Informix Turbodatabase.andP;  Very often, a raw device might span an entire disk.andP;  A raw devicecan be partitioned into &quot;chunks,&quot; wher a chunk is defined simply as astarting offset and an ending offset in the raw device.andP;  Then there is theconcept of a &quot;dbspace,&quot; a logical structure that contains one or more tables.andO;The database administrator assigns chunks to dbspaces.andP;  Additional chunks canbe assigned to the dbspace as it grows, and by using chunks from multiple rawdevices, the dbspace can span multiple disk drives.andM;In Informix's administrator's application a menu selection allows thedatabase administrator to opt for disk mirroring.andP;  The administrator firstchooses which dbspace to mirror.andP;  The application lists the chunks in thatdbspace and asks the administrator to list the device on which to create themirror for each chunk.andM;Informix then creates the mirror and turns on the mirroring process.andP;  Then,according to Informix product manager Tim Shetler, &quot;Whenever a modificationis made to the database mirror, Informix Turbo will automatically do twowrites; one to each chunk.&quot;andM;In the event of error, the system alerts the administrator, who can arrangeto have the bad disk replaced.andP;  While the system is being repaired, theInformix Turbo application can stay on line.andP;  Shetler says, &quot;All the time,people can be using the other mirror.andP;  You replace the disk that failed andthen go into the administrator's application and tell the system you want torecover a mirror.andP;  In the background, it will create that mirror for youagain on the fly.&quot;andM;Ron Carnahan is a senior management consultant with JYACC Inc., a New Yorkbased company providing management and technical consulting services as wellas a line of application development tools.andO;</TEXT></DOC>