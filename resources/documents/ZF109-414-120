<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-414-120  </DOCNO><DOCID>09 414 120.andM;</DOCID><JOURNAL>UNIX Review  Dec 1990 v8 n12 p48(4)* Full Text COPYRIGHT Miller Freeman Publications 1990.andM;</JOURNAL><TITLE>Polishing the apple. (C Advisor) (column)</TITLE><AUTHOR>Allman, Eric.andM;</AUTHOR><DESCRIPT/><TEXT>In my last few columns, I've been talking about using various profilers tooptimize your programs, in Vol.andP;  8, No.andP;  8 we saw that by using prof you canoptimize individual modules using procedure-call counting and a statisticalmethod that approximates the time spent in individual routines.andP;  Then in Vol.andO;8, No.andP;  10 the gprof program was described, which extends prof'sprocedure-call counting to produce a call graph; this gives information aboutthe inter-module structure of our program.andM;In this column I want to continue that theme, discussing the &quot;big picture&quot; ofoptimization.andP;  In particular, we'd like to know how to make the best use ofthese tools.andM;Motherhood and Apple PieandM;Not too surprisingly, a lot of what is good for optimization is what you havebeen hearing all along.andP;  Good coding style, clean semantic decomposition, andabstraction techniques can help you optimize.andP;  This stems from two importantrules: first, 90 percent of the execution time of your program will be in 10percent of the code, and second, you almost certainly don't know which 10percent are the hot spots,andM;This brings us to corollary one: don't pre-optimize your code.andP;  Many are thetimes I have seen programmers optimize a piece of code as they write it,adding fancy binary searches or caching, when in fact that code isoverwhelmed by another module, or the code in question is only executed onceon a very small data set.andP;  Viewed by itself, a module may be expensive, butyou shouldn't optimize modules, you should optimize systems.andM;The easiest way to optimize a system is to build it, try it, and thenoptimize it based on actual data.andP;  The 90/10 rule suggests that it's reallynot worth optimizing the whole system.andP;  Doing optimization well is difficult;there's no point in wasting your time on parts of your program that aren'tbottlenecks in the first place.andM;Of course, there are exceptions that should be considered.andP;  Certainoperations may not take a large percentage of your program's run time, buthave psychological requirements.andP;  For example, a window system probablydoesn't spend a lot of cycles popping up menus, but when the user pushes thatmouse button they expect fast (read &quot;instantaneous&quot;) response.andP;  These have tobe considered individually.andM;Keeping subroutines small is an important technique to milk as muchinformation as possible out of the profilers.andP;  Most profilers today presentthe information on a per-routine basis.andP;  If you have a large routine thathappens to have one loop out of several that is very expensive, the profilerwon't help you find the loop in question.andP;  Some people carry this to anextreme, insisting on only one loop per subroutine.andP;  I have no do that thismakes profiling more useful, and may even help some phases of debugging, butfrankly, my feeling is that if you can't figure out a good name for asubroutine, it probably isn't semantically meaningful by itself, and shouldbe included as part of another routine.andP;  You will have to make up your ownmind.andM;The tradition of large routines and of in-lining code (that is, reproducing asubroutine rather than calling it) results largely from the VAX era, wherethe overhead of a subroutine call/return was very expensive.andP;  One of thedelights of RISC machines is that they cut subroutine overhead to the barebones.andP;  Even if subroutine overhead is still an issue for you, I stillrecommend clean semantic decomposition first.andP;  if you need to optimize thecode later, you can merge routines together.andP;  This is almost always easierthan taking apart a too-large routine later.andM;Environmental ProblemsandM;This points out another danger of optimization in today's world.andP;  &quot;Back inthe good old days&quot; (when UNIX ran on the PDP-11, thank you very much) &quot;tooptimize&quot; meant &quot;to optimize for a PDP-11.andP;  We knew to use postincrement(*p++) and predecrement (*--p) , since these were implemented in hardware.andO;We knew that routines should have three register variables, since that's howmany registers were available.andP;  (I've even seen programs that &quot;passedvariables&quot; in registers; that is, rather than passing the variables in theparameter list, they would declare the register variables in the same orderin the caller and callee; since the registers weren't trashed when asubroutine was called, they could effectively pass parameters this way.andP;  itwas very, very fast.andP;  And about as unportable as you can get.)andM;When UNIX started seeing ports, first to the Interdata 8/32 and later toother systems such as the VAX, problems started to arise.andP;  For example,in-lining works well on the VAX, but can cause your programs to grow toolarge.andP;  On some architectures this will cause the compiler to switch fromshort-mode addressing to long-mode addressing.andP;  in other words, what is goodon one architecture may de-optimize your program on another architecture.andO;Other examples abound.andM;Sometimes you have a great deal of knowledge about the environment.andP;  Forexample, in my incarnation as a neural-networker, we would write code thatwas carefully designed to fit into the on-board cache for a particularsystem.andP;  This can make all the difference in the world-for example, we hadone program that we tried on two systems, called (for the sake of discussion)S and M. Originally, S ran faster than M, but that turned out to result froma larger data cache on S. A different model of M had a larger data cache,which turned out to run much faster than S. in this environment, we weremeasuring our run times in days or even weeks.andP;  Low level hacks arelegitimate techniques here.andM;But most of the time you want to write code that will run well in a varietyof environments.andP;  Low-level hacks are just going to come back to bite you.andM;LibrariesandM;It's worth pointing out the special case of writing libraries.andP;  Since youdon't have the full system (your users will write that) you can't profile thewhole thing.andP;  There is a strange tension here-programmers optimize theirprograms in part to get around inefficiencies in the library (for example,calling bcopy instead of strcat when possible) , but library writers try tooptimize for the &quot;expected&quot; program.andM;About the only thing you can do here is to come along later and find some&quot;typical&quot; programs that use your library, and do the optimization later.andO;This is the technique used for the standard I/O library, for example.andM;Other MonitorsandM;A lot of systems require ongoing performance evaluation and tuning that canbe made without modifying C code.andP;  For example, when you get a UNIX operatingsystem, you can make a lot of decisions regarding things like disk balancingthat are independent of code changes.andP;  It would hardly be appropriate to shipa kernel and ask customers to examine prof output.andM;Instead, most versions of the kernel come with various performance monitorsinserted.andP;  Typically these are simple counters that can be read and displayedby an appropriate program.andP;  The iostat program is a good example of this inBSD-derived systems.andP;  Using various flags, I can see the ongoing load fromdisk traffic, terminal I/O, and CPU overhead.andM;The netstat program has similar numbers for the network, and vmstat displaysthe virtual memory performance.andP;  An especially interesting (albeit dense)display that merges all of these is shown by systat-v which manages to putall of this information (and more!) on one 24 x 80 screen.andM;If you are going to have this sort of external monitor, a good rule is toavoid doing any but the simplest data reduction in the system beingmonitored.andP;  An example of the kind of reduction you will need to provide arethose needed to constrain the space needed to store the values.andP;  For example,the rawest data describing disk performance is to output a record every timea disk access is performed.andP;  This gives you a great deal ofinformation-besides the fact that a transfer has been made, you know when,the disk block accessed, and whether the operation was a read or a write.andO;This is called a trace, and is very useful for certain kinds of performancestudies.andP;  However, there are two large problems with this.andP;  First, the amountof data collected is huge.andP;  Second, you have to put the datasomewhere-probably on disk-so the fact that you are collecting the data canchange the data being collected.andP;  This is called an artifact.andP;  The artifactcan be reduced (for example, by dedicating a disk or a tape drive to collectthe data; writes to this drive don't tend to pollute the information beingcollected from the rest of the system) or factored out-for example, byignoring all the writes in the trace that are in the area used by the tracelog.andP;  Notice however that in either case the timing figures will be skewed,since it takes time to write out the trace data.andM;A simple alternative to this is to use a one-word counter that getsincremented when you access a particular disk drive.andP;  If you want more detailwhile still keeping the space needed down, you could have one word percylinder on the disk.andP;  Similarly, if you wanted to measure reads versuswrites, you could have two counters.andP;  In any case, you have lost the timeinformation and most information about the order in which disk blocks weretouched.andM;Programs like systat regain some of the timing information by collecting thenumbers over short intervals, typically one to ten seconds.andP;  Within thecollection interval you cannot tell when disk activity occurred, but you cantell that it happened in this interval rather than that interval.andP;  Forexample, when we read the stock-market pages we normally see figures like&quot;the market fell 53.42 points yesterday&quot; (a sample interval of one day) .andO;Occasionally you may see the figure hour-by-hour.andP;  But of course, the realityof the market is that trades occur continuously.andP;  We simply don't need thatdetail to get a good feel for how things are behaving.andM;You do have to be careful of range errors on many of these techniques.andP;  Forexample, suppose you are counting the total number of interrupts received.andO;If you use a 32-bit counter, you can safely store up to 4,294,967,296 events.andO;This seems like a lot, but if interrupts come in at, say, 100 per second (notan unlikely number these days) , you will overflow that counter in just overa year.andP;  Pshaw! you say.andP;  Well, phone switches can and do stay up that long,and an order of magnitude increase in processor speed (it's been a good betso far) reduces that number to about fifty days-well within the uptime rangeof UNIX systems today.andM;A particularly tricky technique for storing very large numbers in smallcounters is described in Morris's CACM article &quot;Counting Large Numbers ofEvents in Small Registers&quot; (Communications of the ACM, 2 1, 1 0, October1978, pp.andP;  840-842).andP;  This solution has the property that while the numbersare small, the counter is very accurate, but as the numbers get larger, thecounter increments less quickly.andP;  Briefly, the counter stores the value log(1 + n) , where n is the actual number of events so far.andP;  When an eventoccurs, the counter is incremented with a probability based on the currentcounter.andP;  If it is small, it is more likely to be incremented.andP;  As it getslarge, it will be incremented more slowly.andP;  The data won't be completelyaccurate, but often all we need is to know that &quot;about ten&quot; or &quot;about 1000&quot;events have occurred.andP;  Note, however, that this technique is not appropriatefor systems that do periodic sampling unless the act of sampling also resetsthe counter.andM;Eric Allman works on the Mammoth project at the University of California atBerkeley and is active in the Usenix Association.andP;  Previously, he worked atthe International Computer Science Institute, Britton Lee Inc., and on theINGRES project at the University of California.andP;  During his first incarnationat Berkeley, he authored sendmail, the -me macros, and the sccs front end,among other contributions to BSD.andO;</TEXT></DOC>