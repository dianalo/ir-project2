<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-228-509  </DOCNO><DOCID>07 228 509.andM;</DOCID><JOURNAL>PC Magazine  Jan 31 1989 v8 n2 p85(2)* Full Text COPYRIGHT Ziff-Davis Publishing Co. 1989.andM;</JOURNAL><TITLE>Seek and ye shall find - but how? (locating useful information onhigh-density storage media can be difficult) (column)</TITLE><AUTHOR>Manes, Stephen.andM;</AUTHOR><SUMMARY>Storage technology is improving rapidly and as the density ofstorage media increases the need for high-performance searchsoftware is becoming more evident.andP;  Techniques such as indexingare useful, but only to a point.andP;  Users must be able to findrelevant information quickly and indexing is inadequate since itdoes not arrange retrieved material in any useful order.andO;Computers, however, could be used to analyze retrieved articlesand determine which are useful, saving the user the task of havingto read every article.andP;  Algorithms that can do this have beenaround for a long time, but efforts to implement them are hamperedby arguments concerning their effectiveness and by their slowoperation.andM;</SUMMARY><DESCRIPT>Topic:     Search StrategyInformation Storage and RetrievalUser NeedsCD-ROMData bases.andM;</DESCRIPT><TEXT>SEEK AND YE SHALL FIND--BUT HOW?andM;With the help of CD-ROM, giant hard disks, and other storage technologies,you can now put gigabyte upon glorious gigabyte of information at yourfingertips.andP;  The catch: How do you winnow the wheaty information you reallyneed from the enormous pile of data chaff?andP;  There are so many differentanswers that n unheralded guerrilla was is erupting for dominance in searchand retrieval software.andP;  So far there's nothing that even remotely resemblesa standard; no individual product seems to hold even as much as 10 percent ofthis fragmented market.andP;  In part that's because the software remains in thedark ages.andM;OLD AS THE WORDandM;The problem of text retrieval is as old as the written word.andP;  Over the ages,scholars and librarians have developed and refined elaborate indexing,abstracting, cross-referencing, and cataloging systems.andP;  But the computerpermitted--and demanded--utterly new and vastly more powerful methods offinding relevant information.andM;These techniques showed up first in on-line databases--from which most oftoday's retrieval tools are all too obviously descended.andP;  Most of the earlymodel retrieval software was so dreadful it spawned a cadre of trainedspecialists; inexpert users' stumbling over question-mark-prompt interfacescould run up charges resembling the gross national product of third-worldcountries.andP;  With the advent of the personal computer, the software gotmarginally better--in many cases, via the expedient of a PC-resident shellthat helped bring order to the chaos of the multifold ugly faces presented bydistant mainframes.andM;The techniques of retrieval soon migrated to PCs in such products as ZyIndexand GOfer, which allow you to search through your own files via Boolean AND,NOT, and OR means as well as such non-Boolean terms as NEAR.andP;  GOfer usesbrute force to search in much the same way as a word processor; ZyIndex, onthe other hand, must create and store information about each file in an&quot;inverted index&quot; before it can search.andP;  The problems with indexing are itsdemands on disk space and the user's memory; forgetting to index a filerenders it nonexistent for search purposes.andP;  But once an index is ready, itspeeds up searching enormously.andP;  Whether indexing is a good idea forchangeable data can be a tough call.andM;But CD-ROMs are so inherently slow that any retrieval software worth its saltmust make use of indexes--extensively.andP;  In fact, the indexes themselves--whatthey contain, how they're structured, and how they relate to the physicallayout of the disk and the text stored on it--are crucial to retrievalperformance.andP;  For example, indexes can be carefully structured to avoidmaking the long head seeks that are the bane of CD-ROM access times.andM;Current CD-ROM software, alas, tends instead to rely on quick-and-dirty waysof improving performance.andP;  If, for example, you're creating a single diskwith the contents of 40 separate books, you can set up 30 or 40 separateindexes and allow users to search only one at a time--reasonable enough ifthe separate databases contain wildly disparate information.andP;  But Microsoft'sProgrammer's Bookshelf uses a variant of this technique to the detriment ofthe product.andP;  Want to search the whole disk for every reference to interrupt17?andP;  No can do: it's up to you to search through nine separate sections.andM;But that's a minor problem.andP;  The key question with any big collection of datais how the user gets relevant information.andP;  Spped isn't simply a mechanicalfunction of the retrieval engine; it's a question of the user interfaces aswell.andM;Let's say, for example, you're looking for articles about using CD-ROM in thereal estate business.andP;  You perform a typical seach on CD-ROM AND REAL ESTATE.andO;The software finds 50 &quot;hits&quot;--articles that match your seach.andM;At this point the software might show you an abstract or title for eacharticle it's found.andP;  It might even let you search through each article andfind the words you searched on.andP;  But the catch--critical when hundreds of&quot;hits&quot; are on the list--is that it probably won't arrange the material in anyuseful order.andP;  You could search more narrowly--by taking the current searchand restricting it with, say, SEARCH 1 AND PROPERTY VALUES--but that runs therisk of eliminating articles you really might want to look at.andP;  So you'reforced to look through virtually every article if you want to be thorough.andM;But what if the software made some guesses--based on, say, the number ofappearances of your search words in each article relative to its length?andP;  Theprogram could then present you with the articles it suspects are mostrelevant at the top of the list and the least relevant items--likely &quot;falsehits&quot;--at the bottom.andM;And studies show that even trained searches miss lots of relevant articles.andO;What if the computer could actually analyze the articles you have found tohelp find the ones you overlooked--for example, by noticing that certainwords appear with significantly more than random frequency in the articlesyou've found and then generating a search based on that information?andM;Algorithms to do this have been around for a long time; implementations arescarce, in part because experts differ on their effectiveness, in partbecause such analysis can be extremely slow.andP;  But here again, indexingtechniques might come to the rescue.andP;  If the index contained most of thecontent analysis already, response could be speeded significantly.andM;Indexing optimization aside, there are other ways of improving performance.andO;One is to buffer intermediate results in a big chunk of RAM, on the theorythat you may want to &quot;go back&quot; and check out something you've already seen.andO;So far most CD-ROM software doesn't work that way; just moving the cursorthrough a list of topics in programs such as Microsoft Bookshelf often causesannoying waits.andM;Another technique (ripe for OS/2) is the idea of background queuing ofinformation as you work in the foreground.andP;  The idea is to keep pouringlikely information into RAM even while you're just reading the screen,deciding what to do next.andP;  If, for example, your search came up with 50items, the full text from those items would begin to be yanked from disk intoRAM even before you asked for them in a sort of &quot;prefetch cache&quot; arrangement.andM;Then there's hypertext--but fortunately I'm out of space.andP;  No matter: thedevelopment of text retrieval is still in its infancy, with plent of time toadd heft and sinew as it grows up.andP;  If you want a real challenge, think abouthow we're going to retrieve graphics and sound.andO;</TEXT></DOC>