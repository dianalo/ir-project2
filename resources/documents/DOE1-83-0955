<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO> DOE1-83-0955 </DOCNO><TEXT>Conventional query processing techniques are aimed at queries which accesssmall amounts of data, and require each data item for the answer. Incase the database is used for statistical analysis as well as operationalpurposes, for some types of queries a large part of the database maybe required to compute the answer. This may lead to a data access bottleneck,caused by the excessive number of disk assesses needed to get the datainto primary memory. An example is computation of statistical parameters,such as count, average, median, and standard deviation, which are usefulfor statistical analysis of the database. Yet another example that facesthis bottleneck is the verification of the truth of a set of predicates(goals), based on the current database state, for the purposes of intelligentdecision making. A solution to this problem is to maintain a set of precomputedinformation about the database in a view or a snapshot. Statistical queriescan be processed using the view rather than the real database. A crucialissue is that the precision of the precomputed information in the viewdeteriorates with time, because of the dynamic nature of the underlyingdatabase. Thus the answer provided is approximate, which is acceptableunder many circumstances, especially when the error is bounded. The tradeoffis that the processing of queries is made faster at the expense of theprecision in the answer. The concept of precision in the context of databasequeries is formalized, and a data model to incorporate it is developed.Algorithms are designed to maintain materialized views of data to specifieddegrees of precision.</TEXT></DOC>