<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-384-700  </DOCNO><DOCID>07 384 700.andM;</DOCID><JOURNAL>Computer Language  July 1989 v6 n7 p111(4)* Full Text COPYRIGHT Miller-Freeman 1989.andM;</JOURNAL><TITLE>Issues in distributed databases.andO;</TITLE><AUTHOR>Rapaport, Matthew.andM;</AUTHOR><SUMMARY>Distributed data base systems, because they localize data where itis most often used and accelerate query performance, aretheoretically more effective than centralized multi-user systemsin large organizations.andP;  Heterogeneous distributed data basesremain a fantasy due to a number of technical hurdles.andP;  Threehigh-level developments (global data reference, converting datafrom native to query formats, and network intermediaries for theapplication software) are necessary before data scattered amongmultiple computers can be turned into a distributed data base.andO;Distributed systems running on local area networks need acommunications layer which represents an enormous programmingtask.andP;  Other problems include inconsistency and distributing thesystem catalog.andP;  Distributing data in large organizations alsoraises political questions because people must adapt to workingtogether on new tasks.andM;</SUMMARY><DESCRIPT>Topic:     Distributed Data BasesConcurrency ControlNetwork ManagementData Base Management SystemsSystem Design.andM;</DESCRIPT><TEXT>Issues in distributed databases When it comes time to choose a database, youhave the option of picking a distributed database or a multiuser system.andO;Distributed databases have two advantages over centralized systems.andP;  First, adistributed database localizes data where it is most often used.andP;  Second, itaccelerates query performance by reducing communication between remote nodes.andO;Because natural divisions sometimes occur with data in a shared database,significant proportions may be used by a localized operations group.andP;  Thisgroup might be a department in a larger organization or a geographicallyseparate operation, such as a regional headquarters.andM;Distributing data to remote groups permits use of less-expensive orspecialized hardware that is easier to maintain and better serves each localgroup's needs.andP;  When data is not easily localized, distributing copies ofdata tables to multiple machines may enhance the entire system's performance.andM;Data distribution is not the same as process or computation distribution.andP;  Ona theoretical level, process and computation distribution share some of thesame concerns, such as synchronization, recovery, consistency, andreliability.andP;  The goal of process distribution, even if it involves onlyconventional, sequential computer systems, is to make use of idle CPUcapacity.andM;In data-distribution architectures, the system making a database requestperforms all processing.andP;  Other machines involved contain data pertinent tothe request.andP;  The initiating node makes a data request.andP;  The system isinformed of the data's location through a directory facility.andP;  Upon receivingthe request, the remote system supplies data.andM;For example, a company has 10 regional sales offices that track localsalesactivity.andP;  A user at the corporate headquarters might want to sum sales foreach week by seller.andP;  As each regional machine receives requests for thedata, the data for each seller is transmitted to the requesting system.andM;This transmission includes all data in the sales records, including itemsthat have nothing to do with the specific request.andP;  All summation and dataprojection activity is performed by the system making the request.andO;Processing in the remote nodes is limited to sending the data.andP;  No datamassage needs to be performed.andM;Distributed processing is concerned primarily with minimizing data andcontrol information communication between systems.andP;  This communicationsoptimization is achieved by performing just enough data segregation on eachmachine participating in a multi-system transaction and exporting the minimumdata required to achieve transaction objectives.andM;Radical, heterogeneous distributed databases are a fantasy.andP;  Vast amounts ofvaluable data are often scattered among micro- and mini-computers throughoutcorporations.andP;  Taken as a whole, these computers compose a databasecollection, though they are not coordinated or controlled.andP;  Turning thiscollection into a distributed database requires three high-leveldevelopments.andM;First, some global reference to existing data must be created.andP;  Thiscatalogue would contain metadata about the target data's location and format.andO;Location information includes the network address of the target system andwhere the data resides on the system.andP;  Location specifications might includesuch things as disk designations, file names, and column identifiers.andO;Desired data might exist in many formats, including spreadsheets, graphicssystems, and various database formats.andM;Dictionary distribution must be addressed in any distributed database.andP;  Thedictionary must be distributed to all systems that have authority to pulldata from the rest of the network or it can reside in one machine, forcingeach requesting system to determine where to go for other data.andP;  Ifdistributed, the dictionary must be kept up to date in a controlled manner.andO;Centralized data dictionaries are easier to maintain, but cost something incommunications overhead, as every request has to first address the centraldictionary.andM;Second, some utility set must be available to convert data from its nativeformat to formats found in the querying system's application software.andP;  Ifthe querying systems' DBMSs are standardized, developers need concernthemselves only with conversions among the target data and standard DBMSformats.andP;  If systems making queries can each have different DBMSs thatanalyze data, many more converters will be necessary.andM;Third, the application software that controls the data requires anintermediary between the network through which data is communicated andtranslated.andP;  Externally, the intermediary layer presents data or commands toextract data to the application software on either end of a connection.andP;  Thisdata might be SQL requests or spreadsheet macros.andP;  This layer connects eitherdirectly to the target system's data or controls the translator's execution.andO;With the layer in place, data is in a format suitable for network transfer.andM;This software layer must also assume responsibility for transaction integritymanagement.andP;  A network operating system or ode interface might inform aremote transaction that a file is busy.andP;  Handling the clean continuation ofthe transaction would fall to the communications handler.andP;  If the initiatingsystem were DBMS-based, this might mean informing the DBMS of a locked fileand responding to subsequent messages.andP;  If the originating request came froman application without its own integrity controls, the communications handlerwould be responsible for intelligently stopping or modifying other actionsthat might be related to the query.andM;IF the distributed system runs on a local area network, the communicationslayer assumes the role of the sixth presentation layer in the ISO seven-levelmodel.andP;  If the network is based on something more ad hoc (for example,independent systems joined through a telephone network), the communicationssoftware may assume as many as four levels (presentation, session, transport,and network) of the ISO model.andP;  Configurations could exist anywhere inbetween.andM;In theory, a distributed system could be built today, in the DOS and UNIXenvironments.andP;  The Macintosh world has its own tool-set for producing suchintermediate layers, and the programmers at Apple Computer Inc. are busybuilding connections between their systems and IBM mainframes.andP;  However, theprogramming task is, at the current level of technology, enormous.andM;Implementing a communications layer would be difficult enough.andP;  However,maintaining an up-to-date data dictionary is prohibitive in CPU cost.andP;  Alarge network might have thousands of spreadsheets and databases.andP;  As theirstructures changed, the changes would have to be communicated to thedictionary.andP;  Currently, only a few application programs have a means ofcommunicating such changes without operator intervention.andM;These difficulties imply that the dictionary must contain metadata thatmirrors the file structure of every application in use throughout thenetwork.andP;  Further, this matching must be detailed enough so any change thataffects the outcome of any legal query in the network wi ll be incorporated.andO;As structural changes occur in the network's file collection, their impactmust be automatically detected and communicated to the dictionary.andP;  The CPUoverhead of testing every affected file before any query is made against itcould overwhelm the individual CPUs responsible fopr the data.andM;Vendors have chosen to concentrate on achieving data and process distributionin a more homogenous environment, where the distributed DBMS can use thecommunication controls of a network and, at the application layer, operatewith a limited number of data types and structures.andP;  The need for convertersbetween one application and another are eliminated by the DBMS engine,because all applications built around the engine must use its data tables andphysical file structures.andM;Even where one DBMS engine controls all corporate data, a significant numberof problems still remain.andP;  Suppose, for example, that the major tables of aDBMS application are replicated on geographically remote machines to reducecommunications overhead.andP;  If we consider queries against this database, thedistribution strategy makes a lot of sense.andM;When it comes time to insert or update data, we run into trouble.andP;  If eachtable can be assigned a master node, all insert and update transactions for agiven table are communicated to their masters.andP;  The master node updates eachreplicated copy of the table, either immediately or with a frequencyappropriate to the application.andP;  If u pdates are periodic, queries done atthe remote nodes may not always reflect the state of the master table.andP;  If updates are done immediately, the communications overhead required might cancelany gains achieved by copying the tables for query purposes.andM;If each node containing a copy of a replicated table is free to update thatcopy, the combined updates of each node must be communicated to the rest ofthe network immediately or the same record on two nodes may be updateddifferently.andP;  Consistency control is difficult enough with a centraldatabase; consistency becomes much more complicated if multiple data tablesmust be kept current at the same time.andM;Two-phase locking is the most common form of consistency control.andP;  In phase1, locks to all records being updated by a transaction are acquired.andP;  Phase 2ensures that no locks are released until the DBMS recognizes that all updatesassociated with this transaction are successful.andP;  If multiple copies areinvolved, locks must be acquired for all the copies.andP;  If one update fails,the transaction is inconsistent and fails for all table copies.andM;Inconsistency is not the only problem.andP;  Networks can become partitioned; thatis, a communication system failure may temporarily isolate some nodes.andP;  Ifpartitioning occurs and systems on both sides of the partition continue toupdate table copies, the DBMS cannot ensure that copies can be madeconsistent after the communication link is restored.andP;  Such transactions wouldhave been rpevented if system communication had never been partitioned.andM;If the distributed DBMS does not use copied tables, consistent table mergingon each side of the partition is theoretically possible after the network isrejoined.andP;  However, the difficulty involved is proportional to the degreethat the database tables are normalized.andP;  If fully normalized, many moretransactions will be required to make the database consistent.andP;  Deliberatelydenormalizing the system's primary tables makes the reconstruction processeasier but complicates consistency and integrity control when the network isoperating normally, consuming more communication bandwidths updatingredundant attributes.andM;Considerations about the distribution of the system catalogue or dictionaryparallel those of data distribution.andP;  If the catalogue is purely centralized,a node partitioned from the central catalogue will lose all capacity to useits tables, even for query purposes.andP;  Under normal, nonpartitionedcircumstances, every transaction must begin by addressing the centralcatalogue, cancelling all distribution gains.andP;  If the catalogue is fullycopied on all nodes, catalogue updates present the same problems as copieddata tables.andM;If the catalogue is distributed so each node contains a copy of the cataloguepertinent to its local tables, a node starting a transaction must query everynode in the system to see if it contains data relevant to the transaction.andO;However, some central, complete catalogue must still be maintained.andM;Beyond the technical problems, many political hurdles must be overcome when alarge organization contemplates distributing data.andP;  People who do notnormally work together must find the time to do so.andP;  Parts of organizationsused to the services provided by centralized MIS must take responsibility formaintaining data, while groups conditioned to ownership of their local datamust allow others access to that data, even though they may continue to beresponsible for protecting it.andM;Whether or not to distribute a database is a major design decision.andP;  Usually,we want the system design to be as independent of the physical implementationas possible.andP;  However, communication among multiple systems imposes a set ofconditions that decreases overall system reliability.andP;  Communication alsointroduces a potential bottleneck because intersystem communication bandwidthis inevitably lower than intrasystem CPU, memory, and I/O bandwidths.andP;  Wecan't assume that a correctly designed system will function in bothdistributed and centralized implementations.andM;Distributed databases should not necessarily be ruled out.andP;  Performanceproblems will disappear as faster and more reliable communication channelsevolve and new transaction, consistency, and integrity mechanisms develop.andO;Even now, environments exist that lend themselves to designs involvingdistributed data.andP;  One environment suggesting a distributed approach wouldhave geographically dispersed nodes where most transactions are applied todata that is naturally localized.andM;Another possibility, using multiple table copies with centralized updating,makes sense if the majority of transactions are queries and updates takeplace infrequently or on a periodic, scheduled basis.andP;  Large collections oftextual and statistical data used for reference may fall into this category.andM;A third possibility involves high-performance LANs where budgets eliminatepartition problems with expensive, fault-tolerant file or DBMS servers andredundant, high-bandwidth communication channels.andP;  This solution becomes morereasonable as fiber optic network connections become more common.andM;We are still left with a majority of DBMS applications better offcentralized, either in a single DBMS server connected to a LAN or on amultiuser system.andP;  Data should not be distributed just for the sake ofdistribution.andP;  DBMS theory and application have come a long way in the last20 years.andP;  Large centralized DBMS systems are now available with multiplelevels of data integrity and transaction-consistency insurance.andP;  By contrast,distributed implementations are in their infancy.andP;  Many problems must beresolved before risking corporate data assets on an immature technology justbecause it exists.andO;</TEXT></DOC>