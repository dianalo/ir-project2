<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO> WSJ870827-0016 </DOCNO><HL> LEUSURE andamp; ARTS -- Bookshelf:One Man, One Cray---By George Gilder</HL><DD> 08/27/87</DD><SO> WALL STREET JOURNAL (J)</SO><TEXT>   Is this the supercomputer era? Or is the supercomputer in its present form already obsolete, as industry guru Carver Mead of California Institute of Technology has charged?    Supercomputers, mostly made by a Minnesota firm called Cray Research Inc., operate at near 10 gigahertz, or 10 billion cycles per second, and are some 20 times more powerful than IBM mainframes and thousands of times more powerful than personal computers. Yet, with a few additional features, they use essentially the same architecture as personal computers. The chief issue in the industry is how long this conventional architecture, invented by John Von Neumann in 1946, can prevail.    &quot;The Supercomputer Era&quot; (Harcourt Brace Jovanovich, 313 pages, $19.95) by Sidney Karin, director of the San Diego Supercomputer Center, and Norris Parker Smith, a writer at the Energy Department's Livermore Laboratories, gives a detailed and balanced account of the supercomputer and its uses from the point of view of the supercomputer establishment. The hero of the book is the National Science Foundation, which launched a five-year, $200 million program to establish supercomputer centers at several American cites, including San Diego.    The leaders of N-Cube Corp. -- a vendor of &quot;massively parallel&quot; computers that promise to outperform the dominant Crays by a factor of hundreds -- have not been allowed in the door of the San Diego center and are left unmentioned in this otherwise illuminating volume. Mr. Mead and other proponents of the new processing structures believe that the multiprocessors and parallel architectures such as N-Cube's will enhance the cost effectiveness of computing by a factor of millions in coming decades. But Messrs. Karin and Smith suggest that progress is getting harder and slower. The authors believe that the widespread use of parallelism remains a long shot, balked by the problem of adapting software to the new architectures. They also fear that conventional supercomputers may be approaching their fundamental physical limits.    Judging from the technologies described in this book, the authors make a strong case. Supercomputer designers are immersing their machines in liquid nitrogen to carry off the heat and in liquid helium to achieve superconductivity in the interconnections; they are juggling deadly arsenic in the form of gallium arsenide to increase switching speeds by a few billionths of a second; they even contemplate launching their machines into outer space to chill them to superconductivity. All in all, they are struggling to get the central processing units, vector processors, co-processors and storage devices to run faster while they look over their shoulders at the Japanese, who have recently sold their first supercomputer in the U.S.    In the most telling chapters of the book, the authors show the voracious and almost unlimited need for computer power unleashed by modern science and industry. From fluid dynamics and quantum physics to chip simulation and bio-engineering, from strategic defense and submarine acoustics to heart modeling and materials synthesis, existing supercomputers are extremely useful but as yet far from sufficient.    With supercomputers urgently needed for thousands of projects, it is as important to make supercomputing powers affordable to the masses of scientists as it is to make marginal speed improvements at the top of the line. Current supercomputers cost some $15 million to buy and time-sharing charges are some $1,000 an hour. Many urgent projects require hundreds of hours.    In this domain of cost effectiveness, the supercomputer establishment, dominated by Cray, now faces a dire challenge. The assault comes from vendors of highly parallel minisupercomputers, now among the fastest-growing firms in the industry. The most attention and largest profits are currently going to relatively conservative designs from companies such as Convex Computer Corp. and the Cray-compatible Scientific Computing Corp. But an even greater threat to Cray would be a breakthrough in massively parallel machines based on the hypercube architecture conceived by Charles Seitz at Caltech or the Thinking Machines model, also a hypercube, conceived at Massachusetts Institute of Technology and then produced by Daniel Hillis, who hooked up 64,000 processors in parallel.    Lurking on the horizon is a more far-reaching issue in the supercomputer field: Whether the general-purpose computer in all its forms is obsolescent. Recent breakthroughs in computer-aided engineering enable designers to create specialized chips with massively parallel architectures to achieve targeted capabilities of artificial intelligence. These chips, which can be designed and manufactured profitably for less than $100 apiece, far excel supercomputer performance for specific tasks. These functions -- computer simulation, graphics, animation and various forms of pattern recognition -- closely resemble the leading current uses of supercomputers.    So by all means read this compelling story of the supercomputer establishment. But this era still belongs chiefly to the microcomputer and to the application-specific parallel system. The supercomputer era awaits the creation of a desktop supercomputer for $10,000. It will happen sooner than they think at the San Diego center.    ---   Mr. Gilder writes frequently on technology issues. </TEXT></DOC>