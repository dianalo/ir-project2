<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-567-497  </DOCNO><DOCID>09 567 497.andM;</DOCID><JOURNAL>Communications of the ACM  Oct 1990 v33 n10 p119(11)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Economic analysis of microcomputer hardware. (technical)</TITLE><AUTHOR>Lynch, Brian D.; Rao, H. Raghav; Lin, Winston T.andM;</AUTHOR><SUMMARY>An econometric analysis of the purchasing of computer hardware,especially microcomputers, uses the Box-Cox quantitativestatistical method to interpret the more complex computer market.andO;Since microcomputers are no longer equivalent to simple personal,single-user machines, businesses are altering their buyingdecisions accordingly.andP;  The choices are more complicated sincemicrocomputers may be multi-user systems, workstations and fileservers, any of which may provide more computing power for thecost than an ordinary mainframe.andP;  An analysis of the cost-benefitsinvolved in computer purchasing decisions using the Box-Coxmethodology yields better estimates than older methods, especiallywhen partial- or double logarithmic models are used.andP;  Factorsparticular to microcomputer hardware selection become apparent andthe relationship among these predictors can be clarified.andM;</SUMMARY><DESCRIPT>Topic:     MicrocomputersComputer industryEconomics of ComputingHardware SelectionPurchasesIndustry AnalysisQuantitative MethodsTechnology.andO;Feature:   illustrationtable.andO;Caption:   Comparison of computer unit sales. (table)Cost per MIPS (in thousands). (table)Summary of previous studies. (table)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>ECONOMIC ANALYSIS OF MICROCOMPUTER HARDWAREandM;The 1980s have been marked by the commodization of the microcomputer hardwaremarket [1, 21] and the emergence of the microcomputer as a significant factorin organizational computing power.andP;  The ascent of the microcomputer isemphasized by the concentration of computer power shifting from mainframes topersonal computers (PCs) and workstation microcomputers as shown in Table 1.andO;This is not surprising since purchasers get substantially more in terms ofinstructions per second for their money in the micros as compared to themainframes.andP;  Table II shows this in detail.andP;  A trend associated with thegrowth of these low-cost powerful micros is that purchase decisions arefrequently made by computing practitioners who range from highly trainedcomputing professionals to end users such as senior executives, businessentrepreneurs, and clerical workers [12, 16, 17].andM;The traditional textbook definition [17, 23] of microcomputers labels thesemachines as personal computers, primarily because they were designed for useby one person at a time.andP;  Until recently, &quot;micros were personal computers forindividual use only&quot; [23].andP;  Technological developments, however, have been sorapid now that vendors are marketing products that permit &quot;several users atonce: the multiuser micros that are configured with workstations&quot; [23].andP;  Todate, reearch in the economic analysis of microcomputers is markedly lackingeven though there is extensive literature on the economic and econometricmodeling of mainframe and minicomputer purchase decisions [3, 5, 6, 13, 14,24].andP;  One exception is a small benchmarking study of twenty-fourmicrocomputers by Sircar and Dave [33].andP;  Since their study, the microcomputermarket has expanded significantly, leading to a proliferation of both vendorsand micros, with the result being that benchmarking is no longer acost-effective mechanism for analysis [3, 19].andM;As unit costs of hardware decline and the emphasis on microcomputers in theUnited States market rises, microcomputer-purchasing decisions will continueto be made repetitively within and across institutions.andP;  Microcomputers,especially personal computers, are having a significant impact on largeUnited States corporations.andP;  Since expenditures for microcomputers can becomea major component of the institution's MIS budget, the decision processregarding microcomputer acquisition is being taken seriously.andP;  For instance,studies of purchasing situations at the large organizational levels (e.g.,andO;Fortune 500 firms) show that companies pay close attention to cost/benefitanalysis when making hardware and software investment decisions [18, 26, 28,29, 31, 36].andM;The essential motivationfor this research is that both the research communityand the practitioner recognize the importance of using a systematic approachto the computer purchase decisions, as shown in the literature concerningthis area [3, 5-7, 9, 13-15, 17, 24, 33].andP;  In addition, past literature hasalso given rise to a continuing debate on how best to model the relationshipsbetween attributes of computers and their costs.andP;  The essence of this articleis the departure from previous research by applying the Box-Cox methodlogy[2] for choice of a model that best describes such relationships.andP;  Theadvantage of using the Box-Cox methodology is that the estimation procedurechooses the transofrmation necessary to best fit the data [35].andM;Double log, partial log, and linear or polynomial models have traditionallybeen used in similar research relating to larger computers [3, 5, 13, 15,24].andP;  Our research indicated that a double log or partial log model bestcaptured the behavior of the microcomputer hardware market.andP;  It was foundthat variables which characterize the bundles of attributes were highlysignificant in the model.andP;  The results indicate that it is the bundle ofattributes offered by a machine rather than specific technicalcharacteristics (e.g., speed) that most affects cost for micros.andM;An interesting result is that certain technological characteristics (e.g.,andO;direct access storage devices (DASD) and random access memory (RAM)), whichhave been found to be important explanatory variables for cost in the largermachines, were not significant for the micros.andP;  Thus, the purchaser isprobably most influenced by a machine's broad classification rather than itsspecific technical dimensions.andP;  As the expertise of end users/purchaserscontinues to increase in the future, however, the emphasis might shift tospecific technical characteristics rather than machine classification.andM;The purpose of our article is two-fold.andP;  First, we attempt to isolate themost significant predictor hardware characteristics from a large data set[37].andP;  Second, we present a methodological exploration to choose a model thatdescribes relationships among the significant hardware characteristics.andP;  Morespecifically, econometric models with cost as the dependent variable andvarious hardware attributes as the independent variables are studied [13].andO;The analysis is based on the historical work of Cale et al.andP;  [3], Ein-Dor[5], and others, and it extends the most recent study of Kang [13] tomicrocomputers--specifically, personal computers.andP;  The study makes amethodological contribution in that it uses the regorous, generalized Box-Coxtransformation method [2] for model selection and verification and utilizesthe models' forecasting ability as a measure of model comparison.andM;Throughout the article, we use the term microcomputer to be synonymous withthe personal computer segment, not the workstation segment of themicrocomputer market.andM;A Survey ofandM;State-of-the-ArtandM;ResearchandM;In his 1953 article Grosch [9] remarked that computers &quot;give added economyonly as the square root of the speed.&quot;andP;  This remark, known since as Grosch'sLaw, has given rise to various studies from the late 1970s to the mid 1980sthat have focused on its veracity and relevance for computers [5, 14, 24].andM;Much of the research in this area is based on linear and polynomial functionsor alternately on Cobb-Douglas Douglas type formulations of the followingform [13]:andM;[Mathematical expression omitted]andM;where Y = output (computer performance) serving as the dependent variable,[X.sub.i] = input i (computer characteristic), for i = 1, 2, .andP;  .  .  , K,[[alpha].sub.i.] = a positive or negative constant,andM;such that [Mathematical expression omitted]andM;or, in a logarithmic transformation:andM;[Mathematical expression omitted]andM;The typical dependent variable has been price, or price/MIPS, or somecomputer performance index.andP;  Independent variables include various computercharacteristics ranging from hardware attributes such as main memory to yearof introduction and IBM compatibility.andP;  In addition, some studies [6, 15]also investigate various interaction terms and polynomial terms in the modelformulation.andP;  Table III summarizes the important results of previous studies.andM;As indicated by Table III, recent research in this area has deviated fromGrosch's Law in favor of model formulations consisting of multiple hardwarecharacteristics as independent variables.andP;  Further, recent research hasemphasized the utilization of microeconomic production theory as the moreappropriate theoretical basis for work in this area [13].andP;  Except for [33],however, research relating directly to microcomputers is extremely limited.andM;Sircar and Dave [33] did a small benchmarking study of 24 microcomputermachines.andP;  Their model formulation had price as the dependent variable withindependent variables being main memory size in kilobytes, performance on twoof the five benchmark tests utilized, and various interaction terms.andP;  Theirresults yielded a high [R.sup.2] (0.9998).andP;  Benchmarking is not a feasiblealternative, however, when a large number of machines are involved (e.g., ourdata set consists of a large amount--over 300 microcomputers).andP;  Benchmarkingwould entail an exorbitant amount of time and expense [3].andP;  Additionally,Lewis and Crews [19] cite the Government Accounting Office's recommendationsthat benchmarking should not be used when system cost is under $2 million orwhen competition is limited due to high costs and other difficulties (e.g.,andO;developing representative tests) associated with benchmarking.andP;  Thus, thoughbenchmarking might be able to achieve significant predictive results, it isin general, time and cost prohibitive.andM;Theoretical JustificationandM;for Model SelectionandM;Economic theory usually does not provide the functional relationship amongvariables.andP;  This task is left to the researcher's subjective judgment [34].andO;Box and Cox [2] provide a method for removing some of the subjectivity frommodel selection and, more specifically, from data transformation.andP;  Thegeneralized Box-Cox methodology provides the researcher with a proven meansto ascertain the correct data transformation model.andP;  The method, however,does not select which variables should be in the model.andP;  Theoreticaljustification for variable inclusion is the researcher's task.andP;  Spitzer [34,p. 488] points out the key strengths of the generalized Box-Cox method:andM;(1) the transformations obtained are results of the estimation, not a priorisubjectivity;andM;(2) the transformations obtained include almost all those commonly used byeconometricians; andandM;(3) the estimation process itself generates a ranking value which candiscriminate the effectiveness of alternative models.andM;Box and Cox note that linear multiple regression analysis is typicallyjustified by assuming: (i) simplicity of structure for the expected value ofthe dependent variable--e(y); &quot;.andP;  .  .  (ii) constancy of error variance;(iii) noramlity of distributions; (iv) independence of observations&quot; [2, p.andO;211].andP;  If the data under study violate any of assumptions (i)-(iii), thennonlinear transformations of the data may improve matters (i.e., theassumptions would no longer be violated).andM;The generalized Box-Cox method involves a transformation of data through theuse of a parametric variable ([lambda]) in the following manner:andM;[mathematical Expression Omitted]andM;The parameter [lambda] is then estimated utilizing the maximum likelihoodmethod.andP;  The maximum likelihood value, Lmax, is calculated for differentvalues of [lambda] using the following equation:andM;[mathematical expression Omitted]andM;where n is the total number of observations and SS denotes the sum ofsquares.andP;  A typical set of values used for [lambda] is {+-2, +-1.5, +-1,+-2/3, +-1/2, +-1/3, +-1/4, +-1/3, 0}.andP;  The Lmax value is then plotted, andthe maximum point is determined.andP;  The [lambda] that maximizes the Lmaxfunction is the [lambda] used to transform the data in the final model.andP;  The[lambda] value at this point is then referred to as [lambda]*.andP;  (Readers withfurther interest in the generalized Box-Cox method and rationale are referredto [2], [34], and [35].)andM;Data and MethodologyandM;DataandM;Historically, Computerworld and Datapro have served as the primary datasources for econometric modeling of computer hardware characteristics.andO;During the 1980s, however, Computerworld has become the standard data source.andO;The data utilized in this study was obtained from a Computerworld article byXenakis [37].andM;The data reported by Xenakis [37] represents the informaiton supplieddirectly by over 150 different microcomputer vendors.andP;  Table IV describes thevariables used in the final model.andP;  Table V shows the mean and standarddeviations for the variables for the AT and XT portions, as well as for thepooled data set.andP;  Table VI provides information about other variablesreported on by Computerworld, but were not part of our final model.andM;For business-use microcomputers, IBM compatibility has become the standard[1].andP;  Even the Apple Computer Corporation is being affected by the newstandard.andP;  A new version of the Macintosh that will accept add-on cards andthus provide IBM compatibility is being discussed [25].andP;  It is, therefore, nosurprise that the data for microcomputers [37] exclusively spotlightsIBM-compatible micros, concentrating on IBM-XT and IBM-AT compatibles.andM;The data set consists of 336 observations with 208 observations regardingAT-compatible machines and 128 observations regarding XT-compatible machines.andO;The operating system is typically [MS-DOS.sup.TM] for both ATs and XTs.andP;  In12 of the 336 observations, [UNIX.sup.tm] is cited as a potential operatingsystem along with MS-DOS.andP;  The obvious implication is that thesemicrocomputer vendors are targeting their machines to the IBM/MS-DOScompatible market.andP;  A dummy variable, termed MSDOS, equals 1 if the machinesupports MS-DOS.andP;  MSDOS equals zero if the machine does not support MS-DOS.andO;Virtually all the observations supported MS-DOS, except for a few XTobservations, which supported PC-DOS only.andM;The ATXT dummy variable is used to flag a machine as an AT (ATXT = 1) or anXT (ATXT = 0).andP;  The ATs are technologically more powerful than XTs.andP;  Keydifferences include faster chips, more main memory, and more direct accesssecondary memory for the AT-type machines.andP;  The technological superiority ofthe ATs over the XTs is reflected by the AT's cost being $1,143 higher thanthe XT.andP;  The C80386 variable can be interpreted as an AT-plus machine.andP;  Inthe data set, the typical AT machine utilizes the C8028C chip.andP;  Thirty of theAt machines in the sample use the next generation chip, the C80386.andP;  NoXT-type machine in the sample uses this chip.andM;DUMEXP is a dummy variable that equals 1 if the machine has eight expansionslots, and zero otherwise.andP;  This representation for expansion slots waschosen because 248 machines contained eight expansion slots.andM;Intuitively, it was felt that eight expansion slots represented a standardconfiguration and that a different number of expansion slots would cause avariation from the standard, and therefore cost more.andP;  Most of the machinescould be run at more than one speed setting.andP;  SPMIN represents the lowestoperational speed in megahertz for each machine.andM;SERPORTS represents the number of serial ports supplied with each machine.andO;Serial ports are typically used to attach a modem or second printer to amicrocomputer.andP;  An examination of Table VI indicates that the average ATmachine dominates the average XT machine on all variables included in ourfinal model formulations.andM;Variable SelectionandM;The relative absence of significant previous research in the econometricmodeling of hardware characteristics in the microcomputer area makes the taskof variable selection difficult.andP;  This necessitates careful consideration andtesting for potential inclusion of all the variables in the Computerworlddata source.andP;  In this study, Pearson correlations and factor analyses wereused to generate potential interaction terms and test for potentialinclusion.andP;  The Pearson correlations showed no evidence of multicolinearityfor the selected variables in the model.andP;  Various data transformations weretried.andP;  Variables used in previous econometric models for larger machines(e.g., RAM, DASD, various formulations of speed) were examined closely fortheir impact on system cost.andP;  For example, the following model examines ninevariables, which describe the basic configuration of a machine when purchased(refer to Tables IV and VI for a description of the variables):andM;LN(PRICE) = 6.686 + 0.037 LN(ATXT) (12.58) (8.07)*andM;+0.045 LN(C80386) + (6.54)*andM;0.022 LN(SERPORTS) (6.54)*andM;--0.024 LN(DUMEXP) + (-6.80)* 0.199 LN(SPMIN) + (2.37)** 0.093 LN(RAMMIN)(1.60) + 0.060 LN(FDMIN) (1.31) - 0.015 LN(MSDOS) (-1.24) - 0.002 LN(MSDOS)(-0.43) [R.sup.2] = 0.6152  F-statistic = 57.91 (Note: t-statistics are inparentheses; *significant at the 0.01 level; ** significant at the 0.05level)andM;Finally, the variables that were most highly significant were used in ourfinal model (ATXT, C80386, SERPORTS, DUMEXP, and SPMIN) and were seen toprovide most of the explainable variation in system cost.andP;  Interestingly,some of the variables that were significant for the larger machines (DASD(i.e., HDMIN, FDMIN) and RAM (RAMMIN)) were not found to be significant forthe micros.andP;  Inclusion of additional variables or interaction terms addedlittle to the R-square but increased model complexity.andM;Models and EvaluationandM;Once the issue of variable selection was resolved satisfactorily, thegeneralized Box-Cox method was employed to ascertain the most appropriatedata transformation.andP;  A stepwise multiple regression of the untransformeddata was run, and it was analyzed for comparative purposes.andP;  The results ofthis analysis are presented in Tables VII, VIII, and IX.andP;  The double naturallog model provided best results.andP;  Table VII displays the three models withtheir associated coefficients.andP;  As noted in Table VII, Model I is the linearmodel; Model II is the double natural log model; and Model III is thetransformed model as per the generalized Box-Cox method.andP;  Models II and IIIrepresent formulations of the Cobb-Douglas type.andP;  Bartlett's test forheteroscedasticity was performed for all three models [27, pp.andP;  147-148].andO;This analysis showed that heteroscedasticity was present at the 0.5 percentsignificance level for the untransformed linear model, but was notsignificant for both the double natural log model and the Box-Cox transformedmodel.andP;  This further supports the use of data transformation, since theconstancy assumption of error variance is now satisfied.andP;  Table VIII showsthe t-statistics for the variables for each model and the coefficients ofdetermination ([R.sup.2]) for the three models.andP;  The linear model has thelowest [R.sup.2] value and t-statistics and is markedly different from ModelsII and III.andP;  The t-statistics and [R.sup.2] for Models II and III are almostidentical.andP;  The F-statistics for Models II and III as well as thet-statistics for the individual variables are highly significant, at both the0.05 and the 0.01 levels.andM;As Table VII indicates, the application of the generalized Box-Cox methodresults in a [lambda]* of 0.03.andP;  The confidence interval for [lambda] isgiven by the formula:andM;[lambda]* [+ or -] (1/2) [[chi].sup.2.(1 - [alpha]).andM;The double log model (i.e., [lambda] = 0) falls within this interval.andP;  Forthis model, the dummy variables equal to zero are approximated by a numberclose to zero (0.000001) [13, 15].andP;  The SERPORTS variable also hasobservations equal to zero, which were transformed in an identical manner.andP;  Apartial natural log model was also analyzed.andP;  Given that the partial naturallog model and the double log model are identical in terms of theircoefficient of determination and t-statistics, only the double natural logmodel is analyzed further.andP;  1  The fact that the double natural log model andthe generalized Box-Cox model (with [lambda]* of 0.03) produce very similart-statistics and [R.sup.2] is not surprising since the double natural logmodel falls within the 95 percent confidence interval.andM;Measures of ForecastingandM;AccuracyandM;We will now discuss the issue of which model, if any, provides statisticallysuperior forecasting ability.andP;  Three measures of forecasting the accuracyfrequently used to aid in the model selection and verification process arethe mean square error (MSE), the mean absolute deviation (MAD), and the meanabsolute percentage error (MAPE) [22].andP;  The MSE measure penalizes a modelmore for large errors than for small ones.andP;  MAD does not have this problem.andO;MAPE relates the magnitude of the forecasting error of the actual value.andM;Table IX presents the performance of Models I, II, and III based on thesethree accuracy measures.andP;  The linear model performs worse than the doublenatural log and the generalized Box-Cox transformed models by the MAD andMAPE measurement standards.andP;  At the same time, it performs about the same asthe double natural log model, but worse than the generalized Box-Cox model ifevaluation is based on the MSE measure.andP;  The performances of the doublenatural log model and the generalized Box-Cox model are extremely close.andP;  Thegeneralized Box-Cox performs slightly better on the MSE measure whileperforming marginally worse on the MAD and MAPE measures.andP;  All three modelshave relatively large MADs and MAPEs.andP;  The standard deviation for cost is$1,133, which is approximately 65 percent of the mean for a cost of $1,751(see Table V).andP;  MADs around $510 and MAPEs around 31 percent representsignificant improvements in contrast to the large standard deviation.andP;  Thenext section of the article will discuss the implications that can be drawnfrom the empirical results of this research.andM;Analysis andandM;ImplicationsandM;Model AnalysisandM;The Box-Cox model formulation can be used as a screening mechanism foridentifying microcomputer alternatives that would be worthy of in-depthconsideration by a decision maker.andP;  The statistics (t-values, F-values, andthe coefficients of determination) of the generalized Box-Cox and doublenatural log models are extremely similar.andP;  The double natural log model,however, is computationally less complex; therefore, it may be preferable andeasier for a decision maker (practitioner) to use.andM;Our models identify five variables that have a highly significant impact onthe cost of microcomputers.andP;  Table X presents the effect by variable for thedouble natural log model.andP;  The variables, in order from most significant toleast, are C80386, ATXT, DUMEXP, SPMIN, and SERPORTS.andP;  Specifically, thenegative (-0.628) effect for C80386 implies that the cost of a givenmicrocomputer is significantly less if the C80386 chip is not present.andP;  TheATXT variable has the second largest effect of the five variables.andP;  Asmentioned in the DATA section, the ATXT dummy variable differentiates betweenthe XT-type and AT-type micros while the C80386 variable differentiatesbetween the standard AT-type micros and the AT-plus type micros.andO;Intuitively, we expect that these two variables would explain a large amountof the cost for a given machine.andP;  In other words, in general, the threecategories (i.e., XT, AT, or AT-plus) of microcomputers fall into distinctlydifferent price ranges.andP;  The results show that the cost difference between anAT and AT-plus is greater than the cost difference between an XT and AT(i.e., -0.570 versus -0.628).andM;The variable with the next greatest impact on cost is DUMEXP.andP;  The numbersindicate that micros with a nonstandard configuration of expansion slots aregoing to cost significantly more than those with the standard configurationof eight expansion slots.andP;  It should be noted that the vast majority ofnonstandard expansion slot configurations contain less than eight expansionslots as opposed to more than eight expansion slots.andP;  Thus, the apparentimplication is that vendors who use and/or buy the standard configurationsave money and pass savings on to the consumer.andM;SPMIN is the next most important varaible in the model.andP;  The values for SPMINnoted in Table X match with micro types as follows: 4.77 megaherts for XT; 6,8, 10 for AT; 16 for AT-plus.andP;  As noted in the DATA section most of themicros had multiple speed settings.andP;  For example, an aT-plus micro might havespeed settings of 4.77, 6, 8, 10, 12, and 16.andP;  Thus the correlations arerough yet representative estimates.andP;  For example, most AT and AT-plus microsdo not have a 4.77 setting while most XT micros do.andP;  Nonetheless, thedifference in terms of cost between the highest SPMIN of 16 and lowest SPMINof 4.77 is 0.211 (i.e., 0.522-0.311).andP;  Also, the difference between the meanSPMIN for AT (including AT-pluses) and XT-type micros is only 0.87 (i.e.,andO;0.426-0.399).andP;  In general, the variables previously discussed have a muchlarger impact on cost than SPMIN.andM;The final variable in the model, SERPORTS, has the lowest impact on cost ofthe variables in the model.andP;  Based on our own experience with micros, we feelthat serial ports frequently go unused.andP;  This may explain their small effecton cost.andM;In summary, the variables that differentiate machine types (i.e., ATXT,C80386) and the variable related to standardization of expansion slots (i.e.,andO;DUMEXP) have the biggest effect on cost.andP;  These results are not totallyunexpected.andP;  We were surprised, however, that SPMIN (and other measures ofspeed tried but found to be lacking in significance) did not have a biggerrole in the final model.andP;  This seems to imply that it is the bundle ofattributes offered by a machine and not simply the speed that most effectscost for micros.andP;  An explanation for this may be the high number of competingvendors in the personal computer segment of the microcomputer market.andO;Individual vendors must first position their machines as AT- or XT-typemachines in order to attract the attention of purchasers looking for anIBM-compatible machine.andP;  After this broad classification, the vendors thencan attempt to differentiate their machines based on specific technicalcharacteristics.andM;In comparison, the numbers of vendors offering mainframes or minicomputers isquite small, and differences initially based on technical characteristicsmight be a viable strategy for a vendor.andP;  Thus, specific technical dimensionslike speed or main memory are probably not as important as a general measureof a micros's capabilities.andP;  To the purchaser, a general measure of a micro'scapability is best captured by machine-type classification.andP;  This may wellchange in the future as purchasers become more sophisticated in terms ofmicros.andP;  Also fromt he vendor perspective, once a decision is made to build agiven type of machine many of the tec hnical dimensions affecting cost areprobably beyond the vendor's control.andP;  The components to make the machinemust be bought on the open market from suppleirs.andP;  Thus, productdiferentiation and cost differentiation may depend more on marketingtechniques than on technical considerations.andM;Grosch's LawandM;The validity of Grosch's Law was also analyzed per a model formalationsimilar to Mendelson's [24].andP;  Meldelson's work supported the existence ofeconomies of scale for speed for microcomputers, but not for the largerclasses of machines.andP;  The model results for our Computerworld data set wereas follows (t-statiscs are in parentheses):andM;In (PRICE/SPMIN) =5.609-0.126 In SPMIN) (37.65) (-1.65) [R.sub.2]=&quot;0.008.andM;The coefficient for SPMIN is not statistically significant at even the 90percent level.andP;  Like Mendelson's findings for the larger machines, theseresults strongly suggest a constant value for In(SPMIN), aside from randomnoise, for the microcomputer class.andP;  The results support the constant returnsto scale model and do not support Grosch's Law.andM;Limitations and FutureandM;ResearchandM;The F-statistic for the final model and the T-statistics for the final fivevariables are highly significant, suggesting that the variables consideredstrongly influence cost.andP;  The high MADs, MAPEs, MSEs, and [R.sup.2] value of0.60, however, indicate that varibles not captured in the data set need to bemeasured and included in future models (e.g., price discounts, number ofunits sold, and cost of labor).andP;  This means that there are other significantvariables that affect cost, which are not included in the model.andP;  the C80386and ATXT dummy viariables explain a large amount of the variation in cost fora given machine.andM;The logical question that arises from these results is whether the magnitudeof cost differneces can be used as aproxy for making statements about themagnitufe of the technological differences among the three types of micros.andO;Unfortunately, our present research does not allow us to make such adeduction.andP;  Further, given that the limited number of AT- plus observations(i.e., 30) and that the C80386 was just beginning to the utilized when thisdata set was collected, it would be prudent to collect and analyze a data setwith more equal representations of the three machine types.andM;One variable that might well improve the model would be the year ofintroduction for each machine [3, 13].andP;  Given the number of vendors involvedand the problem of ascertaining reliable year of introduction dates, wasdecided to leave this variable out of the model.andP;  Additionally, it can bereasonably assumed that most of the mach ines covered were upgraded withintwo years of the data collection point.andP;  A larger data set covering severalyears would be ideal for this type of technological change analysis.andP;  Otherimportant variables might be vendor specific:andM;(1) number of units manufactured/soldandM;(2) target user market (e.g., sophisticated hobbyist, unsophisticatedfirst-time buyer, businesses, etc.)andM;(3) mode of distribution (e.g., mail-order, vendor-owned stores, etc.)andM;(4) level of after-sale support givenandM;(4) level of after-sale support givenandM;(5) geographical marketandM;(6) diversity of product lineandM;(7) price-discounting practicesandM;(8) production costs (e.g., labor)andM;Two other recent technological innovations which have the potential of havinga high impact on cost are the following: (1) [OS/2.sup.tm] and Unix aspotential operating systems, thus allowing multitasking and multiuser systems[8, 38], and (2) local area network (LANs), which allow a networking of manycomputers in order to share hardware and software resources [32].andP;  Francis[8] points out the Unix runs on multitasking computers in a multiaskingenvironment, while OS/2 performs multiasking ona single-user microcomputer orworrkstation in a standalone or networked mode.andP;  In 1987, the OS/2 operatingsystem entered the market place.andP;  Nontheless MS-DOS continues to be theprimary operating system for microcomputers.andP;  As Zachary [38] notes,Microsoft is set to sell more than 12 million copies of MS-DOS in 1989 alone,as compared to only a few hundred thousand copies of OS/2.andP;  OS/2 ispenetrating the microcomputer market at a rate much slower than expected.andP;  Atthis point, it remains to be seen if OS/2 will become the successor to MS-DOSas the primary operating system for micros.andM;The second technological innovation, networking, reduces hardware andsoftware costs by facilitating the sharing of software and hardwareresources.andP;  Edwards [4], however, notes that 80 percent of PCs are stilloperating completely outside of networks.andP;  Further, those PCs that are on anetwork are more likely to be on a mainframe or minicomputer-based networkwhere the PCs are acting as terminal emulators running host-based software,as opposed to being intelligent terminals.andP;  Though still in the infancystage, peer-to-peer protocol-based LANs are seen as having a significantimpact on future corporate computing practices, by allowing symbioticrelationships between hosts and multiple pcs through new cooperative software[4, 30].andP;  The communication capability of microcomputers under evaluation isan important selection criterion to consider and will be even more importantin the future [30].andM;Future research should attempt to capture these variables to the highestextent possible.andP;  In this regard, we expect the generalized Box-Coxparametric transformation to become a more fruitful methodology.andP;  Thisarticle has already made a useful methodlogical contribution on the issue ofthe best way to model the relationships between computers' attributes andcost--a debate of the last decade.andP;  It is our belief that the generalizedBox-Cox transformation process is a methodological approach more appealingthan the traditional multiple regression analysis commonly used in previousresearch.andM;ConclusionsandM;The application of the rigorous, generalized Box-Cox method during the modelfoundation, testing, and selection stage provides emperical justification (orrejection) of the researcher's model.andP;  The generalized Box-Cox method can beparticularly helpful when there is no previous ground work to suggest whichtypes of models might be examined [34].andP;  Additionally, when models arecompared for forecasting superiority, several measures of forecastingaccuracy should be examined.andP;  The MSE, MAD, and MAPE measures utilized inthis article are well known and accepted for this purpose.andM;Our analysis of the XT and AT IBM microcomputer market has shown that machineclassificatiion is the most important factor in determining cost.andP;  Further,although the popular press argues that the microcompuer market is becoming acommodity market, the data shows a surprisingly wide variation in cost fortechnologically similar machines.andP;  This is probably explained by twofactors--one, a wide variation in expertise among purchasers; and two,purchasers are buying more than technology (e.g., future vendor support).andO;Additionally, the result indicate that, in terms of speed, for microcomputersconstant returns to scale cannot be rejected in favor of economies of scaleas suggested by Grosch's Law.andP;  This finding also contradicts Mendelson'spreliminary work.andP;  In addition, Kang [15] noted that during the first half ofthe 1980s both purchase prices and lease prices for larger computers declinedat rates exceeding 25 percent annually, the implication being that excesscapacity should be minimized.andP;  These findings--no apparent economies of scaleand rapidly declining costs--would seem to imply indifference tocentralization versus decentralization of the MIS function and favor theutilization of microcomputers.andP;  Future research should focus on whether thesefactors are influencing structural decisions within organizations and which,possibly noneconomic, factors are becoming dominant.andM;The Computerworld [37] article notes that corporate purchasers are becomingmore and more likely to buy non-IBM machines due to the advantageousprice/performance ratios in comparison to an IBM machine.andP;  Over time, thistrend will likely intensify as more decision makers develop knowledge of themicrocomputer market.andP;  They will realize that Big-Blue (IBM) is not the onlylegitimate alternative for microcomputers.andP;  Verity and Lewis [21] arguepersuasively that differences among microcomputer brand names are blurring,making price differentiation a critical marketing strategy.andP;  Thus, expansionof the model to include all microcomputer architectures (in addition to theIBM-compatible ones used here) would be a useful direction for future work.andM;AcknowledgmentsandM;We would like to thank Dr. Vijay Sethi and Dr. R. Ramesh of the StateUniversity of New York at Buffalo for valuable comments.andP;  Helpful suggestionsand comments from Dr. Young Moo Kang of the Owen Graduate School ofManagement at Vanderbilt University and Dr. Roger Alan Pick of the School ofBusiness at the University of Cincinnati, during the early part of theproject, are deeply appreciated.andP;  We would also like to thank the anonymousreferees for suggestions that have increased the lucidity of the paper andProfessor Ed Sibley for his encouragement.andM;(1) Ein-Dor [6] notes that different choices of a number close to zero canhave a significant effect on a model's coefficients.andP;  However, Ein-Dor alsonotes that the choice has no significant effect on a model's [R.sup.2] ort-statistics.andP;  The t-statistics and [R.sup.2] obtained for the partialnatural log model are identical to the double natural log model through themodels have different coefficients.andP;  Thus, our work supports Ein-Dor'sconclusions.andM;ReferencesandM;[1] Are personal computers a commodity market?andP;  Purchas.andP;  101, 2 (July 24,1986), 78-95.andM;[2] Box, G.E.P., and Cox, D.R.andP;  An analysis of transportations.andP;  J. RoyalStat.andP;  Soc., Series B, 26, 2(Apr.andP;  1964), 211-243.andM;[3] Clae, E.G., Gremillion, L.L., and McKenney, J.L.andP;  Price/performancepatterns of U.S.andP;  computer systems.andP;  Commun.andP;  ACM 22, 4 (Apr.andP;  1979),225-232.andM;[4] Edwards, J.L.andP;  PCs on LANs: Dumb terminals or application team players.andO;Data Commun.andP;  17, (Aug.andP;  1988), 125-130.andM;[5] Ein-Dor, P. Grosch's law re-revisited: CPU power and the cost ofcomputation.andP;  Commun.andP;  ACM 28, 2 (Feb.andP;  1985), 142-151.andM;[6] Ein-Dor, P., and Feldmesser, J. Attributes of the performance of centralprocessing units: A relative performance prediction model.andP;  Commun.andP;  ACM 30,4 (Apr.andP;  1987), 308-317.andM;[7] Ein-Dor, P., and Feldmesser, J. Authors' response to comments on&quot;Attributes of the performance of central processing units: A relativeperformance prediction model.&quot;andP;  Commun.andP;  ACM 32, 2 (Feb.andP;  1989), 259-261.andM;[8] Francis, B. Desktop yug of war: OS/2 versus Unix.andP;  Datamation 35, 4 (Feb.andO;15, 1989), 18-25.andM;[9] Grosch, H.A.andP;  High-speed arithmetic: The digital computer as a researchtool.andP;  J. Opt.andP;  Soc.andP;  Amer.andP;  43, 4 (Apr.andP;  1953), 306-310.andM;[10] Henkel, T. Hardware roundup.andP;  Computerworld 15, 26 (Jul.andP;  13, 1981),11-19.andM;[11] Henkel, T. Hardware roundup.andP;  Computerworld 19, 33 (Aug.andP;  19, 1985),22-37.andM;[12] Hillkirk, J. Workstations, PCs gain popularity.andP;  USA Today, (April 26,1989), B1-B2.andM;[13] Kang, Y.M.andP;  Computer hardware performance: Production and cost functionanalysis.andP;  Commun.andP;  ACM 32, 5 (May 1989), 586-593.andM;[14] Kang, Y.M., Miller, R.B., and Pick, R.A.andP;  Comment on &quot;Grosch's lawre-revisited: CPU power and the cost of computation,&quot;  Commun.andP;  ACM 29, 8(Aug.andP;  1986) 779-781.andM;[15] Kang, Y.M., and Pick R.A.andP;  Comments on &quot;Attributes of the performance ofcentral processing units: A relative performance prediction model.&quot;andP;  Commun.andO;ACM 32, 2 (Feb.andP;  1989), 256-259.andM;[16] King, J.L.andP;  Centralized versus decentralized computing: Organizationalconsiderations and management options.andP;  Comp.andP;  Surv.andP;  15, 4 (Dec.andP;  1983),319-349.andM;[17] Laudon, K.C., and Laudon, J.P.andP;  Management Information Systems: AContemporary Perspective.andP;  Macmillan, New York, 1988.andM;[18] Leisner, R.M.andP;  Preparing your firm for the 21st century--How are wedoing it.andP;  Legal Econ.andP;  14, 7 (Oct.andP;  1988), 33-45.andM;[19] Lewis, B.C., and Crews, A.E.andP;  The evolution of benchmarking as acomputer performance evaluation technique, Manage.andP;  Inf.andP;  Syst.andP;  Q. 9, 1(March 1985), 7-15.andM;[20] Lewis, G., Field, R.F., and Hafner, K.M.andP;  Zoom!andP;  Here come the newmicros.andP;  Bus.andP;  Week 2971-2979, 2975 (Dec.andP;  1, 1986), 82-92.andM;[21] Lewis, G., and Verify, J.W.andP;  Computers: The new look.andP;  Bus.andP;  Week3024-3032, 3028 (Nov.andP;  30, 1987), 112-123.andM;[22] Lin, W. Modeling and forecasting hospital patient movements: Univariateand multiple time series approaches.andP;  Int.andP;  J. Forecasting 5, 2 (June 1989),195-208.andM;[23] Long, L. Management Information Systems.andP;  Prentice-Hall, EnglewoodCliff, N.J., 1989.andM;[24] Mendelson, H.andP;  Economies of scale in computing: Grosch's law revisited.andO;Commun.andP;  ACM 30, 12 (Dec.andP;  1987), 1066-1072.andM;[25] Meth, C. Buyers' guide to personal computers, Admin.andP;  Manage.andP;  48, 2(Feb.andP;  1987), 27-31.andM;[26] Pastore, R. PC investments: Rent or Buy?andP;  Computerworld 23, 14 (Apr.andP;  3,1989), 91.andM;[27] Pindyck, R.S., and Rubinfeld, D.L.andP;  Econometric Models andamp; EconomicForecasts.andP;  2d ed.andP;  McGraw-hill, New York, 1981.andM;[28] Qualls, J.H.andP;  The PC Corner, Bus.andP;  Econ.andP;  24, 2 (Apr.andP;  1989), 53-54.andM;[29] Reichert, A.K., Moore, J.S., and Byler, E. Financial analysis amonglarge U.S.andP;  corporations: Recent trends and the impact of the personalcomputer.andP;  J. Bus.andP;  Financ.andP;  Acc.andP;  (UK) 15, 4 (Winter 1988), 469-485.andM;[30] Reifsnyder, J.andP;  Think ahead in buying a personal computer.andP;  Office 107,4 (Apr.andP;  1988), 54-55.andM;[31] Remmlinger, E., and Waldmann, R.G.andP;  Avoid mistakes when implementingmicrocomputer systems.andP;  Healthcare Financ.andP;  Manage.andP;  42, 10 (Oct.andP;  1988),90-94.andM;[32] Shimman, D. Developments in PC communications.andP;  Telecommun.andO;(International Edition) 23, (Jan.andP;  1989), 47-50.andM;[33] Sircar, S., and Dave, D.andP;  The relationship between benchmark tests andmicrocomputer price.andP;  Comun.andP;  ACM 29, 3 (Mar.andP;  1986), 212-217.andM;[34] Spitzer, J.J.andP;  A Monte Carlo investigation of the Box-Cox transformationin small samples.andP;  J. Amer.andP;  Stat.andP;  Assoc.andP;  73, 363 (Sept.andP;  1978), 488-495.andM;[35] Spitzer, J.J.andP;  A fast and efficient algorithm for the estimation ofparameters in models with the Box-Cox transformation.andP;  J. Amer.andP;  Stat.andP;  Soc.andO;77, 380 (Dec.andP;  1982), 760-766.andM;[36] Wolfe, C., and Vigor, R.E.andP;  Microcomputer technology: Changingstandards.andP;  J. Acc.andP;  167, 4 (Apr.andP;  1980), 52-61.andM;[37] Xenakis, J. Spotlight: Line shifts in PC field.andP;  Computerworld (May 25,1987), S1-S24.andM;[38] Zachary, G.P.andP;  Software makers get a chill from Microsoft's Windows.andO;Wall Street J., (October 10, 1989), BU andamp; B4.andM;CR Categories and Subject Descriptors: K.1 [Computing Milieux]: The Computerindustry--statistics; K.6.0 [Computing Milieux]: Management of ComputingInformation Systems--economicsandM;General Terms: EconomicsandM;Additional Keywords and Phrases: Box-Cox tranformation, computercharacteristics, cost function, economies of scale, Grosch's Law,microcomputers, microeconomics, personal computers.andM;BRIAN LYNCH is a doctoral candidate in management information systems at TheState University of New York at Buffalo.andP;  His research interests include theimpact of information technologies on job characteristics.andM;H.andP;  RAGHAV RAO, the principal author, is an assistant professor of MIS atSUNY Buffalo.andP;  His research interests are information economics and theintegration of decision support systems and expert systems.andM;W.andP;  T. LIN is a associate professor of management science at SUNY Buffalo.andO;His research interests are in applied econometrics and statistics.andO;</TEXT></DOC>