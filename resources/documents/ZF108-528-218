<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-528-218  </DOCNO><DOCID>08 528 218.andM;</DOCID><JOURNAL>Software Magazine  May 1990 v10 n6 p64(9)* Full Text COPYRIGHT Sentry Publishing Company Inc. 1990.andM;</JOURNAL><TITLE>Follow these 13 rules to make DB2 run faster. (includes relatedarticle on DB2 usage) (tutorial)</TITLE><AUTHOR>Montgomery, Stephen L.andM;</AUTHOR><SUMMARY>Thirteen rules facilitate the design of IBM DB2 relational database management system (RDBMS) applications for optimumperformance.andP;  DB2 is a powerful and flexible RDBMS for mainframecomputers, but DB2 applications may not perform as expected due tobad design and poorly tuned DB2 systems.andP;  There are several DB2data base design tools that help facilitate good data base design,but these do not obviate adherence to the 13 rules.andP;  The rules arecarefully select table access mechanisms, tune sequential scanprocessing, limit the amount of data searched, store multipletables in a single table space, optimize the size and number ofdata buffers, use high-speed disk drives, use sequential prefetch,carefully use table clustering, use frequent COMMIT commands,segment tables to be updated, and consider the use of insert anddelete processing, checkpoint/restart processing, and utilityprocessing.andM;</SUMMARY><DESCRIPT>Ticker:    IBM.andO;Product:   DB-2 (Data base management system) (programming).andO;Topic:     Relational DBMSApplications ProgrammingTutorialProgramming InstructionPerformance ImprovementOptimizationAdd-In-On Software.andO;Feature:   illustrationcharttable.andO;Caption:   Data bases' market shares: information center and productionenvironments. (chart)Representative DB2 tools. (table)andM;</DESCRIPT><TEXT>FOLLOW THESE 13 RULES TO MAKE DB2 RUN FASTERandM;DB2 is a powerful and flexible database management system.andP;  It is quicklybecoming the standard system for new mainframe applications.andP;  However, asanyone who makes heavy use of DB2 knows, there are times when applicationsbuilt with DB2 do not perform as well as expected.andP;  most often performanceproblems can be traced to bad database designs and poorly tuned DB2 systems.andM;Version 2 of DB2 brought two major improvements: greater performance andreferential integrity.andP;  (Referential integrity ensures certain types ofconsistency between entries in different database tables).andM;Queries involving sorts are more efficient in version 2.andP;  This affectsprocessing of multiple table joins and SQL GROUP BY and ORDER BY clauses.andO;Improvements can be seen in locking, the Optimizer's choice of access paths,use of indexes and management of disk writes.andP;  Still, designers anddevelopers will experience problems in some situations that require tuning.andM;Referential integrity offers ease in programming application database calllogic, but now DB2 must handle referential constraint checking.andP;  In manycases, this should lead to higher performance than applicationprogram-controlled referential integrity processing.andP;  DB2 referentialintegrity avoids the need to code multiple application program routines tohandle referential constraints.andM;Nothing in the relational database model itself should limit the performanceof production systems.andP;  The relational model is a logical concept,independent of performance concerns.andM;DATABASE DESIGN TOOLSandM;Developers should begin considering database performance issues when theystart to design storage groups, databases, table spaces, tables, columns andindexes.andP;  If the databases are not designed correctly, performance can sufferand redesign can be expensive.andM;Database design tools include the Bachman Data Base Analyst for DB2 fromBachman Information Systems, Burlington, Mass.; Excelerator/DB2 from IndexTechnolgy, Cambridge, Mass.; and the IEW Design Workstation fromKnowledgeWare, Atlanta.andM;Each of these tools helps to translate an entity-relationship data model intoa physical DB2 design.andP;  An additional benefit of good database design is thatwriting applications and queries becomes easier.andP;  Data managementprofessionals should take the following 13 rules into consideration whendesigning DB2 applications:andM;1.andP;  Carefully Select Access Mechanisms: DB2's options for table accessing andassociated application programming should be carefully selected.andM;SQL does not directly allow explicit requests for certain data access pathswith DB2, only what data to acess.andP;  The DB2 Optimizer chooses access pathsand guesses what the best path should be, based on database statistics storedin the DB2 catalog.andP;  STill, a database designer can determine whichmechanisms are available to the Optimizer in choosing the best access path.andM;2.andP;  Tune Sequential Scan Processing: A sequential scan of DB2 tables mustcheck many rows for ones that satisfy an SQL request.andP;  Sequential scans canbe inefficient when many ros are inspected, and only a small portion of theserows fulfill the selection criteria.andM;Such scans can be efficient in these situations:andM;* when searching small tables;andM;* when a high percentage of rows will be selected; andandM;* when index or other access methods are too costly.andM;Scan cost factors include the number of rows accessed the number of I/Ooperations and the type of data request executed.andP;  Some SQL requests willperform better than others, so some experimentations with various forms ofSQL requests (such as joins versus subqueries) might help.andM;Database setup factors, such as blocking of table rows, can make sequentialscanning very efficient.andP;  Sequential scans can be appropriate forlow-priority requests such as overnight batch jobs.andM;Even if other access methods are too costly to consider, database designersshould try to limit the number of rows searched or find other ways toincrease sequential scan processing speed.andM;3.andP;  Limit the Amount of Data Search: Limiting the amount of data searched canhelp improve performance.andP;  Therefore, increasing the proportion of rowsretrieved from the total number of rows searched is an important goal.andM;Clustering rows in a certain order within a table can allow DB2 to beginscanning where the requested values begin, and end searching as soon as thelast qualifying have been passed up.andM;Using table space partitions to store only part of a table in a table spacecan be useful in certain situations.andP;  A table can be split into severalseparate table spaces based on certain column values: a CUSTOMER table couldbe partitioned by values of the CUSTOMER_ID column.andP;  DB2 might be able tolimit a sequential scan to only one of several partitions based on columnvalues referenced in an SQL WHERE clause.andM;4.andP;  Store Multiple Tables Per Table Space: Storing more than one logical DB2table in a single table space is another way to increase performance.andP;  Thiscan be done to increase the number of rows scanned for each row returned inan SQL request.andM;However, this technique could cause locking problems if several concurrentrequests make use of the same table space.andP;  Interleaving rows from separatetables might enhance join processing.andP;  If rows are logically related, thisinterleaving could improve performance of joins of the related rows.andM;5.andP;  Size and Number of Buffers: I/O processing can often become a bottleneck,and careful choice of data buffers can often reduce the number of physicalI/O operations performed.andM;When DB2 tries to access a row twice, and the row is already in a buffer thesecond time the acess is performed, a second physical I/O operation isavoided, improving performance.andP;  Sequential scans can dramatically improve ifmost or all of the desired rows are already stored in a buffer.andP;  DB2'ssequential prefetch uses this concept, but good choice of the number and sizeof buffers can definitely enhance performance.andM;6.andP;  Use High-Speed Disk Drives: High-speed disk drives also make a differencein database performance.andP;  Obviously, faster disk storage devices can quickendata access.andP;  Frequently scanned data should be placed on faster disks.andM;However, developers should assess which data items are accessed concurrentlyby different requests.andP;  Placing such items on one storage device might not bea good idea if multiple concurrent accesses end up waiting for each other.andM;7.andP;  Use Sequential Prefetch: DB2 uses a normal scanning speed for handlingmost data requests, but a higher-speed scan is used when large sequentialblocks of data are scanned.andP;  DB2 retrieves one block of data per I/O requestfor normal scanning and many physical blocks with each I/O request when usingsequential prefetch.andP;  A database administrator can determine when the fastersequential prefetch is appropriate.andM;8.andP;  Exercise Care in Table Cluster: Clustering involves storing table rows ina predetermined order, based on values of one or more key columns of a table,a technique that can improve data access times.andP;  Clustering can reduce oreliminate table row sorting because the rows can be stored in a predeterminedsequence.andP;  Sequential scans in the order of clustering are much faster, andsequential retrieval of a range of values based on a clustering column alsobenefit.andM;Normally, SQL requests do not cause rows to be ordered unless the ORDER BYclause is used.andP;  For unordered rows, DB2 must retrieve all rows requested,sort the rows, put them in the proper order and present the results to theprogram or user requesting them.andP;  Proper clustering can avoid the sortingoverhead (as will an index).andM;Developers should consider clustering every DB2 table, although very smalltables might not benefit much from clustering.andP;  The clustering order chosenshould match the most critical or frequent data requests.andP;  To keep tables inproper clustering order, they will have to be periodically reorganized usingthe DB2 REORG utility.andM;9.andP;  Use Frequent COMMIT Commands: Normally, developers should issue frequentCOMMIT commands in their transaction processing programs to quickly releaselocks and thus free up table resources for subsequent table accesses.andO;Frequent commits of transaction data are important to avoid deadlocks.andP;  Tominimize locking problems, small lock granularity, clustering, partitioningand table segmenting can be used.andM;10.andP;  Segment Tables to be Updated: If a database has large numbers of batchupdates, they should be segmented into smaller groups, allowing each groupaccess to different partitions, segments or ranges of key values.andP;  Customertransactions could be segmented or partitioned by ranges of CUSTOMER_IDvalues.andP;  Multiple batch jobs could access a customer table concurrently byaccessing different ranges of key values.andP;  This technique can be useful evenif tables are not physically segmented or partitioned due to the reducedlocking of table pages.andM;11.andP;  Consider Insert and Delete Processing: If database transactions update alarge volume of table rows (especially long rows), the following factorsmight cause poor performance: logging activity, locking, commit processingand index maintenance.andM;To insert a large number of rows, the DB2 LOAD command can be used instead ofan SQL Update statement for each row.andP;  Also, logging can be disabled duringload operations to avoid unnecessary logging overhead.andM;Dropping indexes before mass insert or update operations and recreating themafterward is a way to avoid index maintenance overhead for update of veryhigh data volumes.andP;  Another alternative is to create table partitions so thatmass delete operations become partition drops and reloads.andP;  However, the costto rebuild indexes that span partitions must be factored into the decision.andM;According to Jim Cunningham, technology manager in the databaseadministration area for Ameritech Applied Technologies, Milwaukee, Wis., &quot;Inthe design of your DB2 tables, it is very important to get the right kind oflocks.andP;  It takes more CPU overhead to lock pages than to lock tables.andP;  Ifyour users are just reading tables, it is better not to incur the extraoverhead associated with locking the tables.&quot;andM;Ameritech's systems programmers would feed information back to the databasegroup on whether an application is tying up the CPU excessively.andP;  &quot;Thesystems people give us that information,&quot; said Cunningham.andP;  If the systemsprogrammers identify a poorly performing program, &quot;We look into it furtherand find out what's going on,&quot; he said.andM;12.andP;  Examine Checkpoint/Restart Processing: When data requests involve largeamounts of data, developers should carefully examine the effect of potentialtransaction rollbacks and the resulting restart required.andP;  SQL does notcommit work to a database until the end of a transaction.andP;  If transactionsfail before the commit point, restarting from the beginning may be necessary,resulting in a lot of overhead.andM;One solution is to break up time-consuming SQL statements into multiplestatements and store intermediate results in a table.andP;  This involves moreprocessing for successful transactions but saves time for abortedtransactions.andM;13.andP;  Consider Utility Processing Options: If DB2 tables are very large, theycan be partitioned or segmented into smaller pieces to facilitate shorter andconcurrent processing of: RUNSTATS, REORG, backups and recoveries.andM;Developers can alternate full and incremental backups and perform utilitieson separate table partitions.andP;  Some utilities lock an entire table duringprocessing, causing extra contention for DB2 catalog tables.andM;Database system performance depends on good database design.andP;  Storage groups,database, table spaces, tables, columns and indexes should all be carefullyconstructed.andM;RUNSTATS, STOSPACE, DB2 Performance Monitor and other tools are available tohelp with database tuning.andM;Thread use, locking, dynamic and static SQL and cursor processing should beconsidered during application design.andP;  Other considerations includeaggregation statements, views, data definition and indexes.andM;Applications can be tuned with devices such as Explain, accountinginformation, performance traces and monitoring tools.andP;  There are severaltools available to make DB2 design and tuning easier.andM;Ameritech's Cunningham emphasized use of the Explain facility.andP;  &quot;Explain isan important design consideration,&quot; he said.andP;  His group can delete indexesnot being used by end users, based on information provided by the Explaintool.andM;In Cunningham's view, independent software suppliers have an opportunity toimprove on IBM's Explain facility.andP;  &quot;I don't see too much of it being done,&quot;he said.andM;DB2 PERFORMANCE TOOLSandM;Once a performance problem is detected, the cause must be determined.andO;Performance problems can stem from:andM;* transaction delays,andM;* transaction I/O operations,andM;* CPU consumption, andandM;* the Optimizer.andM;User delays can result from applications handling aborted transactionsincorrectly.andP;  Decisions must be made to either abort or commit pendingtransactions when the transaction process exits.andP;  Neither option may be rightfor a given process.andP;  Problems will result when a transaction is waiting forlocks held by another process that is not handling aborted transactionsproperly or is otherwise running inefficiently.andM;CPU consumption can be difficult to detect at times because experience tellsprogrammers to expect this processing to increase when an SQL statementbecomes more complex.andP;  Varying the form of SQL statements and rerunning arequest is one option, but this can require a high degree of expertise.andM;Disk I/O can significantly affect database processing times.andP;  The amount ofdisk I/O required for a process can be estimated based on the distributionand volume of the data being affected by a particular SQL request.andP;  Whenusing indexes, the disk I/O required, as well as data and distribution, mustbe considered.andP;  Excessive disk I/O is often much more sensitive to the amountof data processed than excessive CPU consumption.andP;  Tools such as IBM's DB2Performance Monitor can be of great help here.andM;The DB2 Optimizer sometimes can be at fault when disk I/O and CPU consumptionare high.andP;  It often takes a high level of experience with DB2 and theapplication environment to make rough estimates of disk I/O and CPUconsumption in order to make appropriate query strategies work well.andO;Sometimes more than one query strategy will work, but each one will need tobe tested to determine what works best in a particular application situation.andM;Many of Ameritech's problems with DB2 relate to timeout mechanisms, which mayafter a predetermined elapsed time, instruct a DB2 transaction to abend.andP;  Thetimeout interval requires a coordination between DB2 and IBM's Data FacilityHierarchical Storage Manager (DFHSM).andP;  Batch-oriented software originallydeveloped for IBM's 3850 jukebox-type storage device introduced in the 1970sand withdrawn in 1987.andP;  DFHSM is an IBM product for managing data migrationand recall.andP;  It is considered a crucial product in IBM's System ManagedStorage strategy, which is IBM's approach to data management.andM;&quot;A lot of our problems relate to HSM for timing considerations.andP;  As far as Iam concerned, DB2 and HSM are incompatible at this point,&quot; Cunningham said.andO;&quot;I would not recommend that DB2 datasets be put out on HSM.&quot;andM;Ameritech gets around the issue by storing data on &quot;live disks,&quot; he said.andO;When timeout occurs and a DB2 transaction abends, users get an artificialmessage to the effect that the data was not available.andP;  Many times, if theuser makes a second attempt, the transaction goes through.andP;  &quot;It's more of afrustration than anything else,&quot; Cunningham said.andM;Because application design and tuning can be complex, there are times whenDB2 tools can be of help.andP;  DB2 database design should include estimates ofgrowth rates and distribution of growth.andP;  This can be helpful in schedulingDB2 utilities for recovery and reorganization.andP;  However, just depending onfuture estimates might not be good enough, and operations time, computer costand application downtime can be adversely affected.andM;Rules are needed for governing the maximum times that are allowed for normalrecovery, disaster recovery, image copying and reorganizing.andP;  Knowledge abouttape management and job scheduling systems can be used to automate databaseobject maintenance.andP;  DB2 tools can help with these tasks.andM;One tool, a DASD manager, analyzes DB2 data sets and tries to identifyproblems that reduce database access efficiency or prevent access to data.andO;Application migrators can assist in moving application objects fromdevelopment to test and production in a highly controlled fashion.andM;IBM's solution in this area is Software Configuration Library Manager (SCLM),a component of ISPF/PDF, which is IBM's mainframe screen managementapplication development facility.andP;  It is roughly analogous to PresentationManager within OS/2 Extended Edition.andM;Migrator tools may recreate databases, table spaces, tables, views, synonymsand Vsam Clist control language.andP;  They can also create authorities and copydata where necessary.andM;Since DB2 applications typically consume a lot of disk storage, many toolshave been developed to aid in managing this storage.andP;  These tools help createobjects of the appropriate size by asking how many rows will be in tables,how fast the tables will grow and the average size of variable-lengthcolumns.andM;One notable product in this area is the Bachman Database Analyst for DB2,which uses expert systems technology and tight integration with DB2's catalog(via Bachman's Catalog Extract product).andP;  The Bachman DBA uses estimates oftable growth to create the appropriate Freepage and Pct-free values andestimate data set sizes.andP;  Excelerator/DB2 from Index Technology andKnowledgeWare's IEW Design Workstation provide similar help.andM;Dataset size estimation is easy compared with managing the sizes of heavilyupdated indexes.andP;  Even a highly skilled database analyst must somehow monitorstorage use and fix problems that arise.andP;  Two DB2 utilities are RUNSTATS forexamining database update patterns and STOSPACE for measuring dataset growth.andO;Both are useful but may be inadequate for managing large systems.andM;Most DB2 applications have many datasets with multiple extents, and DB2offers RECOVERY, REORG and DSN1COPY to help fix size and extent problems.andO;But these utilities are not automated enough.andP;  Some products are available toaid with dataset management.andM;DASD analyzers examine datasets and allow developers to explore the impacttheir structure will have on performance.andP;  These tools read space-map pagesof a dataset or each page of a dataset, and can determine what space isavailable for updates, the amount of data in overflow pages and the distancesbetween pages.andP;  These products can also provide information on disk drivevolume table of contents so that data managers can determine whether deviceplacement might help performance, and whether and where dataset extentsexist.andM;Tools that analyze datasets maintain statistics useful for trend analysis.andO;This can help determine when to run DB2 utilities.andP;  Products such as thesecan be invaluable in controlling production DB2 applications.andP;  Tools are alsoavailable to fix datasets in extents quickly.andP;  Some offer dataset consistencychecking, and some allow modification of DB2 structures for design changesand tuning.andM;AMERITECH CHOSE PLATINUMandM;Ameritech has chosen to standardize on the tools from Platinum Technology,Chicago.andP;  The firm also has experience with tools from BMC Software,Sugarland, Texas.andP;  &quot;The tools enhance our performance and development time,&quot;said Cunningham, who encouraged other DB2 developers to use available tools.andM;&quot;Tools are very important, whether you go with Platinum, BMC, On-LineSoftware or other vendors.andP;  We tested a lot of products.andP;  They are all verygood and do whatever they say they do.andP;  It's just a matter of choice,&quot;Cunningham said.andM;Montgomery is an IBM employee, working in the Systems Integration division ofIBM Consulting Services in Milwaukee.andP;  He is the author of the book, DB2:Design and Application Tuning (1990, Van Nostrand Reinhold), and aforthcoming book an automating data modeling and DB2 design.andP;  Montgomery'srole at IBM is to help DB2 users understand how automated tools can helpdesign and implement DB2 databases and applications.andP;  (Software Magazine'seditors added information to this story about users' experiences with non-IBMtools.)andO;</TEXT></DOC>