<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-146-918  </DOCNO><DOCID>08 146 918.andM;</DOCID><JOURNAL>Communications of the ACM  Feb 1990 v33 n2 p203(10)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Post implementation evaluation of computer-based informationsystems: current practices.andO;</TITLE><AUTHOR>Kumar, Kuldeep.andM;</AUTHOR><SUMMARY>Post-implementation of computer-based information systems (CBISs)may lead to improved development practices or beneficial decisionsand to the evaluation and training of new personnel.andP;  A study ofsummative evaluations involving a survey of computer users ispresented.andP;  Issues and concerns summarized in evaluationliterature were used to develop a questionnaire that wasdistributed to 462 senior information systems executives.andO;Ninety-two of these responded.andP;  Thirty percent of the respondentswere evaluating 75 percent or more of their CBIS; 26 percent wereevaluating between 25 and 49 percent of the installed systems; and21 percent were not evaluating any systems.andP;  User managers andsystem development managers are the personnel most involved inperforming evaluations.andP;  It is concluded that the main reason forpost-implementation evaluation is to verify the completeddevelopment project against specifications and transferresponsibility for the system to end users.andP;  This use limits thebenefits of evaluation.andP;  A longer-term view of the system and itsdevelopment process would consider the ultimate impact on theorganization and the effectiveness of system users.andP;  Evaluatorsshould consider adopting this approach and using a more global setof evaluative criteria in order to realize the full benefits ofpost-implementation evaluation.andM;</SUMMARY><DESCRIPT>Topic:     Management of EDPPerformance MeasurementInformation SystemsTechnology.andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Post Implementation Evaluation of Computer-Based Information Systems: CurrentPractices With the increasing investment in computers and computer-basedinformation systems (CBIS), the evaluation of these systems is becoming animportant issue in the management and control of CBIS [1, 3-5, 21, 29].andP;  Bothmanagement [4, 27] and IS professionals [23] recognize evaluation of theapplications as one of the important unresolved concerns in the managingcomputer resources.andP;  A 1976 SHARE study [7] recommends evaluation as theprimary technique for establishing the worth of information systems.andM;This evaluation of the systems as they are developed and implemented may takeplace at the completion of various stages of the systems development lifecycle (SDLC) [13].andP;  For example, when a system is evaluated prior toundertaking systems development, evaluation is referred to as feasibilityassessment.andP;  The next set of evaluation activities may be performed at theend of requirements specification and the logical design phase (specificationand design reviews, and approvals), followed by evaluations at the end ofphysical design, coding, or testing.andP;  Finally, evaluations may be performedjust before (acceptance tests and management reviews) or just after (postinstallation reviews) installation.andP;  This will be followed by evaluations ofthe system once it has a chance to settle down (systems-operations postinstallation reviews) [13].andM;A useful way of summarizing and classifying the variety of evaluations isfrom the program and curriculum evaluation literature [24, 28].andP;  Thisliterature distinguishes between formative and summative evaluations.andO;Formative evaluation produces information that is fed back during developmentto help improve the product under development.andP;  It serves the needs of thosewho are involved in the development process.andP;  Summative evaluation is doneafter the development is completed.andP;  It provides information about theeffectiveness of the product to those decision makers who are going to beadopting it.andM;In this study we focus on the summative or post implementation evaluation ofcomputer-based information systems.andP;  The summative evaluation, as definedabove, serves the evaluative information needs of those (user and topmanagement, systems management and system developers) who would finally beaccepting and using the information system.andP;  Therefore, post implementationevaluations include evaluations performed just before installation, justafter installation, and considerably after installation after the system hasa chance to settle down.andM;The information systems literature lists a variety of benefits of postimplementation evaluation of information systems.andP;  Hamilton [12] suggeststhat information system evaluation may result in beneficial outcomes such asimprovement of systems development practices; decisions to adopt, modify, ordiscard information systems; and evaluation and training of personnelresponsible for systems development.andP;  Green and Kiem [10] include benefitssuch as ensured compliance with user objectives, improvements in theeffectiveness and productivity of the design, and realization of cost savingsby modifying systems through evaluation, before, rather than after, a realoperation.andP;  Zmud [30] states that evaluation makes the computer-basedinformation system &quot;concrete&quot; for managers and users so that they canrecognize, if and how, the existing information systems need to be modified.andO;Evaluations are critical to IS investment evaluation [20] and are highlyrated by IS executives as a technique to evaluate information systemseffectiveness [6].andP;  The need for evaluation and its associated benefits havealso been described by others [8, 16, 19, 25].andM;Despite the perceived importance of and the need for the post implementation,the state of knowledge concerning current information systems evaluationpractices is relatively minimal [13, 17, 22].andP;  The common perception seems tobe that post implementation evaluation is seldom performed [12, 26] or is notbeing performed adequately [9, 10, 12, 30].andM;There are three studies which provide limited empirical evidence on postimplementation evaluation practices.andP;  The first is a survey of 31 membercompanies of the Diebold Group, performed in 1977 [6].andP;  The second is anunpublished survey of 51 mid-western U.S.andP;  organizations by Hamilton in1979-80 [13].andP;  Both these studies are somewhat dated and were conducted withlimited, unrepresentative samples.andP;  Furthermore, the Diebold study waslimited to a fairly localized sample of 31 participants in the DieboldResearch Program.andP;  Finally, a study by Hamilton [11] provides empiricalevidence about the criteria, organization, and system characteristicscommonly correlated with the selection of applications for postimplementation reviews.andM;The purpose of our study is to document the current state of practice of postimplementation evaluation of computer-based information systems in businessorganizations.andP;  Specifically, it attempts to answer the following specificquestions:andM;* How prevalent is CBIS Post Implementation Evaluation?andM;* Which stakeholders are typically involved in the evaluation process?andM;* What criteria are currently being used for evaluating CBIS?andM;* What benefits are attributed to CBIS evaluation?andM;* What are likely barriers to post implementation evaluation?andM;This study is useful from both practitioner and researcher perspectives.andP;  Forthe practitioner it highlights the current practices and identifies areaswhich currently do not receive adequate attention.andP;  Practitioners can alsouse the study to compare their organization's evaluation practices againstthe overall norm and investigate the differences (if any).andP;  For theresearcher, the study highlights areas which require further research effortsand are relevant to the business executives' evaluation needs.andM;RESEARCH METHODOLOGYandM;The research approach consisted of three phases.andP;  In Phase I of the study anextensive review of the evaluation literature was performed.andP;  The surveyrevealed that although there was a variety of information systems literaturedealing with the evaluation of information systems, most was limited toprescriptive and normative techniques for performing the evaluation of theCBIS.andP;  There was also a vast amount of literature providing descriptiveevaluations of the existing inventory of installed computer-based informationsystems.andP;  With the exception of the three studies [6, 11, 13] we havementioned, however, very little attention seems to have been given towardsunderstanding and describing information systems evaluation practices intheir organizational setting.andM;In Phase II, the issues and concerns summarized from the literature surveywere used to develop a questionnaire that dealt with the evaluation practicesin organizations.andP;  The questionnaire was designed using the principles usedin market research and pretested at two locations (Southeast U.S.andP;  andSouthwest Ontario) using a total of five information systems executives andthree information systems academics.andP;  Finally, the questionnaire was reviewedby a committee of four information systems professionals and academics.andM;In Phase III of the research project, a cover letter and the questionnairewere addressed and mailed to 462 senior information systems executives of thetop 500 firms in the Canadian Dun and Bradstreet Index.andP;  In order to maintainthe integrity and the independence of the data-collection, data-coding, anddata-conversion procedures, a professional marketing research firm wasengaged to manage the questionnaire mailing, collection, coding, anddata-conversion tasks in this phase.andP;  Of the 462 questionnaires mailed, 32were returned as &quot;individuals moved--address unknown;&quot; and 92 completedquestionnaires were returned for a total response rate of 21 percent.andM;RESPONDENTS' CHARACTERISTICSandM;The range and distribution of the size of the information systems departmentsin the survey, as measured by the monthly hardware budget (rental equivalent)as shown in Figure 1 indicates that the sample includes a wide breadth of MISorganizations.andP;  The median monthly hardware budget for the firms in thesample was between $20,000 to $50,000 with the mode being between $100,000and $500,000.andP;  Ten percent of the organizations in the sample had monthlyhardware budgets exceeding $500,000.andM;A majority of the respondent organizations (69.6 percent) have a history(greater than 10 years) of computer-based information systems use.andP;  On anaverage, the organizations in the sample have been using computer-basedinformation systems for approximately 15 years.andP;  The detailed distribution ofthe number of years of CBIS use for the respondent sample shown in Figure 2is consistent with that reported in a 1979 study (adjusted for time) usingthe same population base [4, p. 81, Table 5.4].andP;  This evidence is a furtherconfirmation of the representativeness of the sample.andM;Along with the maturing use of computer-based information systems, the ISfunction appears to be becoming independent of its earlier origins, where itwas often a subunit of accounting, finance, or some other operatingdepartment.andP;  The sample statistics regarding the organizational location ofthe IS function shown in Figure 3 are consistent with this trend.andP;  In 41percent of the organizations, MIS is an independent-line function, and in 15percent of the organizations, it is a staff department reporting directly totop management.andP;  Only in 40 percent of the organizations does the MISdepartment continue to report to the accounting or finance departments.andM;Finally, the approximate percentage of the MIS budget (operations,development, and maintenance) spent on the three major categories ofinformation systems in the IS portfolio [2] is presented in Figure 4.andP;  Itreflects the current preponderance of transaction processing andoperation-support applications with a move towards management control andstrategic planning systems.andM;RESEARCH FINDINGSandM;This section presents the detailed research findings regarding postimplementation evaluation (PIE) practices in the respondent organizations.andM;The study found that 30 percent of the organizations surveyed were evaluating75 percent or more of their computer-based information systems as shown inFigure 5.andP;  Another 26 percent of the organizations were evaluating between 25percent and 49 percent of the installed CBIS.andP;  Twenty-one percent of theorganizations were not evaluating any of their installed CBIS.andP;  (Theserespondents were eliminated from further analysis).andP;  These figures areconsistent with Hamilton's earlier findings that in 1980, approximately 80percent of the organizations were either performing post implementationreview (PIRs) or indicated plans for implementing PIRs [13, p.14].andM;Timing of EvaluationandM;Respondents were asked to indicate the most frequent stage in the systemsdevelopment process that post implementation was performed.andP;  As shown inFigure 6, most of the organizations performed post implementation evaluationseither just before (28 percent) or just after (22 percent) the cut-over tothe newly installed CBIS.andP;  These, along with the evaluations performed atcut-over (4 percent), constitute the majority (52 percent) of theorganizations.andP;  The distribution had two minor peaks at 3 months (18 percent)and 6 months (14 percent) indicating the presence of systems operations PIRsafter the system is fully installed and has had a chance to settle down andmeaningful performance data is available.andP;  In total, 39 percent and 18percent of the organizations reported that such operations PIRs are performedafter 3 and more, and 6 and more months, respectively, after the cut-over,respectively.andM;Who is Involved in Evaluation?andM;Table I presents summary statistics about the nature of the involvement ofdifferent system stakeholders in the system evaluation process.andP;  The dataindicate that the system development team members are the major participantsin post implementation evaluation.andP;  In 54 percent of the organizations, theyactively manage and perform evaluation, and in 32 percent of the cases, theydetermine both the evaluation criteria and the evaluation method.andP;  In only 18percent of the organizations, however, the team members are allowed toapprove follow-up action (such as system enhancements or modifications) thatmay result from evaluation.andM;After development, the user managers (32 percent) and the systems departmentmanagers (18 percent) are the most involved in managing and performingevaluation.andP;  In 25 percent and 19 percent of the organizations, respectively,they also determine the criteria used for evaluation.andP;  This reflects theirinterest in adopting an effective system and maintaining adequate quality ofthe systems implemented.andM;Being a post implementation or summative evaluation, the evaluation processproduces evaluative information for those decision makers who adopt and usethe system.andP;  This is reflected in the high percentage of organizations wherethe results of evaluation are reviewed by the user management (56 percent),systems department management (54 percent), and the corporate seniormanagement (32 percent).andP;  (1)  They are also the major participants inapproving such action (SD managers (38 percent), user managers (32 percent),and corporate senior management (26 percent)).andM;Finally, in the management and external-auditing literature, there is anincreasing indication of the desirability of auditor involvement in thedevelopment and evaluation of computer-based information systems.andP;  Our dataseem to indicate cautious progress towards this goal.andP;  In 18 percent of theorganizations, internal auditors actively perform or manage evaluations.andO;Though in 24 percent of the organizations they are not involved in theevaluative process, they review the results of evaluations in 40 percent ofthe organizations.andP;  In 15 percent of cases, they are instrumental indetermining evaluation criteria and evaluation methods.andM;CBIS Evaluation Criteria--What is being Evaluated?andM;A substantive issue in evaluation is the question of what is being evaluated?andO;In order to measure this, the respondents were presented with a list ofcriteria or factors which are commonly mentioned in information systemsliterature as candidates for evaluation.andP;  The respondents were asked toindicate the frequency with which these criteria were considered in theevaluation process.andP;  A five-point scale raning from &quot;never evaluated&quot; through&quot;occasionally,&quot; &quot;frequently,&quot; and &quot;usually evaluated,&quot; to &quot;always evaluated&quot;was used to determine the extent to which these criteria were being evaluatedin practice.andM;As shown in Table II, the five most frequently evaluated criteria, in orderof the frequency with which they are evaluated, were the accuracy ofinformation, timeliness and currency of information, user satisfaction andattitudes towards the system, internal controls, and project schedulecompliance.andP;  These top criteria reflect the user, systems development team,and management and internal-audit participation in the riteria determinationprocess discussed in the previous section.andM;The five least used criteria, sorted by lowest to highest frequency, were thesystem's fit and impact upon the organization structure; quality of programs,net operating costs, and savings of the system: system's impact on users andtheir jobs; and quality and completeness of system documentation.andP;  In thecontext of previous studies which indicate the lack of interest insocio-technical issues exhibted by the systems professionals, [14, 18], it isnot surprising that the two criteria dealing with these issues (system's fitwith organization and system's impact on users and their jobs) were among theleast frequently evaluated criteria.andP;  In light of the large amount ofprofessional and research literature dealing with program and documentationquality and cost-benefit analysis of information systems, however, it wassurprising to find that technical and economic issues such as programquality, quality and completeness of documentation, and net operating costsand savings were also among the least frequently evaluated criteria.andM;Finally, in order to understand the underlying structure of the evaluationcriteria, a factor analysis of the criteria was performed.andP;  (2)  After afactor-loading cutoff level of 0.5 was employed, a three-factor structureresulted, with sixteen of the seventeenth criteria loading at that level.andO;The results of the factor analysis are shown in Table III.andP;  The first factorincludes all criteria related to the information product of the system (i.e.,andO;accuracy, timeliness and currency, adequacy, and appropriateness ofinformation) and has been named the &quot;Information Criteria&quot; Factor.andP;  Thesecond factor includes those criteria that do not directly influence the useand effectiveness of the primary system product (information) but areimportant aspects for the continuing operation of the system (such as systemsecurity, internal control, user satisfaction, net operating costs andsavings, and quality of documentation).andP;  We call this factor the &quot;SystemFacilitating Criteria&quot; factor.andP;  The third factor includes those criteriaconcerned with evaluating the consequences or impacts of the newly-installedsystem (system's impact on users and their jobs, system's fit with and impactupon the organization, system usage, and the user friendliness of the systeminterface) and is termed the &quot;System Impact Criteria&quot; factor.andP;  The onlycriteria that did not load onto any of the three factors at the 0.5 level was&quot;Quality of Programs,&quot; which was also found to be one of the least evaluated(second from bottom) criteria in practice.andP;  While no priori loadings werehypothesized, the factor analysis indicates that a logical structure ofcriteria (i.e., Information Criteria, System Facilitating Criteria, andSystem Impact Criteria) does exist.andM;Uses and Benefits of EvaluationandM;The senior information systems executives, as the major reviewers of theevaluation results, and the most frequent approves of follow-up action wereasked their opinion about the more important uses of the results.andP;  Theimportance of a variety of uses and benefits was measured on a five-point,Likert-like importance scale ranging from 1 for low importance to 5 for highimportance.andP;  The results are presented in Table IV.andM;The five most important uses, in the order of importance, are to verify thatthe installed system meets user requirements, to provide feedback to thedevelopment personnel; to justify the adoption, continuation, or terminationof the installed system; to clarify and set priorities for neededmodifications; and to transfer the responsibility for the system from thedevelopment team to the users.andP;  The least important use indicated is theevaluation of the systems development personnel.andP;  This finding shouldreassure those who may be resisting a formal system evaluation because ofapprehension of its use as a personnel evaluation device.andP;  The use of theevaluation process to assess the system's development methodology and theproject management method is also rather low on the importance scale, therebyindicating that systems management has not been able to conceptualize thelink between development methodologies and the quality of information systemsproduced.andP;  The results of a factor analysis on the uses and benefits ofevaluation variables were inconclusive.andM;Inhibitors of EvaluationandM;All respondents (including those who did not perform evaluations) were askedto rate reasons for not performing evaluations.andP;  The reasons were rated on afive-point scale from &quot;Very Unlikely to Inhibit Evaluation (1)&quot; to &quot;VeryLikely to Inhibit Evaluation (5).&quot;andP;  As shown in Table V, the reason mostlikely to inhibit evaluation was the unavailability of users to spend time onthe evaluation activities.andP;  This, along with the unavailability of qualifiedpersonnel and management perceiving inadequate benefits from evaluation werethe greatest inhibitors of evaluation efforts.andP;  The IS executives did notseem to feel that the lack of evaluation methodologies and the lack ofagreement on evaluation criteria were likely to hinder post implementationevaluation.andM;After a factor-loading cutoff of 0.5 employed, a factor analysis (Table V) ofthe inhibiting variables resulted in a two-factor structure, with four of theseven variables loading at that level.andP;  The first factor, which we term&quot;Evaluator Availability,&quot; included the variables &quot;users not available tospend time on evaluation' and &quot;project personnel reassigned; not availablefor evaluation.&quot;andP;  The second factor, &quot;Evaluation Criteria and Methods,&quot;included two relatively weak inhibitors: &quot;the lack of an appropriatemethodology&quot; and &quot;the lack of agreement on evaluation criteria.&quot;andM;DISCUSSIONandM;This study investigated the current practices in the post implementationevaluation of computer-based information systems.andP;  The results of the studyindicate that 79 percent of the organizations surveyed are currentlyperforming post implementation evaluations of some or most of their installedCBIS.andP;  Only 30 percent of the organizations, however, evaluate a majority (75percent or more) of their CBIS, whereas 26 percent evaluate between 25percent to 49 percent of their CBIS.andP;  This finding is consistent withHamilton's earlier findings [12, 13] that post implementation evaluation isperformed only on a small fraction of the systems developed.andM;Among those organizations that perform post implementation evaluations, mostare performed either just before or just after system cut-over andinstallation.andP;  This may reflect the high importance attached to projectcut-over and close-out uses of the evaluation process such as: verificationthat system requirements are met by the installed system, justification ofthe adoption or termination of the installed system, clarification andpriority setting for further modifications, and transfer of responsibilityfor the installed system to the user.andP;  Only 18 percent of the organizationsperform systems operations PIRs (six or moe months after installation, [12]),with the primary intention of assessing and improving the systems productrather than with closing out the development project.andM;The view that, in most cases, evaluation could be a project close-out deviceis further supported by the finding that the major participants in evaluationare the members of the systems development team.andP;  As the developers areusually interested in finishing up the current project so that they can moveon to the next set of development projects, the closing out of the currentproject could be a motivation for performing evaluation.andM;The research findings reveal that much of evaluation is performed and managedby the members of the systems development team.andP;  These are the people whohave the most say in determining evaluation criteria and evaluationmethodology.andP;  Since the design ideals and the values of the developers areinstrumental in shaping the system design and the systems development process[14, 18], it is unlikely that an evaluation managed and performed by thedevelopment team will discover any basic flaws in the process or the productof design.andM;Nonetheless, both user managers and systems development managers participatein evaluation and are the major stakeholders who review the results ofevaluation and approve follow-up action.andP;  As long as this participation issubstantive some of the concerns about the bias of developer-conductedevaluation may be mitigated.andP;  Though internal-audit groups have not madeinroads as major participants in performing evaluations, they help todetermine criteria and to review results.andM;The most frequently evaluated criteria include evaluations of the informationproduct (accuracy, timeliness, adequacy, and appropriateness of information),user satisfaction with the system, and internal controls.andP;  Not surprisingly,reflecting the current value biases of systems developers [14, 18],socio-technical factors such as the system's impact on the users and theorganization were among the least evaluated criteria.andP;  Finally, two criteria,quality of programs and the quality and completeness of system documentation,which are usually emphasized in both practitioner and computer scienceliterature as being important to future operations and maintenance of thesystem, are also among the least frequently evaluated criteria.andP;  This couldreflect the use of evaluation primarily as a responsibility transfer deviceand as a method for the justification and adoption of the installed system.andM;It seems that at least two primary stakeholders in the evaluation process,i.e., the systems development team and the systems management, use evaluationprimarily as a means of closing out the systems project and disengaging fromthe system.andP;  The most important uses of evaluation results included: theverification that the installed system met requirements; the justification ofthe adoption, continuation, or termination of the new system; theclarification and prioritization of further modifications: and the transferof system responsibility to the user.andP;  All of these activities are importantfor closing out the development project.andP;  The use of evaluation results, as afeedback device for improving future development and project managementmethods and for evaluating (and improving) the systems development projectpersonnel, was found to be unimportant, thereby reinforcing the conclusionregarding the primary use of evaluation as a disengagement strategy.andM;The factors most likely to inhibit evaluation were found to be theunavailability of two of the major participants in the systems developmentprocess--the users and the qualified project team personnel.andP;  This againsuggests that once the system is completed and implemented, the majorstakeholders are interested in getting on with other work and use evaluationas a milestone for completion.andM;It was also felt by the respondents that the corporate management did notperceive adequate benefits from evaluation.andP;  Hamilton [12, pp.133-137] hasempirically demonstrated that the behavioral intention to perform postimplementation reviews is strongly influenced by the evaluators' normativebeliefs about what salient referents think should be done and the motivationto comply with them.andP;  Since corporate management is a strong salient referentand since it does not perceive adequate benefits from evaluation, evaluationis less likely to be performed as a evaluative rather than a close-outdevice.andM;Finally, the lack of agreement on evaluation criteria and the lack ofappropriate methodology for evaluation were not found to be major inhibitorsof evaluation.andP;  Given the current controversy in the information systemsliterature regarding appropriate criteria, measures, and methods forinformation systems evaluation, the finding that these factors do not inhibitevaluation is surprising.andP;  It is possible that, given the close-out nature ofevaluation, the evaluators have given only superficial consideration to thesubstantive issues that make the criteria and methods controversial.andM;CONCLUSIONS AND RECOMMENDATIONSandM;The study findings point to three key conclusions.andP;  First, it appears thatthe major reason for performing post implementation evaluation is theformalization of the completion of the development project whereby thedeliverable (i.e., the installed system) is verified against specifications,any unfinished business, such as further modifications, is noted, and theresponsibility for the system is transferred to the users.andP;  Evaluation thenbecomes a major tactic in a project disengagement strategy for the systemsdevelopment department.andP;  Evaluation does not seem to be for the purpose ofeither long-term assessment of the system impact and effectiveness or for thepurpose of providing feedback to modify inappropriate development and projectmanagement practices.andP;  Further, it is not to counsel and educate ineffectiveproject team personnel.andM;This conclusion seems to be reinforced by finding that the majority ofevaluations are performed either just before, at, or after system cut-over,and only in 18 percent of the organizations are true systems operations PIRsperformed.andP;  Given the limited objectives of evaluation, it is doubtful thatmanagement and the users perceive adequate benefits from this exercise.andP;  Thiscould be the reason for the study finding that the top inhibitors ofevaluation include: the unavailability of users and development personnel forevaluation activities and management nor perceiving adequate benefits fromevaluation.andM;Second, much of evaluation is managed and performed, and evaluation criteriaand methods are determined by those who have designed the system beingimplemented.andP;  Since the designers would already have designed most of thefactors they consider important, it is not likely that it will uncover anybasic flaws in the product or the process of systems design.andM;Third, the most frequently evaluated criteria seem to be informed qualitycriteria (accuracy, timeliness, adequacy, and appropriateness) along withfacilitating criteria such as user satisfaction and attitudes and internalcontrols.andP;  Socio-technical criteria such as system's impacts on the user andthe organization, as well as the long-term maintenance and growth of thesystem (system documentation and program quality), are evaluated much lessfrequently.andM;These conclusions suggest that post implementation evaluations are beingperformed for the limited, short-term reason of formalizing the end of thedevelopment project and may not provide the more important long-term,feedback-improvement benefits of the evaluation process.andM;In order to realize these benefits, the evaluators need to take a longer-termview of the system and its development process.andP;  In such a view, long-termimpacts (such as its impact on the organization and the system users andtheir effectiveness) would be considered, and long-term viability (in termsof cost savings, security, maintenance, program quality, and documentationquality) would be assessed.andP;  This would require that corporate and systemsmanagement formally recognize the role of post implementation evaluation as atook for providing feedback about both the systems development product, aswell as the development process, and realize that this feedback is invaluablefor improving both the product and the process.andP;  The results of theevaluation process can then be reviewed to see which of these long-termobjectives have been addressed by the evaluation.andM;As this evaluation would be looking at the longer-term impacts and viability,the formal evaluation should be performed when the system has had a chance tosettle down and its impacts are becoming visible through continued operation.andO;Depending on the scope of the system this may be where between three andtwelve months after the system cut-over.andM;Next, in order to ensure the independence of evaluation and a more global setof criteria than those conceived by the developers, evaluation should bemanaged and performed by people other than the members of the developmentteam.andP;  The mechanism for performing post implementation evaluation may eitherbe an independent quality assurance group or a multi-stakeholder evaluationteam led by the users.andM;An evaluation group independent of the development team does not preclude thepossibility of the developers contributing to the evaluation process.andP;  Theexistence of a formal quality assurance group will also reduce the effect oftwo of the major inhibitors, i.e., the unavailability of the users and thedevelopment personnel for evaluation.andM;Finally, a longer-term, feedback-improvement-oriented post implementationevaluation, with the accompanying system and the development processimprovement benefits, would be helpful in gaining corporate managementsupport for evaluations, thereby increasing the possibility of moresubstantive and meaningful evaluations being performed.andP;  Unless the aboverecommendations are implemented, post implementation evaluations willcontinue to serve the limited purpose of closing out the development project.andM;(1) For the sake of brevity, the detailed statistics for corporate seniormanagement, external auditors and MIS staff other than the project teammembers are not included in Table I.andP;  Significant statistics for thesestakeholder groups are presented in the accompanying narrative.andM;(2) Factor analysis is a statistical techniques used to discover which of theelements or variables in a sample population vary together and therefore maybe candidates for grouping together into groups called factors.andP;  For anintroduction to and an explanation of factor analysis see [15].andM;REFERENCESandM;[1] Ball, L., and Harris, R.andP;  SMIS member: A Membership analysis.andP;  MIS Q. 6,1 (Mar.andP;  1982), 19-38.andM;[2] Benbasat, I., Dexter, A., and Mantha, R. W.andP;  Impact of organizationalmaturity on information system skill needs.andP;  MIS Q. 4, 1 (Mar.andP;  1980), 21-34.andM;[3] Brancheau, J.C., and Wetherbe, J. C.andP;  Key issues in information systemsmanagement.andP;  MIS Q. 11, 1 (Mar.andP;  1987), 23-45.andM;[4] Cooke, J. E., and Drury, D. H.andP;  Management planning and control ofinformation systems.andP;  Society of Management Accountants Research Monograph,Hamilton, Ontario, 1980.andM;[5] Dickson, G. W., et al.andP;  Key information system issues for the 1980s.andP;  MISQ. 8, 3 (Sept.andP;  1984), 135-153.andM;[6] The Diebold Group Inc.andP;  Key measurement indicators of ADP performance.andO;Doc.andP;  No.andP;  S25, Diebold Research Program, New York, N.Y., 1977.andM;[7]Dolotta, T. A., et al.andP;  Data Processing in 1980-1985: A Study of PotentialLimitations to Progress.andP;  John Wiley and Sons, New York, 1976.andM;[8] Domsch, M.andP;  Effectiveness measurement of computer-based informationsystems through cost-benefit analysis.andP;  In Design and Implementation ofComputer-Based Information Systems.andP;  Szyperski, N. and Gorchla, E., Eds.,andO;Sijthoff andamp; Noordhoff, Alphen an den Rijn, The Netherlands, 1979.andM;[9] Dumas, P. J.andP;  Management Information Systems: A dialectic theory and theevaluation issue.andP;  Ph.D Dissertation, Univ.andP;  of Texas, Austin, 1978.andM;[10] Green, G. I., and Keim, R. T.andP;  After implementation what's next?andO;Evaluation.andP;  J. Syst.andP;  Manage.andP;  34, 9 (Sept.andP;  1983), 10-15.andM;[11] Hamilton, J. S.andP;  EDP quality assurance: Selecting applications forreview.andP;  In Proceedings of the Third International Conference on InformationSystem Systems (Ann Arbor, Mich., Dec. 13-15).andP;  ACM/SIGBDP, New York, 1982,pp.andP;  221-238.andM;[12] Hamilton, J. S.andP;  Post Installation systems: An empirical investigationof the determinants for use of post installation reviews.andP;  Ph.D.andO;Dissertation, Univ.andP;  of Minnesota, 1981.andM;[13] Hamilton, J. S.andP;  A survey of data processing post installationevaluation practices.andP;  MIS Research Center Working Paper, MISRC-WP-80-06,Univ.andP;  of Minnesota, 1980.andM;[14] Hedberg, B., and Mumford, E.andP;  Design of computer systems: Man's visionof Man as an integral part of the system design process.andP;  In Human Choice andComputers.andP;  E. Mumford and H. Sackman, Eds., North-Holland, Amsterdam, TheNetherlands, 1975.andM;[15] Kim, J.-O., and Mueller, C. W.andP;  Introduction to Factor Analysis.andP;  SageUniversity Press, Beverly Hills, Calif., 1978.andM;[16] Kleijnen, J.P.C.andP;  Computer and Profits.andP;  Addison-Wesley, Waltham, Mass.,andO;1980.andM;[17] Kriebel, C. H.andP;  The evaluation of Management Information Systems.andP;  IAGJ. 4, 1 (1971), 1-14.andM;[18] Kumar, K., and Welke, R. J.andP;  Implementation failure and system developervalues: Assumptions, truisms and empirical evidence.andP;  In Proceedings of theFifth International Conference on Information Systems (Tucson, Ariz., Nov.andO;28-30).andP;  ACM/SIGBDP, New York, 1984, pp.andP;  1-12.andM;[19] Land, F. Evaluation of systems goals in determining a strategy for acomputer-based Information System.andP;  Comput.andP;  J. 19, 4 (1978), 290-294.andM;[20] Matlin, G. L.andP;  What is the value of investment in Information Systems?andO;MIS Q. 3, 3 (Sept.andP;  1979), 5-34.andM;[21] Mautz, R. K., et al.andP;  Senior management control of computer-basedInformation Systems.andP;  Research Monogrpah of the Research Foundation ofFinancial Executives Institute, New Jersey, 1983.andM;[22] Norton, R. L., and Rau, K. G.andP;  A Guide to EDP Performance Management.andO;QED Information Sciences, Wellesley, Mass., 1978.andM;[23] Powers, R. F., and Dickson, G. W.andP;  MIS project management: Myths,opinions, and reality.andP;  Calif.andP;  Manage.andP;  Rev.andP;  15, 3 (Spring 1973), 147-156.andM;[24] Scriven, M.andP;  The methodology of evaluation.andP;  In Perspectives ofCurriculum Evaluation.andP;  R. W. Tyler, R. M. Gagne, and M. Scriven, Eds., AERAMonograph Series on Curriculum Evaluation, Vol.andP;  1, Rand McNally and Co.,andO;Chicago, 1967, pp.andP;  39-83.andM;[25] Seibt, D.andP;  User and specialist evaluations in system development.andP;  InDesign and Implementation of Computer-Based Information Systems.andP;  N.andO;Szyperski and E. Gorchla, Eds., Sijthoff and Noordhoff, Holland, 1979.andM;[26] Sollenberger, H. M., and Arens, A. A.andP;  Assessing Information Systemsprojects.andP;  Manage.andP;  Account.andP;  (Sept.andP;  1973), 37-42.andM;[27] Waldo, C.andP;  Which departments use the computer best.andP;  Datamation 27,(Mar.andP;  1980), 201-202.andM;[28] Weiss, C. H.andP;  Evaluation Research--Methods for Assessing ProgramEffectiveness.andP;  Prentice-Hall, Englewood Cliffs, N. J., 1972.andM;[29] Welke, R. J.andP;  Information Systems effectiveness evaluation.andP;  WorkingPaper, Faculty of Business, McMaster University, Hamilton, Ontario, Canada.andM;[30] Zmud, R. W.andP;  Information Systems in Organizations.andP;  Scott, Foresman andCompany, Glenview, Ill., 1983.andM;CR Categories and Subject Descriptorsandgt; K.6.1 [Management of Computing andInformation Systems]: Project and People Management--life cycle; managementtechniques; system development; K.6.4 [Management of Computing andInformation Systems]: System Management--management audit; quality assurance.andM;General Terms: ManagementandM;Additional Key Words and Phrases: Information Systems evaluation, postimplementation evaluation, post implementation review, post implementationaudit.andM;KULDEEP KUMAR is an assistant professor of Computer Information Systems inthe College of Business, Georgia state University.andP;  He is a member of IFIP TCWG 8.2 and the IEEE computer society and has served in several programcommittees for IFIP and the International Conference on Information System.andO;His current research interests include: management of information systems,information systems planning, information systems development methodologies,and methodology engineering.andP;  Author's Present Address: Computer InformationSystems, Georgia State Univeristy, University Plaza, Atlanta, GA 30303andM;Permission to copy without fell all or part of this material granted providedthat the copies are not made or distributed for direct commercial advantage,the ACM copyright notice and the title of the publication and its dateappear, and notice is given that copying is by permission of the Associationfor Computing Machinery.andP;  To copy otherwise, or to republish, requires a feeand/or specific permission.andO;</TEXT></DOC>