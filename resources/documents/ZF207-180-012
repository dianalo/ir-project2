<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-180-012  </DOCNO><DOCID>07 180 012.andM;</DOCID><JOURNAL>Hewlett-Packard Journal  April 1989 v40 n2 p75(5)* Full Text COPYRIGHT Hewlett Packard 1989.andM;</JOURNAL><TITLE>Validation and further application of software reliability growthmodels. (HP's Lake Stevens Instrument Div.) (technical)</TITLE><AUTHOR>Kruger, Gregory A.andM;</AUTHOR><SUMMARY>H-P's Lake Stevens Instrument Division develops a softwarereliability growth model that, after two years of use, has beenvalidated with empirical data and has demonstrated itsapplicability to large and small projects.andP;  The model is now beingexpanded to estimate test duration before it begins.andP;  Projectmanagers are now able to plan staffing adjustments as the systemintegration test phase of the software life cycle moves throughthe defect-fixing-limited phase and into thedefect-finding-limited phase.andP;  The model can estimate totaltesting effort and help assess a project's readiness for releaseto customer shipments.andP;  Future success for the model will rely onthe integration of customer representation testing techniques intothe existing test process.andP;  The ultimate objective of the softwaredevelopers is to achieve validation of the modeling processthrough accurate field failure data.andM;</SUMMARY><DESCRIPT>Company:   Hewlett Packard Co. Lake Stevens Instrument Div. (products).andO;Ticker:    HWP.andO;Topic:     ReliabilitySoftware DesignTestingValidation.andO;Feature:   illustrationgraph.andO;Caption:   Project results. (graph)andM;</DESCRIPT><TEXT>Validation and Further Application of Software Reliability Growth ModelsandM;ATHP'S LAKE STEVENS INSTRUMENT DIVISION, a software reliability growth modelhas demonstrated its applicability to projects ranging in size from 6 KNCSSto 150 KNCSS (thousand lines of noncomment source statements), and infunction from instrument firmware to application software.andP;  Reliabilitymodeling curves have been used to estimate the duration of system integrationtesting, to contribute to the release-to-sales decision, and to estimatefield reliability.andP;  Leveraging from the basic model, project managers arebeginning to plan staffing adjustments as the QA effort moves through thedefect-fixing-limited phase and into the defect-finding-limited phase.andM;Basic ModelandM;In the fall of 1986, a software reliability growth model's good fit tohistorical data on a previous firmware product led to the development of aset of release criteria, with defects per system test hour (QA hour) as theprincipal quality measure.andP;  The model and release criteria were then appliedin real time to a new application product.andP;  The modeling effort aided inpredicting when the product was ready for release to customer shipments andprovided estimates for the number of defects that might be found in thefield.andM;The basic exponential model is based upon the theory that the software defectdetection and removal effort will follow a nonhomogeneous Poisson process.andO;In this process the defect arrival rate is assumed to decrease with everyhour of testing (or at least with every correction).andP;  The model has twocomponents.andM;The cumulative number of defects found by time t is given by m(t) =a(1-e).sup.(k/a)t., and the instantaneous new defect-finding rate at time tis given by l(t) = ke.sup.-(k/a)t..andM;Fitting the model requires the estimation of parameters k, the initial defectdiscovery rate, and a, the total number of defects.andP;  The data required isobtained by recording on a daily or weekly basis the time spent executing thesoftware and the resulting number of defects discovered.andP;  The modelparameters may be estimated by the least squares, nonlinear least squares, ormaximum likelihood method.andP;  In most cases, the maximum likelihood method ispreferred.andM;Considering typical software development and system testing practices, theassumptions necessary for the applicability of Poisson theory would seem tonegate the use of the model.andP;  Key assumptions of the model and thecorrespondingly realities are:andM;* Assumption: All functionality is completed before the start of systemtesting.andM;Reality: Many products enter system testing without all the features inplace.andM;* Assumption: Testing can be considered to be repeated random samples fromthe entire input domain.andM;Reality: There is some random testing, but typically testers are morestructured and systematic in the selection of test cases.andM;* Assumption: Defects found are removed with certainty and no new defects areintroduced (a perfect repair).andM;Reality: A defect repair may introduce new defects.andM;* Assumption: The times between failures are independent.andM;Reality: When a defect is found in a particular area of the software, becauseof the suspicion that there may be more defects in the same area, the area isprobed for more defects.andP;  This process usually finds more defects, which isgood, but makes the arrival rate of defects dependent on when the last onewas found.andM;As has been said, with such a set of assumptions, it would seem unlikely thatthis model would fit real-world data.andP;  However, some aspects of the testingprocess at Lake Stevens approximate these conditions.andP;  First, our life cyclecalls for all functionality to be completed by the time we start formalsystem integration testing.andP;  Typical projects have 95% or more of theirfunctionality complete by this time.andP;  Second, the entire set of functionalityis subdivided and assigned to different individuals of the testing team.andO;Therefore, while the testing process cannot be considered to be repeatedrandom samples from the input domain, it is at least sampling from the entirefunctionality set as time progresses.andP;  This is in contrast to a testingprocess wherein some subset of the functionality is vigorously tested to theexclusion of all others before moving on to another subset and so on.andO;Regarding the third assumption, strict revision control procedures at leastmaintain some control over the rate of defect introduction.andP;  Finally, nothingabout the Lake Stevens development process justifies the assumption that thetimes between failures are independent.andP;  After finding a serious defect in aportion of the product, testing effort often intensifies in that area, thusshortening the next time to failure.andM;The model's success in describing the projects at LSID demonstrates somedegree of robustness to these assumptions.andP;  Our past and continuedapplication of software reliability theory is not based on a fundamentalbelief in the validity of the assumptions, but in the empirical validation ofthe model.andP;  ThereforE, we have continued to use software reliability growthmodels with the following objectives in mind:andM;* To standardize the application of the model to all software productsproduced at LSIDandM;* To put in place a set of tools to capture and manage the data and obtainthe best fit curvesandM;* To use the defect-finding rate and the estimated defect density to definethe release goalandM;* To predict the duration of the QA phase before its startandM;* To understand the relationship between model estimates and field results.andM;Standardized ApplicationandM;To date, software reliability growth modeling has been conducted on elevenprojects that have since been released for customer shipment.andP;  Twodemonstrated excellent fit to the model, two very good fit, four showed afair conformance to the model, and three showed a poor fit.andP;  Fig.andP;  1 showsthe curves for one of the projects on which the model gave an excellent fit.andO;Contrast these results to the model's performance on the project shown inFig.andP;  2.andP;  Note that time in this case is measured in calendar days ratherthan test hours.andP;  Here the cumulative defects begin to taper off only tostart up again.andP;  These results reflect inconsistent testing effort, which isnot picked up by simply measuring calendar days of testing effort.andP;  Thecurves in Fig.andP;  2 were obtained by independently fitting the basic modelbefore and after the change in testing effort.andP;  There two best-fit modelswere then tied together to form the piecewise curves shown.andM;ToolsandM;The defect tracking system (DTS), an internal defect tracking tool, is usedby all project teams to log defects found during system testing.andP;  In softwarereliability modeling it is important to record all time spent exercising thesoftware under test regardless of whether a defect is discovered.andP;  DTS hasproven to be unsatisfactory for capturing QA hours that do not produce asoftware defect.andP;  Therefore, project teams separately log test hours at theend of each day.andM;The DTS data is loaded into an Informix data base so that it can be sortedand retrieved as desired.andP;  On projects using DTS for tracking QA time as wellas defect statistics, Informix reports generate files with weekly (or daily)QA hour and defect total data pairs.andP;  On projects tracking QA timeseparately, the weekly (or daily) defect totals are retrieved from theInformix data base and matched with the appropriate QA hours.andP;  In eithercase, the file of cumulative QA hours and cumulative defects found issubmitted to a program that obtains the best-fit model parameters by themethod of maximum likelihood.andP;  At the present time, plots for distributionare generated using Lotus 1-2-3.andP;  Future plans call for using S, astatistical package that runs in the HP-UX environment, to generate thegraphics, thereby conducting the data manipulation, analysis, and plottingall on one system.andM;Release GoalandM;The software modeling process provides two related metrics that help supporta release-to-customer-shipments decision: the defect-finding rate and theestimated number of unfound defects.andP;  A specific goal for one of these twometrics must be established if the model is to be used for predicting theconclusion of system testing.andM;The defect-finding rate is a statistic you can touch and feel.andP;  It can bevalidated empirically--for example, 100 hours of test revealed four defects.andO;On the other hand, one can never really measure the number of defectsremaining.andP;  This metric can only be estimated.andP;  Although the two measures arerelated, it is not true that two projects releasing at the samedefect-finding rate goal will have the same number of defects estimated to beremaining.andP;  Couple this fact with the recognition that the size of theproduct has no bearing on the model fit and the resulting estimated number ofresidual defects and it is clear that two projects releasing at the same findrate could have quite different estimated residual defect densities.andP;  Becauseof its observability, the defect-finding rate has been used as the principalrelease goal on all projects to date except one.andP;  However, both the failurerate and the estimated residual defect density are monitored and used inaiding the release decision.andM;The Project E ExperienceandM;The one project to date using a goal of ending system test with a certainresidual defect density will serve as a good illustration of thecontributions and limitations of software reliability growth models.andP;  ProjectE is an application software product of 156 KNCSS.andP;  This project represents anew release of a previously developed product and is roughly two-thirdsreused or leveraged code.andP;  The stated goal at the start of system integrationtesting was to achieve an estimated residual defect density of 0.37 defectsper KNCSS, goal derived from the performance of the first release of thisproduct.andP;  Such a goal means that the best-fit model should be estimating 58residual defects.andM;A team of engineers was assembled to conduct testing while the project teamfixed defects.andP;  The data was plotted at roughly 30-hour testing intervals andthe model refit each week.andP;  The most recent curve was used to estimate the QAhours required to achieve the objective and these estimates were plottedweekly with statistical confidence limits as shown in Fig.andP;  3.andP;  In mid-April,the decision was made to release the project for customer shipments and tocontinue further testing and refinements for a final release in June.andP;  Theteam had all but reached the goal and the data had tracked the model verywell.andP;  At this point, the engineers on the testing team disbanded andreturned to their original project assignments.andP;  The design team then took onthe task of conducting both continued testing and defect resolution.andP;  Withonly the designers looking, the defect discovery rate jumped up rather thancontinuing to follow the curve as can be seen in Fig.andP;  4.andP;  The designers weretesting specific areas of the code (directed testing), so an hour of testingnow was not equivalent in intensity to an hour of testing with the previoustest team.andP;  The testing process was not meeting the assumption that testingcan be considered to be repeated random samples from the entire user inputdomain.andM;What is clear from this project is that the failure rate data and curves aremodeling more than the software product alone.andP;  They are modeling the entireprocess of testing.andP;  The estimates of failure rates and residual defectdensities are estimates only as good as the testing process itself.andP;  Thedegree to which these statistics match field results will depend upon thedegree to which the testing matches the customer's use profile.andP;  Theidentification of the customer's use profile and the incorporation of thatinformation into the testing strategy is a topic for further investigation.andM;Before QA BeginsandM;Naturally we would like to estimate the duration of the QA phase before itbegins.andP;  But fitting a model to do estimation must wait for testing to beginand for enough data to be collected before an effective statistical analysiscan be conducted.andP;  However, it is possible to use results from past projectsto estimate the two model parameters a and k.andM;In preparation for testing a recent software product, Project F, we reviewedthe total number of defects discovered during system integration testing onpast projects.andP;  Defect densities appeared to fall between 12 and 20 defectsper KNCSS.andP;  Project F had 28.5 KNCSS, so the likely range for the first modelparameter, a, was calculated to be 342 to 570 defects.andP;  Again looking at pastprojects, the initial defect discovery rate averaged around one defect perhour, so the other model parameter, k, could be set to one.andP;  Given a goal forthe failure rate of 0.08 defects per hour, an expected range of 864 to 1440QA hours was calculated.andM;Management ultimately needs an estimated date of completion so the expectedQA hours required for system testing must be converted to calendar time.andP;  Toaccomplish this we again reviewed the data on past projects and discovered anamazing consistency of four QA hours per day per person doing full-timetesting, and an average of 2.3 defects fixed per day per person doingfull-time fixing.andP;  Given the number of team members capable of fixing, thenumber capable of finding and those qualified to do both, the required QAhours for testing could now be converted to calendar time.andP;  Fig.andP;  5 shows thefinal QA projections for Project F and the staffing levels used to convertthe QA hours into calendar time.andP;  Note that the staffing levels givencorrespond to the midrange assumption of 16 defects per KNCSS.andM;Recognize that as testing proceeds, testing and fixing resources will have tobe shifted.andP;  Early in the process, the project is fixing-constrained becausea few testers can and enough defects to keep and available fixers busy.andP;  Overtime this changes, until late in testing, the project is finding-constrainedsince it takes many resources looking for defects to keep only a few fixersworking.andP;  Also, the finders cannot be allowed to outstrip the fixers,creating a large backlog of unresolved defects.andP;  Such a situation only causesfrustration for the finders because of testing roadblocks created by defectsalready found.andM;Our experience to date with using the model to estimate the duration of theQA phase before its start demonstrates the difficulty in estimating the tworequired model parameters without actual system test data.andP;  Project Fconcluded system integration testing with a total QA effort that was 225% ofthe original effort.andP;  Over twice the expected number of defects were foundand resolved.andP;  Not all of this error in estimation can be blamed on thefailure of the model.andP;  In hindsight, we completely disregarded the fact thatthis was not a stand-alone project.andP;  Many of the problems encountered werebecause Project F had to be integrated with another 130-KNCSS product.andM;These results indicate that adjustments are necessary in the way values forthe model parameters are derived.andP;  For instance, currently the values for theparameters are averaged from a heterogeneous population of previous softwareprojects.andP;  Fixing this problem means that if we want to estimate the QAduration for project X then we must base the model parameters on projectssimilar to project X.andP;  Project characteristics such as complexity, teamexperience, and the development language must be formally factored into anyearly estimates of QA duration.andM;Field Failure ResultsandM;Although the modeling process is helping to monitor progress through systemtesting and is aiding in the release decision, based upon limited defect datafrom our field defect tracking system it appears that the curves may beoverestimating the number of defects customers will discover by as much asforty times.andP;  However, it is likely that only a portion of the actual fieldfailures find their way into the tracking system.andM;Our experience testing one firmware project that was an enhanced version ofan old instrument puts an interesting perspective on estimated residualdefect densities.andP;  This particular product had been shipped for ten years atan average volume of over 200 units per month.andP;  Since a market opportunityexisted for an updated version of that product, both hardware and firmwareenhancements were incorporated into a new version.andP;  During system test, anobscure defect in a math routine was discovered that had not only existed inthe original product since introduction but in several other products shippedover the last ten years.andP;  To the best of our knowledge, no customer or HPpersonnel had previously found that failure.andP;  Its existence was furtherargument that the information coming back from the field is not giving a trueperception of residual defect densities.andP;  Not only do customer observedfailures go unreported, but it is highly likely that some failures will neverbe encountered during operation.andP;  It was reassuring to note that LSID'scurrent testing process was uncovering defects so obscure as to beunobservable in ten years of field use.andM;ConclusionandM;With data collected during system integration testing, we have been able touse a software reliability model to estimate total testing effort and aid inassessing a project's readiness for release to customer shipments.andP;  Althoughthe model appears to be somewhat robust to its underlying assumptions, futuresuccess will depend upon the integration of customer representative testingtechniques into our existing testing process.andP;  In addition, there remains thechallenge of using the model to estimate test duration before systemintegration begins.andP;  This will require a thorough analysis of data on pastprojects and key information on the current project to derive better earlyestimates of the model's parameters.andP;  Our ultimate objective remains toachieve validation of the modeling process through accurate field failuredata.andP;  All of these areas will continue to be investigated because they areimportant in determining project schedules and estimating product quality.andO;</TEXT></DOC>