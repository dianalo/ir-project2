<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-745-360  </DOCNO><DOCID>07 745 360.andM;</DOCID><JOURNAL>Communications of the ACM  Oct 1989 v32 n10 p1241(18)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Technical correspondence.andM;</TITLE><DESCRIPT/><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Calendar P's and QueuesandM;The article, &quot;Calendar Queues: A Fast 0(1) Priority Queue Implementation forthe Simulation Event Set Problem,&quot; by Randy Brown (Communications, Oct.andO;1988, pp.andP;  1220-27) presents a mechanism whose time-hadcome-to-be-invented.andO;Working independently, we have both come up with essentially the samemechanism.andP;  In addition, it appears that some features of the mechanism werediscovered independently by a third party [1].andM;In April of 1988, IBM released its VM/XA SP1 operating system which uses myimplementation of the mechanism to do its timer-request-queuing.andP;  In mostoperating systems, components of the operating system may request to benotified when a specified time is reached.andP;  The software must queue eachrequest until the hardware clock reaches the specified time.andP;  I thought itwould be appropriate to write and describe IBM's experiences with this newmechanism for doing timer-request-queuing.andP;  I also want to congratulate RandyBrown for his independent discovery of the idea and for his excellentexplanation of it.andM;In the past, all versions of IBM's VM operating system implemented thetimer-request-queue as a simple linear chain.andP;  But with larger numbers ofusers on the systems in recent years, the O(N) growth in the overhead ofsearching this chain began to be a problem.andP;  The solution used by thedevelopers of VM/SP-HPO (a predecessor of VM/XA SP) was to find ways ofreducing the number of timer-requests per user, while still keeping the 0(N)mechanism.andP;  Obviously, that was only a temporary solution.andP;  While runningVM/XA SF (another predecessor of VM/XA SP) in the lab with experimentalworkloads (large N), there were hints that the O(N) timer-request-queuing wasbecoming a problem.andP;  Sometime in 1986, 1 devised and then implemented anddebugged a timer-request-queue using a &quot;heap&quot; mechanism.andP;  But we did notbother testing the heap in the lab with interesting workloads (large N)because before finishing the heap I had figured out how thetimerrequest-queue could be better implemented using a hash table.andP;  (I hadtoyed with the hash-table idea earlier, but at first could not see how tomake it work.)andM;Normally, a practitioner might be satisfied to stay with the heap at thatpoint since the heap could be expected to produce a dramatic overallreduction in overhead for the large N to be tested in the lab.andP;  But, in thiscase, there was reason to go beyond the heap.andP;  Although the heap would give avast reduction in enqueuing overhead, nevertheless, there would be a moderateincrease in dequeuing overhead.andP;  This was a concern because, due to apeculiarity of the VM/XA operating system, the overhead on the dequeuing sideis more critical than the overhead on the enqueuing side.andM;So, I discarded the heap and implemented the new hash-table mechanism.andP;  Thisimplementation was built upon VM/XA SF code, but too late to ship with VM/XASF.andP;  The modified VM/XA SF system was tested in the lab in May, 1987, on anIBM 3090-600 (a system having six processors), with a workload consisting offive thousand simulated users.andP;  (More users than the shipped version of VM/XASF could handle.) As we expected, the new mechanism reduced the overheadassociated with the timer-request-queue dramatically.andP;  Subsequently, the newmechanism was included in VM/XA SP1 and was shipped in April, 1988.andP;  Aspin-lock is held during all manipulations of the timer-request-queue.andO;Therefore, the overhead under consideration is not only the direct overheadfrom manipulating the queue, but also overhead generated by other processorsspinning while one processor holds the lock to manipulate the queue.) Here isa comparison of Randy Brown's mechanism and IBM's VM/XA SP mechanism.andP;  Theremarkable similarities give credence to the idea that &quot;there is only one wayto do it right.&quot;andM;SimilaritiesandM;1.andP;  Each &quot;day&quot; of the queue is represented by a sortedandM;linked list of the request scheduled for that day.andM;2.andP;  A table (or array) containing one pointer for eachandM;&quot;day&quot; of the &quot;year&quot; is used to find the linked list for a particular day.andM;3.andP;  There is no overflow list.andP;  Requests scheduled forandM;future years (beyond the end of the table) are wrapped back into this year'stable.andP;  They are placed in the table according to the day they come due,disregarding the year.andP;  They are placed in the sortedlinked list with theyear taken into account in the sorting.andM;4.andP;  The size of the table (length of the year) and theandM;width of a &quot;day&quot; are both adjusted dynamically.andM;5.andP;  The table size is always a power of 2.andP;  The width ofandM;the &quot;day&quot; is also a power of 2.andM;6.andP;  We use a special &quot;direct search&quot; to find the nextandM;request to dequeue whenever the forward scan fails to find anything in anentire &quot;year.&quot;andM;7.andP;  We maintain a &quot;current day&quot; cursor to mark whereandM;we are in the scan.andP;  This cursor consists of several data fields, including&quot;index to the table&quot; (day number) and &quot;time at end of current day.&quot;andM;8.andP;  We have similar targets for efficient operation.andM;Randy Brown recommends that at least seventy-five percent of the requestsshould fall within the coming year; the VM/XA target is to get eighty and ahalf percent (seven eighths) within the coming year.andP;  Randy Brown recommendsthat three or four requests from the current year should be found in thecurrent day; whereas, our target is five.andM;Differences/ExtensionsandM;1.andP;  Our operating system requires a &quot;cancel request&quot;andM;function.andP;  To support this function, our chains of requests are doubly linked(forward and backward) so that the chain can be easily closed after a requestis removed.andP;  Also, each request contains a pointer to the anchor-word of itschain.andP;  The anchor may require updating when a request is removed, and thisallows the anchor to be located quickly.andM;2.andP;  When inserting a new request in a queue we canandM;scan the chain either forward or backward.andP;  We are careful about making thischoice, as there is a possibility that there may be a few very long queues.andO;If they exist, we do not want to be scanning them in the wrong direction.andO;Such queues can exist if there are some favorite times that people request.andO;There would be such times as &quot;now plus ten seconds,&quot; &quot;infinity minus onesecond,&quot; etc.andP;  (&quot;Infinity,&quot; here, means the largest time the clock canhandle: all 1bits in the time field.) If the new request is one of these,then it is equal to or greater than the last such request in the chain.andP;  So,it makes sense to scan the chain backward from the far end.andP;  This is alsotrue in the general (normal short queue) case.andP;  We would expect that the newrequest will most likely be later than most of the requests already in thequeue.andP;  There is one case, however, where we do a forward scan when insertinginto a queue.andP;  We scan forward if the last request in the chain is within onehour of infinity (suggesting that this queue may be one of the long ones) andthe new request is not within one hour of infinity.andM;3.andP;  We do not do &quot;trial dequeuing&quot; to sample the overandM;head.andP;  Instead, we track the overhead continuously by counting the nodes thatwe pass whenever we scan a chain and also when we scan forward past emptyanchors in the table.andP;  (In assembler, this takes just one instruction in theloops to increment a register.) We keep the following statistics:andM;(a) Count of (recent) enqueue operations.andM;(b) Count of nodes skipped (recently) while scanningandM;chains to find the insert point.andM;(c) Count of (recent) dequeues of lowest-valuedandM;request.andM;(d) Count of empty anchors skipped (recently) whileandM;scanning forward through the table (to find lowest-valued request).andM;(e) Count of requests currently in the queue.andM;This data is checked once every thirty seconds to see if the width-of-day orsize-of-table should be changed.andM;4.andP;  Our goal is to have the width-of-day set so that, onandM;the average, we are dequeuing (for expiration) five requests from each &quot;day&quot;of the table before advancing to the next day.andP;  If the statistics show thatwe are above one hundred and eighty percent of that or below fifty-fivepercent of it then we consider halving or doubling the width.andM;5.andP;  We also keep statistics on how often we make aandM;change.andP;  Re-organizing the table causes quite a bit of overhead, so we do notwant to oscillate back and forth every thirty seconds between the same twoday-widths.andP;  The decision to make a change is in two parts: first, to decideif changing the day-width or table-size might reduce overhead; and second, todecide if making the change is worth the effort.andM;6.andP;  We allow the table to grow, but not to shrink.andP;  AfterandM;the load on the system has peaked, the table size no longer changes.andO;However, we do continue to adjust the width of the day up or down asnecessary.andP;  As the load falls off after a peak, the width can increase toprevent the critical part of the table (the part just ahead of the cursor)from becoming too sparsely populated.andP;  It is true that if the distribution oftimes requested is very erratic (e.g., bimodal), then we might benefit fromallowing the table to shrink (to reduce the time spent in occasional longforward scans), but so far we have not found this to be necessary.andM;7.andP;  Before we will make the table bigger, two conditionsandM;must be met: (a) the density (queued requests divided by table size) mustjustify a bigger table (this is the only condition that Randy Brownrequires); and (b) the overhead of enqueuing a request must have exceeded athreshold.andP;  (The threshold is based, in a complex way, upon the targets ofeighty-seven and a half percent and criterium 5 mentioned earlier.)andO;Rationale: there seems to be no point in making the table bigger-even when itis very dense-if the density is not causing overhead.andP;  And we can hope thatour backward scan, when inserting into the sorted-linked lists, will keepoverhead low even when the table is densely populated.andP;  (Whether or not theseconsiderations have actually resulted in our table being smaller than itotherwise might be is unknown.)andM;8.andP;  In our multi-tasking, multiprocessing environment,andM;we can find that when a request is received, it is already due to expire.andP;  Inother words, we may receive requests that precede the &quot;current day&quot; cursor.andO;These expired requests cannot be readily accommodated by the generalmechanism.andP;  Fortunately, expired requests are very rare, so we just maintaina separate queue (a sorted-linked list) for them.andP;  We check this queue firstwhenever looking for the earliest event to dequeue.andP;  Except for theinsignificant activity at this special queue, our implementation should runin 0(1) time.andM;Gerald A.andP;  Davison IBM Corp., 48TA/914 Neighborhood Road Kingston, NY 12401andM;AUTHOR'S RESPONSEandM;I applaud Gerald Davison's letter, although I would perhaps dispute his maincontention.andP;  I applaud his letter because he describes the solution to amajor weakness of the calendar queue algorithm: the mechanism for choosingcalendar length and length of day.andP;  The calendar queue, as I described it,readjusts these parameters only when the queue size changes by a factor oftwo, which may be too seldom.andP;  One can visualize them needing change even ina situation where only hold operations are being performed.andM;In addition, gathering efficiency statistics, as he suggests, to aid inchoosing the new values seems to me to be worthwhile.andP;  The mechanism Isettled on seems to work, but it is essentially a guess, I do not know how toprove that it would always work well.andP;  Basing the new day length and calendarsize on the number of empty days recently encountered and the number of chainnodes recently examined per item enqueued or dequeued seems like a much moresolid mechanism.andP;  Finally, comparing the expected gain from a readjustment tothe cost of readjustment before making one seems sensible.andM;Before commenting on whether it was inevitable that this algorithm bediscovered soon, let me describe briefly how I came to create it.andP;  I wasscanning through back issues of Communications one day (recreational reading)when I came across Jones's [1] excellent article on priority queues.andP;  I didnot even know what a priority queue was until then, but I saw immediatelythat a desk calendar solves the event-set problem in 0(1) time, and myintuition was outraged by the thought that a computer could not also be madeto do it in 0(1) time.andP;  I felt sure that if I applied myself to the problem,I could do better than was currently being done.andP;  Since I was seekingpublications in order to acquire tenure, I began work on the problemimmediately.andM;When I began work on the problem in earnest, however, I discovered it wasmore difficult than I had anticipated (as is frequently the case).andP;  Theproblems of keeping the number of days in a calendar and the length of dayappropriate, and of handling overflow all in 0(1) time soon presentedthemselves.andP;  I prayed over the problem, asking the Lord's help, and beganmentally examining possible solutions.andP;  After several days work, I arrived atessentially the published algorithm.andM;In my case at least, the discovery of the algorithm did not come from anextensive knowledge of the literature.andP;  Nearly everything I knew aboutpriority queues came from Jones's article.andP;  The driving force behind my workwas a strong intuition and a good understanding of data structure techniquesin general, If calendar queues are an invention whose time has come, then thenecessary foundation is the general state of a computer science plus theinformation in Jones's article, It is not obvious to me that these weresufficient to guarantee discovery of the algorithm.andP;  I will note, however,that an article such as the one written by Douglas Jones is a powerful aidand incentive to invention.andP;  I would like to take this opportunity to thankhim for his article.andM;Since the algorithms Gerald Davison and I discovered are so nearly identical,in spite of having considerable complexity, it may be that the requirementsof the problem do determine the structure of the solution rather precisely.andO;I would like to complement Gerald Davison on his excellent work.andM;Randy Brown Dept.andP;  of Electrical Engineering 3179 Bell Engineering CenterUniversity of Arkansas Fayetteville, AK 72701 DISK FRAGMENTATION AND CACHEMACHINES It is difficult to analyze disk (and disk cache) performance so thatthe results are repeatable and useful in the real world.andP;  It is my experiencethat the layout of directory blocks and files on a hard drive is both(seemingly) random and very important; often, performance testing does nottake this into account.andP;  For example, performance improvements which show upin a test program may be much lower than the improvements which occur in reallife in a highly fragmented and almost full hard disk drive.andP;  In such cases,seek time can be greatly exaggerated over the case of essentially contiguousfiles present on empty, unfragmented drives.andP;  It is my informal observationthat in the real world, seek times are more important than I/O starts and I/Oprocessing times.andP;  Please note that none of the commercially availablede-fragmentation programs help with this problem: the newly contiguous filesmay still be many tracks away from each other.andM;The Flash cache software mentioned by Jalics and McIntyre (Communications,Feb.andP;  1989, pp.andP;  246-255) has some important features which can help reduceseeking enormously.andP;  Flash has a &quot;sticky&quot; mode, during which blocks read intothe cache stick there and are never unloaded.andP;  Whenever I boot my PC, I do acomplete directory scan with set Flash in sticky mode.andP;  Thereafter, I neverpay seek penalties when accessing the directory structures on my disk.andP;  Thisspeeds file access not only at file open time, but when reading largefiles(where additional file layout information must be accessed).andM;Another Flash feature with which Jalics and McIntyre seem unimpressed is theability to specify files to be loaded into the cache.andP;  I often pre-load myeditor, compiler, and linker at boot time, which speeds up the load times forthese frequently used utilities.andP;  While the following comment may be somewhatoutside of the context of their article, one thing which they seem to havemissed about Flash is that it is not strictly a cache: it offers other,related features which can increase system performance.andM;Jalics and McIntyre seem to feel that write caching is not important, but itcan help in three ways.andP;  First, in cases where a directory block is beingwritten repeatedly (as when a series of files are being copied into adirectory), that block is actually only written once.andP;  This one effect can bealmost magical when copying to a floppy.andP;  Second, while deferred writescannot eliminate the processing required to do the actual write, the programdoes not suffer performance penalties during the time required to seek theblock to be written, only for the actual write.andP;  Finally, even though totalprocessing time may be no less with write caching, the user of an interactiveprogram may appreciate getting control back immediately even if the actualphysical writes have not yet been performed.andM;Jalics and McIntyre's comments about cache size are useful, but then I wouldnever consider using a cache smaller than the programs I run (I use a 1.5MBcache).andP;  This brings up a related problem in the latest crop of 80386computers: they often offer huge main memories (RAM) with trivial caches(64KB) which cannot be expanded.andP;  I know of one text editor which performstext insertions by block-moving text to make room for the insertion.andP;  Thissingle (and frequent) operation can clear the entire cache, eliminating itsusefulness in an instant.andP;  The lessons which Jalics and McIntyre offer needto be better appreciated in other related arenas.andM;They seem to feel that caching does not help compilations significantly, butI disagree.andP;  For one thing, I preload my compiler into cache, as describedabove, insuring rapid loading.andP;  Another issue is language-related.andP;  I useModula-2, a language which encourages the generation of many softwaremodules.andP;  When you reference a tool module from within an application module,the compiler must get the interface specifications for the tool module; eachsuch specification resides in a different file.andP;  Thus, if one is re-compilinga given application module repeatedly, having the interface specificationfiles in the cache speeds up compilations.andP;  The key issue here is that insome languages, more files must be referenced than just the source and objectfiles.andM;Thank you for printing an article such as that written by Jalics andMcIntyre.andP;  The Communications is often both dry and marginally applicable tothe &quot;real world.&quot; I encourage you to keep printing articles with relevance tothose of us &quot;in the trenches.&quot;andM;Jon Bondy President, JLB Enterprises, Inc.andP;  Box 148 Ardmore, PA, 19003andM;AUTHORS' RESPONSEandM;We agree with Mr.andP;  Bondy that disk fragmentation can be a very substantialproblem.andP;  Our goal was to repeat experiments in as repeatable an environmentas possible and concentrate on only one aspect: caching.andP;  We do agree that1/0 starts under a primitive operating system like DOS have relatively smalloverhead when compared to full-scale operating systems with multiprogramming.andM;While we believe caching mechanisms should be transparent to the user, we donot for a moment denigrate the performance potential of sticky modes forfiles, or preloading certain files into a cache.andM;We did not say and do not mean that write caching is unimportant.andP;  Itcertainly produces impressive results in the hardware cache and, as Mr.andO;Bondy indicates, in some software cache applications as well.andP;  However, whatwe did say is that write caching with software caches is not as important aswe had thought.andM;Mr.andP;  Bondy's comments regarding hardware caches on 80386 computers are welltaken.andM;Regarding compilations, we do not say that there is no potential there.andP;  Whatwe do say is that we tried it on one high-speed Cobol compiler and that thesavings were unimpressive.andM;Paul J Jalics David R.andP;  McIntyreandM;James J.Nance College of Business Administration Dept.andP;  of Computer andInformation Science Cleveland State University 2121 Euclid Avenue Cleveland,OH 44115andM;LINDA IN CONTEXTandM;In the recent Communications [1] Carriero and Gelernter compare Linda to &quot;theBig Three&quot; families of parallel languages: concurrent obj ect-orientedlanguages, concurrent logic languages, and functional languages.andP;  While Iaccept many of their statements concerning concurrent object-orientedlanguages and functional languages, I find the comparison with concurrentlogic languages misguided.andM;Linda = Prolog - Logic + ConcurrencyandM;Carriero and Gelernter like to think of Linda as very different from all thethree models they explore.andP;  I do not think this is so and would like tosuggest that Linda incorporates fundamental aspects of the logic programmingmodel, although perhaps in a degenerate form.andM;Specifically, observe that the content of a Tuple Space, i.e., a multiset oftuples, can be represented directly by a multiset of unit clauses (alsocalled facts), and that the basic tuple space operations of Linda, out andin, are variants of Prolog's database manipulation operations assert andretract over a database of facts; in is a degenerate form of retract in thesense that it uses a home-brewed form of matching, rather than fullfledgedunification.andP;  The rd operation on a Tuple Space similarly corresponds toquerying a database of facts; it also uses matching rather than unification.andM;Assert and retract save concurrency and are the major extra-logicaloperations in Prolog.andP;  That is essentially all that Linda is offering; hence,the title of this section.andM;The remaining difference between Linda and the database manipulationcomponent of (sequential) Prolog is concurrency: Linda allows concurrentTuple Space operations, whereas, by definition, sequential Prolog issequential.andP;  Concurrent execution in Linda means, first, that the in and outoperations are atomic, and second, that in suspends if a matching tuple isnot found.andP;  This is in contrast to retract in sequential Prolog, which failsin such a case.andM;Database-oriented versions of Prolog, such as LDL [3], are enhanced with theconcept of transactions to support concurrency control of database update.andO;However, to my knowledge no one has attempted to use assert and retract as abasic communication and synchronization mechanism, as is done in Linda.andM;Research in concurrent logic programming has taken a different approach.andO;Concurrent logic languages do not incorporate extra-logical primitives likeassert and retract.andP;  Instead, they specify concurrency and communication bypure logic programs, augmented with synchronization constructs.andM;The Dining PhilosophersandM;Carriero and Gelernter did not have to set up a strawman for the purpose ofcomparing Linda with concurrent logic programming-Ringwood, inCommunications, (January 1988) has done this job for them [4].andP;  With itsseventy lines of (incomplete) code, six procedures, and four illustrativefigures, Ringwood's Parlog86 program for solving the dining philosophersproblem is indeed an easy mark.andP;  However, this program is not an appropriatebasis for comparison with concurrent logic languages.andP;  The reason is that adining philosopher can be specified by a much simpler concurrent logicprogram:andM;phil(Id, [eating(LeftId, done) I Left], Right)andM;andless;- phil(Id, Left, Right).andM;% Left is eating, wait until he is done.andM;phil(ld, Left, [eating(RightId, done) I Right])andM;andless;- phil(Id, Left, Right).andM;% Right is eating, wait until he is done.andM;phil(Id, [eating(Id, Done) ! Left] arrow pointing upandM;[eating(Id, Done) ! Right] arrow pointing up)andM;.  .  .  eat, when done unifY Done = done,andM;then think, then become:andM;phil(Id, Left, Right).andM;% Atomically grab both forksandM;The program is independent of the number of philosophers dining.andP;  A dinner ofn philosophers can be specified by the goal:andM;phil(l, Forkl, Fork2), phil(2, Fork2, Fork3),andM;.  .  .  , phil(n, Fork n, Forkl)andM;whose execution results in each of the Fork variables being incrementallyinstantiated to a stream of terms eating(Id, done), with the Id's on eachFork reflecting the order in which its two adjacent philosophers use it.andP;  Forexample, a partial run of this program, augmented with &quot;eating&quot; and&quot;thinking&quot; components that take some random amount of time, on a dinner of 5philosophers, provided the substitution:andM;Fork l = [eating(l, done), eating(5, done),andM;eating(l, done), eating(5, -) ! -] Fork2 = [eating(l, done), eating(2, done),andM;eating(l, done), eating(2, -) [ -]andM;Fork3 = [eating(3, done), eating(2, done),andM;eating(3, done), eating(2, -) [ -]andM;Fork4 = [eating(3, done), eating(4, done), eating(4, done),andM;eating(3, done), eating(4, done) ! -]andM;Fork5 = [eating(4, done), eating(4, done), eating(5, done),andM;eating(4, done), eating(5, -) ! -]andM;The run was suspended midstream in a state in which Fork4 is free and thesecond and fifth philosophers are eating using the other four forks.andP;  Up tothat point, each of the philosophers ate twice, except the fourth, which atethree times.andP;  (This output was obtained by running this program under theLogix system [10], a concurrent logic programming environment developed atthe Weizmann Institute.)andM;The program is written in the language FCP(arrow pointing up) (a variant ofSaraswat's FCP(arrow pointing down, !) [5]).andP;  The unannotated head argumentsare matched against the corresponding terms in the goal atom, as in Parlog86and GHC [14].andP;  In contrast, the T-annotated terms are unified with thecorresponding terms, as in Prolog.andP;  A goal atom can be reduced with a clauseif both the input matching and the unification succeed.andP;  The reduction of agoal with a clause is an atomic action: it either suspends or fails withoutleaving a trace of the attempt, or it succeeds with some unifyingsubstitution, which is applied atomically to all the variables it affects.andM;The key to the simplicity of the program is indeed the ability of FCP(T) tospecify atomic test unification: the philosopher atomically tries to grabboth forks, excluding other philosophers from grabbing them.andP;  The mutualexclusion is obtained by unifying the head of the Fork stream with a termcontaining the unique Id of the philosopher.andP;  (Atomic test unification isincorporated in all the languages in the FCP family, but not in Parlog86 orGHC.andP;  Programs exploiting atomic test unification cannot be written easily inthe latter two languages.)andM;The deadlock-freedom of the program is guaranteed by the language semantics;specifically, if several processes compete to unify the same variable(s), onesucceeds and the others will see the variable(s) instantiated (deadlock-freeatomic unification can be implemented using a variant of the two-phaselocking protocol; see [11, 12]).andP;  Like the Parlog86 and the Linda programs,this program is not starvation free.andP;  It should be noted, however, that theLinda and Parlog86 solutions can be made starvation free with the addition ofstandard fairness assumptions.andP;  This is not true for our solution, since twophilosophers sharing a common neighbor can conspire against this neighbor sothat his grabbing the two forks is never enabled, leading to his starvation.andO;If this is to be avoided, the FCP(T) program can be enhanced with the&quot;tickets&quot; method so that it achieves starvation freedom under similarfairness assumptions.andP;  The resulting program is very similar to the Linda oneand still much simpler than the original Parlog86 one.andM;Assigning unique identifiers to philosophers can be avoided if the languageis strengthened with read-only variables [6] or with a test-and-setprimitive.andP;  We consider the second extension.andP;  In FCP( arrow pointing up, !),andO;an !-annotated term in the clause head denotes an output assignment of theterm to a variable in the corresponding position in the goal.andP;  The assignmentfails if the corresponding position is a non-variable term.andP;  It can be mixed(but not nested) with T-annotations, and all assignments and unificationsshould be done atomically.andP;  In FCP(arrow pointing up, !), an anonymous diningphilosopher can be specified  by:andM;phil([eating(done) I Left], Right) andless;andM;phil(Left, Right).andM;% Left is eating, wait until he is done.andM;phil(Left, [eating(done) ! Right]) andless;andM;phil(Left, Right).andM;% Right is eating, wait until he is done.andM;phil[eating(Done) ! Left]!, [eating(Done) ! Right]!) andless;andM;.  .  .  eat, when done unify Done = done,andM;then think, then become: phil(Left, Right).andM;% Atomically grab both forksandM;A dinner of n anonymous philosophers:andM;phil(Forkl, Fork2), phil(Fork2, Fork3), .andP;  .  .  , phil(Forkn, Forkl).andM;is of course not as interesting.andP;  Each Fork variable is bound to a stream ofeating(done).andM;Embedding LindaandM;Sure the picture of Linda versus concurrent logic languages looks differentafter rehabilitating the dining philosophers.andP;  But is this the whole story?andM;Both in Linda and in concurrent logic programs, processes communicate bycreating a shared data structure.andP;  A stream of messages between twoconcurrent logic processes is just a shared data structure constructedincrementally from binary tuples.andP;  However, a shared structure in Lindacannot contain the equivalent of logical variables.andP;  The ability to constructand communicate structures with logical variables (incomplete terms,incomplete messages) is the source of the most powerful concurrent logicprogramming techniques.andP;  The proposed extension to Linda of nested TupleSpaces is yet another approximation-from-below of the power of the logicalvariable, as will be demonstrated shortly.andM;Another difference between Linda and concurrent logic languages is that Lindaincorporates no protection mechanism: any process can access any tuple in theTuple Space by reinventing the appropriate keywords.andP;  In contrast, sharedlogical variables cannot be reinvented and are unforgeable; and, therefore,they provide a natural capabilities and protection mechanism [2]: concurrentlogic processes can only access structures bound to variables they were givenexplicit access to.andM;Another difference is that in a concurrent logic language a shared structurecan only be increased monotonically (by further instantiating variables init) and can only be reclaimed by garbage collection when unreferenced.andP;  InLinda, tuples can be explicitly deleted from the shared Tuple Space, but thelack of discipline in Tuple-Space access makes garbage collection impossible.andM;The similarity between the cooperative incremental construction of logicalterms and Linda Tuple Space operations is perhaps best illustrated by showinga concurrent logic program implementation of Linda's primitives.andP;  Forsimplicity, the concurrent logic program represents the Tuple Space as anincomplete list of terms of the form tuple (OutId, InId, T).andP;  The operationsare defined, of course, so that several processes can perform themconcurrently on the same Tuple Space.andP;  Each in and out operation must begiven a unique identifier for the purpose of mutual exclusion.andP;  An outoperation with tuple T and identifier OutId inserts the term tuple (OutId, -,T) at the tail of the Tuple Space list.andP;  % out(Id, T, Ts) andless;- Tuple T insertedto Tuple Space TsandM;% with an out operation with identifier Id.andM;out(Id, T, [tuple(Id, -, T) I Ts] arrow pointing up).andM;% Found tail of Ts, insert T.andM;out(Id, T, [- !Ts]) andless;- out(Id, T, Ts),andM;% Slot taken, keep looking.andM;An in operation with tuple template T and identifier InId finds a termtuple(-, InId', T') in the Tuple Space such that T' unifies with T and InId'unifies with InId.andP;  % in (Id, T, Ts) andless;-Tuple T in Tuple Space Ts deleted by %an in operation with identifier Id.andP;  in(ld, T, [tuple(-, Id (arrow pointingup), T (arrow pointing up) T) !Ts]).andM;% Found unifiable tuple, delete it.andP;  in(ld, T, [tuple(-, Id', T') I Ts]) andless;andM;(Id, T), not equal (Id', T') !in(Id, T, Ts).andM;% Tuple deleted or doesn't unify, keep looking.andM;The precise interrelation between in and rd is not specified in the Lindaarticle.andP;  Assuming it is defined so that a tuple deleted from the tuple spaceby in should not be visible to any subsequent rd, the correspondingconcurrent logic program is: rd(T, [tuple(-, InId, T[arrow pointing up ) !andO;Ts])andless;andM;var(InId) ! true.andM;% Found tuple that has not been deleted yet.andP;  rd(T, [tuple(-, InId, T') !andO;Ts]) andless;andM;(foo, T) not equal (InId, T') !rd(T, Ts).andM;% Tuple deleted or doesn't unify, keep looking.andM;The program assumes that foo is not used as an identifier.andP;  It uses the varguard construct to check that InId is still a variable, i.e., the tuple hasnot been deleted by a concurrent in process.andP;  If weaker synchronization isallowed between in and rd, then the corresponding logic program may useweaker synchronization as well.andM;These programs can be executed &quot;as is&quot; under the Logix system.andP;  A Logix runof the following goal, consisting of several concurrent operations on aninitially empty Tuple Space Ts:andM;out(l, hi(there), Ts), in(2, hi(X), Ts), rd(hi(Y), Ts)andM;in(3, hi(Z), Ts), out(4, hi(ho), Ts) results in the following substitution:andM;X = thereandM;Z = hoandM;Ts = [tuple(l, 2, hi(there)), tuple(4, 3, hi(ho)) ! -] indicating that hi(there) was inserted to Ts with operation 1, deleted with 2 and hi (ho) wasinserted with 4 and deleted with 3.andP;  In this particular run, in(3, hi (Z),Ts) reached the second tuple before rd(hi(Y), Z), so the rd process remainssuspended.andM;If we execute an additional out operation:andM;out(5, hi(bye), Ts)andM;then both out and the suspended rd terminate, which results in thesubstitution:andM;Y = byeandM;Ts = [tuple(l, 2, hi(there)), tuple(4, 3, hi(ho)),andM;tuple(5, -, hi(bye)) ! -]andM;Note that the last tuple inserted has not been deleted yet, so its InId isstill uninstantiated.andM;This implementation uses genuine unification, rather than the home-brewedmatching mechanism of Linda.andP;  Hence, tuple templates and tuples in the TupleSpace may contain logical variables.andP;  Hence, this implementation can realizenested Tuple Spaces as is'.andP;  For example, the following goal sets up in Ts aTuple Space consisting of two nested Tuple Spaces, called tsl and ts2:andM;out(l, tuple -space(tsl, Tsl), TsandM;out(2, tuple -space(ts2, Ts2), Ts)andM;Tuple Space tsl can be retrieved into the variable SubTs using the goal:andM;rd(tuple -space(tsl, SubTs), Ts)andM;Once retrieved (detecting this requires rd to report successful completion;this is an easy extension), it can be operated upon:andM;out(l, hi(there), SubTs), in(2, hi(X), SubTs),andM;out(3, hi(ho), SubTs)andM;The result of these operations combined is: Ts = [tuple(l, -,tuple-space(tsi, [tuple(l, 2, hi(there)),andM;tuple(3, -, hi(ho)) ! Tsl'])), tuple(2, -, tupleandM;space(ts2, Ts2)) ! -] Tsl = SubTs = [tuple(l, 2, hi(there)), tuple(3, -,andM;hi(ho)) ! Tsl']andM;X = thereandM;Note that operations on a nested Tuple Space are independent of operations onthe parent Tuple Space and, hence, may use the same set of identifiers.andM;Using test-and-set, in and out can be implemented even more simply in FCP(l,!), without using unique identifiers.andP;  A Tuple Space is represented by anincomplete list of terms tuple (T, Gone), where Gone is instantiated to gonewhen T is deleted.andP;  out(T, [tuple(T, Gone) !Ts]!).andM;% Found tail of Ts, insert T out(T, [- ! Ts]) andless;andM;out(Id, T, Ts).andM;% Slot taken keep looking in(T, [tuple(T ( arrow going up)T, gone!) !Ts]).andM;% Found unifiable tuple, delete it in(T, [tuple(T', Gone) ! Ts]) andless;andM;(T, foo) does not equal (T', Gone) !in(T, Ts),andM;% Tuple deleted or doesn't unify keep lookingandM;Supporting eval in a language that already supports dynamic process creationseems redundant, so we do not address it.andM;The concurrent logic programs for out, in, and rd are concise, executablespecifications of the semantics of these operations.andP;  But they areinefficient since they use a naive data structure, i.e., a list, to realize aTuple Space.andP;  Their efficiency can be increased by applying the same ideas toa more sophisticated data structure such as search tree or a hash table, asdone in real implementations of Linda.andM;Embeddings of concurrent object-oriented languages and of functionallanguages in concurrent logic languages have been reported elsewhere.andP;  Thetrivial embeddings of Linda in FCP(T) and in FCP((arrow going up) !) arefurther evidence to the generality and versatility of concurrent logicprogramming.andM;Embedding as a Method of Language Comparison Language comparison is adifficult task, with no agreed upon methods.andP;  There are many &quot;soft&quot; methodsfor comparing languages, and we have seen them being used in arguments in thesequential world ad nausia.andP;  No reason why we should make better progresswith such arguments in the concurrent world.andP;  However, I know of oneobjective method for comparing and understanding the relations betweenlanguages and computational models: embedding (&quot;compiling&quot;) one language inanother and investigating the complexity of the embedding (it has beenextensively used for comparing concurrent logic languages [cf, 6]).andM;In a response to an earlier draft of this letter, Carriero and Gelernterimply that embedding one language in another is always such an easy andtrivial task, that showing its possibility for two particular languages ishardly interesting.andP;  This is far from being true.andP;  Try embedding Linda inOccam, in a parallel functional language, or in an actor language, and seethe mess you get.andP;  Conversely, try embedding these models (and someconcurrent logic language) in Linda.andP;  I doubt if the results of this exercisewill help in promoting Linda, but they will certainly increase ourunderstanding of the relationship between Linda and the rest of theconcurrent world.andM;The DNA ExampleandM;Another example in Carriero and Gelernter's article is the DNA sequenceanalyzer program.andP;  They show a functional Crystal program and a Linda programfor solving it and argue (1) that the functional program is only slightlypreferable on readability and elegance grounds, and (2) that the Lindaprogram is better in giving explicit control of parallelism to the programmerand in not relying on a sophisticated compiler to decide on its parallelexecution.andP;  I show (below) a concurrent logic program solving this problemthat is shorter, clearer, and specifies a more efficient algorithm than theother two.andM;The program specifies a &quot;soft-systolic&quot; algorithm along the lines of theverbal description of the problem by Carriero and Gelernter.andP;  (See [8] forthe concept of systolic programs and [13] for an analysis of theircomplexity.) An array process network is spawned, one cell process for eachpair (i, j), with near-neighbors connections.andP;  Each cell process receivesfrom the left the tuple (HI, PI) containing the h and p values fro neighbor,and from the bottom the tuple (Hbl, Hb, Qb) containing the bottom h and qvalues, as well as the diagonal (bottom left) h value.andP;  It computes the localvalues of the functions p, q, and h and sends them in corresponding tuples toits top and right neighbors.andP;  The top neighbor is also sent HI, the h valuefrom the left.andM;The matrix and row procedures realize the standard concurrent logicprogramming method of spawning a process array network [8).andP;  The matrixprocess is called with two vectors Xv and Yv and a variable Hm.andP;  Itrecursively spawns rows, with adjacent rows connected by a list of sharedvariables (the Ts and Bs of each row are its top and bottom connections).andO;Each row process recursively spawns cell processes.andP;  A cell process haselements X and Y, one from each vector, and it shares with its left, right,top, and bottom cell processes in its Left, Right, Top, and Bottom variables,respectively.andP;  During the spawning phase, the matrix and row processesconstruct the skeleton of the output of the computation, namely, thesimilarity matrix Hm.andP;  They provide each element, H, of the matrix to itscorresponding cell process, which instantiates H to its final value.andM;Although this schema for constructing recursive process networks may requiresome meditation the first time encountered, programs employing it can beconstructed almost mechanically and understood with little effort once theschema is grasped.andP;  % h(Xv, Yv, Hm) andless;- Hm is the similarity matrix of vectorsXv and Yv.andM;h(Xv, Yv, Hm) andless;andM;matrix(Xv, Yv, Hm, Bs), initialize(Bs).andM;matrix([X ! Xv], Yv, [Hv ! Hm] (arrow going up), Bs) andless;andM;row(X, Yv, Hv, (0, 0), Bs, Ts), matrix(Xv, Yv, Hm, Ts).andP;  matrix([ ], Yv, [(arrow going up), Bs).andP;  row(X, [Y ! Yv], [H ! Hv] (arrow going up), Left,[Bottom ! Bs] (arrow going up),andM;[Top ! Ts] (arrow going up) andless;cell(X, Y, H, Left, Right, Bottom, Top), row(X,Yv, Hv, Right, Bs, Ts).andP;  row(X, [], [] (arrow going up), Left, [] (arrowgoing up)[] (arrow going up))- initialize([(o, 0, 0) T I Bs]) andless;andM;initialize(Bs).andP;  initialize([ ]).andP;  cell(X, Y, H, (HI, PI), (H, P) T, (Hbl,Hb, Qb),andM;(HI, H, Q) T) (andM;compare(X, Y, D),andM;P := max(HI - 1.333, P1 - 0.333),andM;Q := max(Hb - 1.333, Qb - 0.333),andM;H := max(Hbl + D, max(P, max(Q, 0))).andP;  compare(X, X, 1(arrow going upT).andO;compare(X, Y, -0.3333 (arrow going up)) andless; - X does not equal to Y ! true.andO;For example, a run of the program on the initial goal h([a, b, a, a, b, c,d], [a, b, c, d, e], Hm) produces the answer:andM;Hm = [[l, 0, 0, 0, 01,andM;[0, 2, 0.667000, 0.334000, 0.001000],andM;[1, 0.667000, 1.666700, 0.333700, 0.000700],andM;[1, 0.666700, 0.333700, 1.3 33400, 0.0004001,andM;[0, 2, 0.667000, 0.334000, 1.000100],andM;[0, 0.667000, 3, 1.667000, 1.3 34000],andM;[0, 0.334000, 1.667000, 4, 2.667000]]andM;which shows that a match of length 4 is obtained at positions (7, 4).andM;The program is more efficient than the Crystal and Linda ones in tworespects.andP;  First, it avoids recomputation of the auxiliary functions p and q.andO;Although in principle, the Crystal program can be transformed into a moreintricate program that avoids the recomputation, a compiler capable ofperforming such a transformation has yet to be demonstrated.andP;  Similarly, theLinda program can be transformed (manually, I presume) to store both the Pand Q matrices in the Tuple Space, in addition to H, thus avoiding theirrecomputation.andP;  It seems that this transformation will adversely affectprogram length and readability.andP;  Second, the concurrent logic programspecifies a regular process structure with local communication only.andP;  Thisstructure can be mapped-manually, using Turtle programs [81, orautomatically-to a concurrent multicomputer in a way that near processesreside in near processors (or even in the same processor).andP;  Efficientlymapping the other two programs, which are not specified by a process networkand have no obvious notion of locality, is more difficult.andM;ConclusionsandM;I have shown that the semantic gap between concurrent logic programming andthe set of constructs offered by Linda can be closed with just six clauses.andO;This, I believe, is not because Linda was consciously designed with theconcurrent logic programming model in mind (or vice versa).andP;  Rather, findingthat an ad hoc construct for concurrency is easily embedded in the concurrentlogic programming model is a recurrent phenomenon.andP;  This was demonstrated invarious contexts for streams, futures, actors, monitors, remote procedurecalls, guarded synchronous communication, atomic transactions, and otherconstructs of concurrency [6, 7].1 believe that this is a consequence of, andthe best kind of evidence for, the generality and versatility of concurrentlogic programming.andM;Concurrent logic languages are not just abstract computational models,general and versatile as they may.andP;  They are real programming languages.andO;This may be surprising given their fine-grained concurrency.andP;  For example, inthe Logix session required to compile, test, debug, and produce the sampleruns of the programs shown in this note, 283,711 processes were created andthirteen minutes of Sun/3 CPU were consumed.andP;  (Since the Logix compiler andprogramming environment arewritten entirely in FCP, most of these processeswere created during system activities such as compilation, rather than duringthe actual runs of the programs.) See [11] for a report on the parallelperformance of FCP.andP;  A fair estimate is that more than 50,000 lines of FCPcode were developed using the Logix system, including the Logix system itselfand numerous applications [7]; and Logix is just one of several practicalconcurrent logic programming systems.andP;  However, presently concurrent logiclanguages offer a gain in expressiveness at the cost of a loss inperformance, compared to conventional approaches to concurrency.andP;  The timewhen this loss in performance is small enough to be easily compensated by thegain in expressiveness has yet to come.andM;Acknowledgmen ts.andP;  I thank Nicholas Carriero, David Gelernter, Ken Kahn, AmirPnueli, and Bill Silverman for their comments on an earlier manuscript.andM;Ehud Shapiro Dept.andP;  of Applied Mathematics and Computer Science The WeizmannInstitute of Science Rehovot 76100, Israel I was not convinced that Linda [3]has any advantages over traditionally message-oriented systems or monitors ontoday's parallel systems.andP;  To convince the market that a product is superiorit must provide either superior run-time performance or a far simplifiedprogramming model for not much less performance.andP;  In addition, it is becomingnecessary to provide these capabilities over a large range of hardwareplatforms.andM;Many of the benefits attributed to the tuple space by Carriero and Gelernterexist in most buffered message passing systems:andM;If we think of communication as a transaction between separate programs orprocesses-a transaction that can't rely on standard intra-program mechanismslike shared variables and procedure callsthen communication is a fundamentalproblem for which few unified models exist.andP;  Two processes in a parallellanguage may communicate; a program in one language may use communicationmechanisms to deal with a program in another language; a user program maycommunicate with the operating system; or a program may need to communicatewith some future version of itself, by writing a file.andP;  Most systems classifythese events under separate and unrelated mechanisms, but the tuple spacemodel covers them all [3, p.andP;  445].andM;The occam/CSP model of communication encompasses all these features withinthe language, without resorting to a computation and communication-intensivesuperstructure [5, 6, 10].andP;  Since occam is built around the concept of safe,reliable communication, all process-to-process, program-to-program,programto-operating system, and program-to-file systems are handled throughthe built-in communication primitives.andP;  As for different types of languagescommunicating, the only way they are allowed to share data is through messagepassing.andP;  Since each language has its own requirements for datarepresentation in stack frames, the easiest way to coordinate differentlanguages is through message passing.andM;The basic messaging system defined in occam does not allow for bufferingsince all the message passing is handled at the hardware level.andP;  Thisrestriction does require blocking-send communications with a result of wellsynchronized code.andP;  If the algorithm does not need to be synchronized, bufferprocesses may be introduced to allow non-blocking message sends.andM;Messaging environments such as Express, Trollius, or Helios provide theequivalent of the buffering and routing capabilities of Linda, with muchlower overhead [2, 8, 9].andP;  Express even offers parallel task invocation, inthe form of an interrupt routine, on receipt of a message of a given type[7].andM;I am stressing the run-time overhead of Linda since performance is the keyissue in production programs on parallel processing systems.andP;  Although theauthors have dismissed monitors because the normal method of implementationrequires a single address space and a synchronous procedure call to getaccess to the data protected by the monitor, they have effectivelyimplemented a monitor in Linda.andP;  Linda only moves these problems to adifferent location but still implements a monitor to handle the tuple space.andM;On a distributed memory environment, such as is needed for a Transputer-basedor hypercube system, each processor must have a monitor task and a messagerouting task operating in parallel to the user's tasks.andP;  The followingdescription is from Cornell's implementation of Linda, called Brenda, and isbased heavily on Carriero and Gelernter's work at Yale [1].andM;Brenda operates by having a server process running on each processor that mayhave objects stored in its address space.andP;  The library functions sendmessages to these servers and receive replies.andP;  In other words, a Brenda callis a classic Remote Procedure Call (with some twists).andP;  The storage processoris determined by a randomizing (hashing) function of the tag.andP;  When an objectis to be grabbed, the request is sent to the server on only one processor, asdetermined by the same hashing function.andP;  In a system with many parallelprocessors, the execution of one Brenda function typically involves thetransmission of several messages over a path that passes through severalprocessors.andP;  Some tricks are utilized to reduce the number of messages passedwhenever possible .andP;  .  .  The speed of execution of Brenda functions, ascompared with the speed of sequential computation, is a crucial parameter.andO;It determines what algorithms should be chosen for a given task, or whetherBrenda is suitable for the task at all.andP;  The execution of one Brenda functioncall is typically an order of magnitude slower than passing one message tothe nearest neighbor.andP;  It takes on the order of 10 milliseconds in thecurrent implementation.andM;In addition to the communication delay, the processor which is handling thetuple space is performing a non-trivial amount of work to search, update,garbage collection, and, otherwise, maintain the tuple space.andP;  This work mustsignificantly reduce the overall performance of the system.andP;  If it did not,why build custom hardware to handle the tuple-space functions?andM;One aspect of the tuple space that was not discussed in great detail in thearticle by Carriero and Gelernter [3] was the reading of values from thetuple space.andP;  Whereas the insertion of values into the tuple space isnon-blocking, retrieval of the data is and can become a bottleneck that hasno equal in a well designed algorithm on a message-based system.andP;  Withdistributed memory, to read a tuple from the tuple space, a message must besent to the appropriate processor; the tuple space must be searched for amatch; the required tuple must then be sent to the requesting process whichcan then continue, In a message-based system, the algorithm can beconstructed so that the required data is in a buffer in the processor whenneeded, The task need only input from the buffer, without the overhead of therequest/response message and the database search.andP;  The difference inperformance of the approaches may be best seen when some data must be sharedglobally, such as in a simulation.andM;In Linda, one process would insert a value into the tuple space, and allother processes would then read the value.andP;  With message passing, a broadcastwould be used.andP;  Examining the message traffic alone on a 64processorhypercube, we find that the broadcast requires only sixty-three messages andonly six message transmission times.andP;  With Linda, each process would have tosend a request and wait for a response.andP;  Assuming a simple implementation,this would generate, on average, three message hops to and from the processorwith the data for each of the requesting processors.andP;  Each processor wouldhave to wait six message hop times and a total of more thanthree-hundred-sixty messages would be generated.andP;  Each communication port ofthe destination processor would have to handle, on average, ten incoming andten outgoing messages.andP;  All sixty-four requests would have to queue foraccess to the tuple space.andP;  The queuing may be reduced through a look-aheadmechanism, but the timeconsuming message passing is not easily avoidable.andO;Since local memory access times can be measured on the scale of microsecondsand message access is two to three orders of magnitude larger for mostsystems [9], the message traffic time will dominate the system performance.andM;It must be remembered that the applicability of a parallel system to a widerange of problems is heavily dependent on the efficiency in which theprocessors can coordinate and share data.andP;  In distributed memory systems,this is the efficiency of the message system.andP;  Applications are alreadylimited by the message system in many multi-processors; an order of magnitudeslower message system would make the problem worse.andM;The authors' claim that C-Linda results in code which is more readable thanParlog86 or Crystal is true, but that it is different from message-basedsystems is not true.andP;  The out command of Linda can easily be replaced by amessage send and the in by a message read.andP;  Compare the following occam codewith the CLinda example for the Dining Philosophers Problem.andP;  Note that inthe occam program [11] the equivalent of the tuple space manager requiresonly twenty-four lines of code.andP;  The remaining program requires fewer linesof code in a lower-level language than C.andP;  Also, note that no call to amysterious initia1ize ( ) routine of unknown length is required,andM;PROC philosopher (CHAN OF INT left,andM;right, down, up) WHILE TRUEandM;SEQandM;.  .  .think down ! 0 - get your room ticket PARandM;left ! 0 - get the leftandM;chopstick right ! 0 - and the right atandM;the same time ...andP;  eat PARandM;left i 0 - put down theandM;chopsticks right ! 0 up ! 0 - and leave the roomandM;:- The following 2 routines implement the 'tuple space' PROC chopstick (CHANOF INT left, right)andM;WHILE TRUEandM;INT any: ALTandM;left ? any - phil to leftandM;gets it left ? any right ? any - phil to rightandM;gets it right ?anyandM;:PROC security ( [ ]CHAN OF INT down, up)andM;VAL INT max IS 4: INT n.sat.down: SEQandM;n.sat.down := 0 WHILE TRUEandM;INT any: ALT i = OFOR5andM;ALTandM;(n.sat.down andless; max) andamp;andM;down [i] ? any n.sat.down := n.sat.down +1 up[i) ?anyandM;n.sat.down :=andM;n.sat.down -1andM;: - the main part PROC secure.phil ()andM;[5] CHAN OF INT left, right, up, down: PARandM;security (up, down) PAR i = 0 FOR 5andM;PARandM;philosopher(left[i], right [i] , down[i] , up[i] ) chopstick(left[i] ,right[(i + 1)\5]) :andM;The stated benefit of not having to know about receivers of messages and viceversa in Linda is potentially dangerous.andP;  Much of the requiredsynchronization in message-based systems is to ensure complete and accurateexecution of the program at all times.andP;  By requiring the programmer to thinkabout the destination of the data and the overall data flow, algorithms canbe better optimized, and loose tuples will not be left in the machine atprogram termination.andP;  When designing the individual modules, the physicalsource and destination of the data need not be defined, only the messagetype.andP;  At system integration, the task of allocating message flow is nodifferent from assigning names to the tuples in the tuple space, and therouting can be accomplished through a standard messaging interface.andP;  Theoverall difference in the approach is that of designing a system instead ofhacking it together.andM;Carriero and Gelernter also did not define what &quot;good speedup&quot; is when theyran Linda on a 64-node iPSC hypercube.andP;  How does their speedup compare withthat obtained by systems such as CrOS III from CalTech or its commercialequivalent, Express? Fox et al.andP;  also claim high efficiency for a largenumber of problems and state the values as being between eighty andninety-five percent [9].andM;Finally, what Gelernter and Carriero have not shown is that Linda offerseither a run-time performance advantage over a message-based system, or thatthe algorithm design, coding, and implementation is significantly easier.andO;What they have shown is that Linda does run on a number of hosts, but, to getadequate performance, custom (read expensive) hardware is needed.andP;  This is aproblem in a commercial environment with customers who are very pricesensitive.andP;  The currently available messaging systems offer higherperformance, more portability, and surprisingly similar programming models toLinda, and, as such, they are currently more commercially viable.andM;Craig Davidson Mechanical IntelligenceandM;922 Grange Hall Rd.andP;  Cardiff, CA 92007 I read Carrierro and Gelernter'sarticle, &quot;Linda in Context,&quot; in the April 1989 issue [2] with interest.andP;  Asone of the first pragmatic, machine-independent, parallel programminglanguages, Linda is worthy of much praise.andP;  However, as the theme of thearticle is a demonstration of the superiority of Linda and the tuple-spaceapproach over other extant approaches to parallel programming, I find itnecessary to point out a few errors of omission.andM;One approach that was omitted in their discussion is logic programming.andP;  Thisstatement may appear strangely superficial, as the authors devote asubstantial portion of the paper to CLP (Concurrent Logic Programming).andO;However, CLP is not at all the same as parallel execution of logic programs.andO;The CLP approach involves logic-based, explicitly parallel languages which(a) allow specification of processes with tail-recursive predicates and (b)use streams represented as partially instantiated &quot;Logical Variables&quot; forspecifying communication between processes.andP;  The logic-programming approach,in contrast, is an implicitly parallel one, much like the FunctionalProgramming approach considered in their article.andP;  Logic-programmingformulations are useful, among other domains, in symbolic-computationapplications involving search, backtracking, and problem reduction.andP;  Thereare over a dozen research groups, not counting the CLP-related groups,working on efficient parallel-execution schemes for logic programs.andP;  Theauthors may wish to use the same arguments against logic programming as theyused against functional programming, but this significant effort certainlydeserves consideration.andM;Functional and logic languages may not be appropriate for expressing certaintypes of computations.andP;  However, the authors contend that even for domainswhere computations can be naturally expressed in such languages, they are notappropriate parallel programming languages because they rely on an &quot;autopilot&quot;-the compiler and the run-time system that decide what to execute inparallel, when, and where.andP;  Such auto pilots, they argue, currently do notand probably can never yield as good a performance as an explicitly parallelprogram (written in Linda, say).andP;  However, this argument overlooks animportant possibility: allowing the programmers to annotate the logic orfunctional programs.andP;  The types of annotations must be chosen carefully.andP;  Inour experience, grainsize control and data dependences are two attributesthat can be specified fairly easily by programmers.andP;  For example, based onsuch annotations, we have recently obtained excellent speedups on differentmultiprocessors, with uniprocessor speeds comparable with sequential compiledProlog programs, in our parallel logic-programming system based on the REDUCEOR Process Model [6,8].andP;  Such annotations are qualitatively different thanLindafying Prolog (to borrow the authors' phrase) because they are highlynon-invasive.andP;  The annotated logic program still reads/feels very much like alogic program, and the programmor does not have to deal directly withmanagement of parallel execution.andP;  The futures of MultiLisp [5] and Qlets ofQLisp [4] are a few of the other examples of systems that use annotationseffectively,andM;My third point concerns the authors' arguments against a rather broadcollection of approaches they have categorized as &quot;concurrent objects.&quot; It isdifficult to rule out such a diverse set of approaches with arguments againsta few members.andP;  The points regarding object orientation being orthogonal toparallel programandM;ming and monitors being of limited use for parallel processing are welltaken.andP;  But, they do not constitute an argument against all the approaches inthe vast spectrum of formulations with synchronous or asynchronousmessage-passing processes, threads, concurrent objects, Actors, etc.andP;  Forexample, consider a language which allows dynamic creation of manymedium-grained processes, includes primitives for asynchronous communicationbetween them, and includes primitives for dealing with specific types ofshared data.andP;  The Chare Kernel parallel programming system we are developing[7] is an example of such a system.andP;  The shared data may be as simple asread-only variables or as general as dynamic tables.andP;  (Dynamic tables consistof entries with a key and data and allow asynchronous insertions, deletions,and lookups, with optional suspension on failure, to find an entry.andP;  Theseare similar, to some extent, to tuple spaces).andP;  None of the arguments in thissection argues against such a system or point out Linda's advantage over suchan approach.andP;  For example, the arguments against actors are mainly that (a)they do not support shared data structures and (b) do not allow messages tobe sent to notyet-created tasks.andP;  A system such as the Chare Kernel surmountsboth these criticisms.andP;  Because it explicitly supports dynamic shareddata-structures, it is possible to deposit data in such structures and tohave another new process pick that data later on.andP;  Of course, the authors maynot have known about this particular system.andP;  But they seem to be arguingthat all of the possible approaches in this broad spectrum are &quot;irrelevantfor parallelism.&quot;andM;In fact, the Chare Kernel-like approaches may have some advantages over Lindaon the basis of the authors' own criteria.andP;  The communication of data is muchmore specific in Chare Kernel.andP;  In particular, when a process A needs tocommunicate data to process B, it specifically directs the message to B; whenmany processes share a dynamic table, they may read and write to this sharedand distributed data structure.andP;  In contrast, in Linda, when a tuple isouted, it is not immediately clear what sort of sharing and communication isto take place.andP;  The programmers usually know what the sharing is to be, butthey are not allowed to express their knowledge in Linda, leaving the burdento the compiler.andP;  In a few cases that the programmers cannot specify the typeof communication, it is reasonable to use the most general mechanisms.andP;  Thecompiler-their own auto pilot-cannot be expected to be perfect regarding this(and even if it were, it would be unnatural for programmers to &quot;hide&quot; thisinformation from the system), Yet, knowledge of communication patterns isessential for efficient implementation on message passing machines.andP;  Althoughthere is a large body of Linda applications reported in literature, thesubset for which the performance data on iPSC/2 hypercube is available isfairly small and consists of simpler programs.andP;  This may simply be indicativeof the (transient) state of development of that system.andP;  However, I expectthat when they attempt to port all the other Linda programs to the hypercubeand/or write more complex Linda programs with more complex communicationpatterns to run on hypercube, the difficulties mentioned above will surface.andO;It is not clear whether the Linda approach can be made to be viable in thatsituation.andM;The work of Athas and Seitz [1] on Cantor and Dally's work [3] on ConcurrentSmalltalk on the &quot;JellyBean machine&quot; represents other interesting extensionsto the actors and object-oriented models that are viable in a largefine-grained multi-computer (and they are both constructing such machines).andM;To summarize, (a) logic programming is an approach toparallel programming onpar with functional programming and is distinct from concurrent logicprogramming; (b) the objections raised against efficiency of parallelexecution schemes for functional (or logic) languages can be surmounted withsimple annotations.andP;  So, such schemes present fairly effective approaches inapplication domains where they naturally apply; and (c) appropriateextensions of message passing or Actor-like systems do not suffer from thedisadvantages attributed by the authors to specific systems within this broadclass.andP;  In fact, the uniformity of tuple-space operations may be detrimentalbecause it passes on the burden of identifying the type of communication tothe compiler, while forcing the programmer to &quot;hide&quot; that information fromthe system.andM;Linda does make a significant contribution.andP;  The tuple space approach is aunique and interesting way of programming which, in addition, can lead togood speeds and speedups over the best sequential programs, onmultiprocessors available today, Only, the &quot;context&quot; of other competingapproaches to parallel programming is much richer than the authors argue.andM;L.V.andP;  Kale Dept.andP;  of Computer Science University of Illinois at UrbanaChampaign 1304 W. Springfield Ave.andP;  Urbana, IL 61801  The article, &quot;Linda inContext,&quot; by Carriero and Gelernter in April's Communications, pp.andP;  440-458,presents an interesting model for parallel computing.andP;  Much of the article isa comparison with other approaches.andP;  Much of this comparison looks likeattacks on concurrent logic programming (along with actors, concurrentobject-oriented programming, and functional programming), The followingparagraph from the conclusions is milder than most of the body of thearticle.andM;Logic programming is obviously a powerful and attractive computing model aswell.andP;  Notwithstanding, we continue to believe that the tools in concurrentlogic languages [sic] are too policy-laden and inflexible to serve as a goodbasis for most parallel programs.andP;  Applications certainly do exist that lookbeautiful in concurrent logic languages, and we tend to accept the claim thatvirtually any kind of parallel program structure can be squeezed into thisbox somehow or other (although the spectacle may not be pretty).andP;  But webelieve that, on balance, Linda is a more practical and a more elegantalternative.andM;This article has generated a flurry of electronic discussions in which manyparticipants argue for their own particular hobby horse and are deaf to whatthe others are saying.andP;  In this note, we attempt to step back from thesereligious platforms to extract the good ideas from the different approachesas well as to understand their relationships.andM;Different ProblemsandM;Linda and concurrent logic programming seem to be solving two differentproblems.andP;  Linda is best not thought of as a language-but rather as anextension that can be added to nearly any language to enable processcreation, communication, and synchronization.andP;  (For very clean languages suchas functional programming languages, adding Linda constructs may destroytheir clean semantics.) Linda is a model of how to coordinate parallelprocesses.andP;  As a solution to this problem, Linda seems pretty good.andP;  We donot see how logic variables and unification can be added to Fortran and C aswell as tuple spaces have been.andP;  The utility of Linda rests on twoobservations about the world we live in.andM;(1) There is a lot of programming language inertia.andM;There are lots of programmers who do not want to (or cannot) leave thelanguages they presently program in.andP;  Organizations prefer to minimize theretraining of their programmers.andM;(2) More and more applications are multi-lingual.andP;  MoreandM;and more programmers deal with multiple languages as they work.andM;Concurrent logic programming has traditionally been addressing anotherproblem-namely, how can one design a single language which is expressive,simple, clean, and efficient for general purpose parallel computing? It doesnot mix concurrent and sequential programming but, instead, takes the radicalstance that all one needs (and should ever want) is a good way to describeconcurrent computations.andP;  The challenges are to design and understand such alanguage, to provide useful formal models of the language, to implement itwell, and to discover important programming techniques.andP;  The great hope isthat this combination of being both semantically clean and real will enableradically new tools (e.g., partial evaluators, verifiers, algorithmicdebuggers, program animators, etc.).andM;The practicality of such tools may rest upon the simplicity of the language.andO;In principle, such tools could be built for C-Linda, Fortran-Linda, etc., butI expect they would be too complex to do in practice.andP;  The tools the Lindafolk have developed (such as the Tuple space visualizer) only deal with thepart of the computation which occurs in Linda.andP;  This is rather incomplete.andO;What is needed are tools that can deal with computation and communicationtogether as an integrated whole.andM;UniformityandM;Both camps are striving for uniformity but of different sorts.andP;  The CLP campstrives for uniformity within a language while the Linda camp strives foruniformity across languages.andP;  There is just one way to communicate in CLP,not as in Linda where there is the dialect specific way of communicating inthe small (e.g., procedure calls) and the tuple space way of communicatingbetween processes.andM;There has been research in embedding objectoriented, functional, andsearch-oriented logic programming languages into CLP by implementing them ina CLP language.andP;  This approach might be generalizable to deal withprogramming paradigm interoperability.andP;  At least programmers would be able toprogram in the style they wish even though existing programming languageswould not be supported.andP;  Research in how these embedded languages might beinter-callable remains to be done.andM;Someone who frequently programs in different conventional languages (C,Fortran, Lisp, etc.) may value most highly the uniformity that Linda offers.andO;As they move from language to language they can deal with process creation,communication, and synchronization in a uniform manner.andP;  From this point ofview, Linda should be compared with distributed operating systems like Machwhich also provide similar capabilities as system calls from differentlanguages.andP;  On the other hand, someone looking for a simple, yet real,concurrent programming language may be more appreciative of the uniformity inconcurrent logic programming.andM;Distributed Computing and Issues of ScaleandM;The Linda model is, in a sense, very high level.andP;  The model provides adecoupling between clients and servers by providing a shared global tuplespace.andP;  It is left up to clever compilers to avoid central bottlenecks and tocompile much of the code as point-to-point communication and specialized datastructures.andP;  The claim is that it is easier to think and program in thisdecoupled manner.andM;Linda cannot scale to large-scale distributed computing (open systems)without making yet another shift in computational model.andP;  It would seem thatLinda's practicality rests upon global compiler optimizations.andP;  As systemsget larger and more distributed, it seems implausible that one module whichadds a tuple of a certain form could be compiled knowing all the othermodules that read tuples of that form.andP;  A solution to this is to providemultiple tuple spaces and control their access.andP;  Indeed, at the end of thearticle, Carriero and Gelernter mention that they are building a version ofLinda with multiple first-class tuple spaces.andP;  We discuss this further below.andM;It is common practice to compare programming languages on the kinds ofaesthetic grounds that underly most of the electronic discussion sparked bythe Linda article.andP;  Shapiro (in a network conversation) proposes that theease and complexity with which one can embed one language within another isimportant.andP;  There are many subtle issues here.andP;  First, we do not see how anaive implementation of Linda in a CLP language demonstrates much.andP;  If thepoint is about how expressive CLP is and how these are real languages forimplementing systems in a serious way, then one needs to present a seriousimplementation.andP;  C-Linda, for example, relies on some sophisticated compileroptimizations based upon a global analysis of the restricted ways in whichclasses of tuples are used.andP;  A good embedding should be capable of suchoptimizations.andM;There are several other objective means of comparing languages besidesembeddings which we explored in the paper &quot;Language Design and Open Systems&quot;[1].andP;  These other criteria do not correspond to &quot;expressiveness&quot; as normallydefined.andP;  Rather, they relate to issues that appear in open systems (largedistributed systems with no global trust, information, or control).andM;Some of these issues are quite important with respect to a form ofexpressiveness which gets surprisingly little attention: modularity.andP;  Toquote from &quot;Logical Secrets&quot; [3]:andM;Trust is a form of dependency between entities; by reducing dependencies themodularity of the system is increased .andP;  .  .  Thus the degree of cooperationpossible among mutually mistrusted agents is one measure for how viable alanguage is in supporting [a certain kind of modularity].andM;In [1], we enumerate the following &quot;hard&quot; criteria:andM;(1) Be able to define servers that are immune from theandM;misbehavior of others,andM;(2) Be able to survive hardware failure,andM;(3) Be able to serialize simultaneous events,andM;(4) Be able to securely encapsulate entities with state,andM;(5) Be able to dynamically create and connect services,andM;(6) And finally, it must not rely on global constructs.andM;These criteria are hard in the sense that languages which are Turingequivalent can formally differ in their abilities to provide security and toreact to external events.andP;  A language which has no means of implementingrobust servers, for example, cannot simulate a language which can.andP;  Inaddition, we give the following &quot;soft&quot; criteria (soft in the sense that theseproperties are not strictly necessary at the distributed language level sincehigher levels of language can provide them):andM;* Can reprogram running programs,andM;* Can use foreign services not implemented in the disandM;tributed language,andM;* Can define services which can be used by foreignandM;computations,andM;* Is based upon computational principles which scale,andM;* Supports the expression of very highly concurrentandM;programs,andM;* Supports small scale programming well.andM;In (1] we explain why we believe actors and concurrent logic programminglanguages meet these criteria well (except (2)), and why functionalprogramming, Or-parallel Prolog, and CSP/Occam do not.andP;  With respect toassessing Linda for open-systems programming, there are really two Lindas:Linda in the small and medium which relies on a single shared tuple space;and Linda in the large which relies on multiple first class tuple spaces.andO;Let us call the first S-Linda and the second M-Linda (for Single versusMultiple tuple spaces).andM;S-Linda does not satisfy properties (1) or (6).andP;  The reason is that any otherprocess can &quot;in&quot; any of the tuples that a given server is trying to serveand, thereby, disrupt communication between that server and its client.andP;  Thisis clearly unacceptable for a secure distributed system but also makesreasoning about the correctness of programs a less local and modular affair.andO;One cannot show that a given subset of a Linda system will proceedsuccessfully without knowing at least something about the correctness of allcomputations participating in that Linda system.andM;M-Linda seems to satisfy all the criteria as well as Actors and CLP.andP;  InM-Linda, used for open systems, there would be at least one tuple spaceprivate to each trust boundary.andP;  Therefore, with regard to open systemproperties, we can regard all computation proceeding inside a single tuplespace as a single agent.andP;  Presumably, the way such multiple agents interactis by creating new tuple spaces, passing tuple spaces around, and havingprocesses that communicate to more than one tuple space.andM;The computational modelof M-Linda among these macro agents is not verydifferent from Actors (a tuple space is like a mailbox) or CLP (a tuple spaceis like a logic variable').andP;  M-Linda seems to have many of the &quot;flaws&quot; ofwhich the Linda researchers accuse messagepassing systems and CLP languages.andM;If the Linda people are effectively arguing that SLinda is superior to Actorsand CLP for parallel programming in the small and medium, while M-Linda givesyou the good properties of Actors and CLP in the large (at the &quot;costs&quot; ofActors and CLP) that would indeed be an interesting and significant argument.andO;It would, however, concede our point about uniformity.andM;Historical FootnoteandM;Linda very closely resembles Bill Kornfeld's Ether language from the late1970s [17].andP;  The major difference being that Ether had constructs equivalentto Linda's &quot;rd,&quot; &quot;out,&quot; etc.andP;  but not &quot;in.&quot;andM;Acknowledgments.andP;  Much of this note is derived from a network discussionwhich included Ehud Shapiro, Nicholas Carriero, Graem Ringwood, Ian Foster,and ourselves.andP;  We are also grateful to Jacob Levy, Vijay Saraswat, and DannyBobrow for their comments.andM;Kenneth M.andP;  Kahn Xerox Palo Alto Research Center 3333 Coyote Hill Road PaloAlto, CA 94304andM;Mark S.andP;  Miller Xanadu Operating Company 550 California Avenue Palo Alto, CA94306andM;AUTHORS' RESPONSEandM;Of the comments published here, we believe Kahn and Miller's to be by far themost interesting and significant, We have responded to Kahn and Miller-ourintent was not so much to refute their comments as to amplify them and toexplore their implications-in a brief separate paper called &quot;CoordinationLanguages and their Significance&quot; [2].andP;  We summarize the paper's main pointsafter addressing other comments.andM;We are glad that Professor Shapiro agrees with us on the topics of concurrentobject-oriented and functional languages, and we appreciate the candor of hisadmission that &quot;The time when this loss in performance [sustained byconcurrent logic languages as against systems like Linda] is small enough tobe easily compensated by the gain in expressiveness has yet to come.&quot; But, inour view, his comments add nothing significant to our comparison betweenLinda and concurrent logic languages.andP;  We will consider his main points in(what we take to be) descending order of significance.andM;Shapiro's embedding experiment is designed (presumably) to shed light on thisquestion: which interprocess communication operators are more expressive?andO;Linda communication is based on tuple spaces; in the concurrent logiclanguages Shapiro discusses, communication is based on streams constructedusing partially-instantiated logical variables.andP;  We have explained in detailwhy we believe that a shared associative object memory (a tuple space) is amore flexible and expressive medium than a CLP-style (Concurrent LogicProgramming) stream.andP;  Streams are merely one of a family of importantdistributed data structures; we need to build vectors and matrices, bags,graphs, associative pools, and (also) a variety of streams.andP;  We have arguedthat tuple spaces make a far simpler and more expressive basis for buildingthese structures than do streams based on logical variables.andP;  Roughlycomparable positions may explain why (for example) Ciancarini's Shared Prologeliminates stream-based communication in favor of a blackboard model relatedto Linda [3], and why Kahn and Miller remark that &quot;We think a promisingdirection to explore is a CLP-like language without logical variables which,instead, uses tuple spaces forcommunication.&quot; (These researchers will speakfor themselves, of course,-we merely present our own reaction to their work.)andM;Now, what does Shapiro bring to counter our claims? His argument seems to be,in essence: &quot;contrary to what you have said, CLP communication is moreexpressive than Linda; if you start with CLP, you have all the resources ofidiomatic CLP streams at your command, and if you want Linda, you can alsohave Linda-observe how easily we can implement Linda given CLP.&quot; But thiscontention as stated is transparently false.andP;  As Shapiro himself realizes,his CLP implementation of Linda is far too inefficient to be usable.andP;  (As hewrites, &quot;they [his embeddings] are inefficient since they use a naive datastructure, i.e., a list, to realize a Tuple Space.&quot;) It follows, that if aprogrammer working in the CLP framework indeed needs the expressivity ofLinda, Shapiro's embedding demonstration is irrelevant.andP;  He needs a realimplementation of Linda.andP;  A real-that is, an acceptablyefficient-implementation of Linda would (obviously) be vastly more complexthan the CLP implementation Shapiro presents.andP;  For one thing, &quot;C-Linda,&quot; asKahn and Miller point out in their note, &quot;relies upon some sophisticatedcompiler optimizations based upon a global analysis of the restricted ways inwhich classes of tuples are used.andP;  A good embedding should be capable of suchoptimization.&quot;andM;This point is so obvious that one might suspect Shapiro of having somethingelse (a deeper and subtler point?) in mind.andP;  We might imagine that his Lindaembedding is merely a kind of &quot;expressivity experiment.&quot; It is not designedto realize the functionality of Linda per se, but merely to demonstrate howexpressive CLP is by presenting an elegant solution to an interestingproblem.andP;  But the objection, again, hinges on the fact that Shapiro'sembedding represents a bad way to implement Linda, and claims regardingexpressivity are presumably to be based on good solutions to interestingproblems.andP;  The mere existence of some solution is uninteresting; theselanguages are all Turing-equivalent in any case.andM;One final, significant aspect of the embedding experiment: FCP is a completeprogramming language; Linda is a coordination language, dealing with creationand coordination of (black-box) computations exclusively.andP;  Shapiro hasattempted to embed the coordination aspects of C-Linda within (the completelanguage) FCP; a comparable experiment would attempt to embed thecoordination aspects of FCP within C-Linda.andP;  As we demonstrated in ourarticle, it is trivial to express CLP-style streams in C-Linda.andP;  But this&quot;embedding experiment&quot; yields code that is efficient enough to be used.andO;(Such structures are, in fact, used routinely in Linda programming.)andM;Shapiro's claims with respect to embedding remind us, in short, of what mightbe put forward by an enthusiastic exponent of Lego.andP;  It is true that you canbuild all sorts of elegant working models using Lego.andP;  But it is also truethat this piece of good news has not proved sufficient, in itself, toestablish Lego as a mainstay of the construction industry.andP;  Furthermore, Legostructures always wind up looking like assemblages of little boxes.andP;  That isfine if you are dealing with boxy structures.andP;  Otherwise, you might want toconduct your building experiments using a more flexible medium.andM;Turning to the dining philosophers example with which Shapiro begins-if wewere members of the CLP community, we would find this demonstration deeplyalarming.andP;  As we noted in our article, dining philosophers is neither anespecially important nor an especially difficult problem.andP;  The Linda solutionis trivial.andP;  In order to get a comparably concise solution in CLP, Shapironeeds to jettison Flat Concurrent Prolog (his own invention) and introduce usto the super-duper new logic language FCP(arrow pointing up), which is avariant of Saraswat's FCP(arrow pointing down), which is in turn a variant ofFCP.andP;  Shapiro seems unaware of the fact that when a simple problem is posed,introducing a new language in order to solve it can hardly strengthen ourconfidence in this new language's ability to handle the unanticipatedproblems that it will be expected to tackle.andP;  This same new language becomesthe basis of the CLP Linda embedding.andP;  But as we have already noted, theLinda embedding is not (so far as we are concerned) an impressivedemonstration of anything, and so we are left wondering what exactlymotivates the new language and whether it is the definitive CLP solution ormerely one of a long series.andP;  (We are happy to concede, for what it is worth,that FCP(arrow pointing up)'s dining philosophers solution is far moreconcise than Parlog86's.)andM;Finally, Shapiro's remarks on the DNA example are irrelevant.andP;  We gave Lindacode for one algorithm.andP;  Shapiro gives the CLP implementation of a differentalgorithm.andP;  Shapiro himself notes (and surely it is obvious) that we couldwrite a Linda program to implement this other algorithm as well.andP;  It istrivially clear that in order to compare the expressivity of two languages,we must control for the problem being solved.andM;We claimed, in our article, that Linda was, in general, more elegant, moreexpressive, and more efficient than CLP.andP;  Nothing in Shapiro's notechallenges this claim.andP;  We continue to believe that a Linda-based parallelProlog is potentially a more elegant, expressive, and efficient alternativeto the CLP languages Shapiro discusses.andP;  Several groups appear to becontemplating such systems, and we look forward to their results.andM;Other CommentsandM;We have no fundamental argument with Professor Kale's note.andP;  Obviously, wereject his claim that &quot;the uniformity of tuple-space operations may bedetrimental;&quot; the approach Kale has taken in his Chare Kernel is far toocomplicated for our tastes.andP;  As he intimates in his note, he has provided asystem in which there is a vaguely tuple-space-like communication constructand a large number of other devices as well.andP;  Although none of the others isstrictly necessary (all could be implemented using the tuple-spaceconstruct), the Chare authors believe that in providing them, they haveanticipated a significant set of special cases.andP;  This is the design approachthat has informed a series of important programming languages from PL/1through Ada and Common Lisp.andP;  Unfortunately, making simple things complicatedis not a cost-free exercise.andP;  You burden the programmer with a more complexmodel to master, the program-reader with more complicated sources, and theimplementer with a harder optimization problem.andP;  We choose to program inlanguages that provide optimized support for a few simple tools and then letus build what we need, But this is an aesthetic question; Kale is obviouslyentitled to his views, and we wish him luck in developing Chare.andP;  We applaudthe absence of ideological fustian in his note.andM;Davidson's note is full of errors and misrepresentations.andP;  The claim thatLinda &quot;implements a monitor to handle the tuple space&quot; is false.andP;  Inferencesabout Linda's performance or implementation drawn from Braner's work onBrenda are puzzling [cf.1, letter by Davidson on Linda; this issue].andP;  True,Moshe Braner has done interesting work on this Linda variant, but his systemexecutes in a different environment from our hypercube system, shares no codewith our system, is roughly an order of magnitude slower than our system[l]-and does not implement Linda in any case.andP;  In claiming that &quot;messagingenvironments such as Express, Trollius, or Helios provide the equivalent ofthe buffering and routing capabilities of Linda,&quot; Davidson appears not tounderstand that the point of implementing a tuple space is not buffering androuting, it is a shared, associative memory.andP;  Express et al, provide no suchthing, The claim &quot;to get adequate performance custom (read expensive)hardware is needed&quot; is false.andP;  Phrasing his comments in terms of what we mustdo to &quot;convince the market&quot; is silly, because (a) our point in this responsewas to address the computer science community, not the market, and (b) &quot;themarket&quot; is rapidly becoming convinced of Linda's significance of its ownaccord.andP;  Well over half a dozen companies are involved in Linda work at thispoint.andM;Kahn and Miller compare two fundamentally different approaches to parallelismin the following terms: &quot;The CLP [Concurrent Logic Programming] camp strivesfor uniformity within a language while the Linda camp strives for uniformityacross languages.&quot; Agreed; it is our view that this distinction in approachis a deep and important one for the evolution of programming systems.andM;We can build a complete programming environment out of two separate,orthogonal pieces-the computation model and the coordination model.andP;  Thecomputation model supplies tools for performing single-threaded,step-at-a-time computations-any ordinary programming language will serve thepurpose.andP;  The coordination model is a glue that can bind many separate,active computations into a single composite program, A coordination languageis the embodiment of a particular coordination model; a coordination languageallows us to create and to manage information exchange among a collection ofindependent, asynchronous activities which may be separated either by spaceor by time.andP;  Our point in introducing the term transcends nomenclature.andP;  Theissue is rather that Linda and other systems in its class are not mereextensions to basecomputing languages; rather, they are complete languages intheir own right, albeit coordination rather than computing languages; theyare independent of and orthogonal to, not mere extensions of, thebase-computing languages with which they are joined.andM;Approaching coordination as one of two orthogonal base axes that jointlydefine the space of programming environments clarifies the significance ofsystems like Linda and conduces to conceptual economy, allowing us to relatea series of superficially-separate programming problems ranging from filesystems and databases to distributed and parallel applications.andM;We refer to the orthogonal-axis approach as the C + C scheme (computationmodel plus coordination model).andP;  This approach has a series of what we regardas ' significant advantages.andP;  It supports portability of concept-themachinery required for explicit parallelism is always the same, no matterwhat the base language (to get parallelism, we must be able to create andcoordinate simultaneous execution threads).andP;  The C + C approach allows us totransform any base-computing language into a parallel language in essentiallythe same way-thus C.Linda, Fortran.Linda, Scheme.Linda, and so on.andP;  The C + Cscheme is granularity-neutral so far as parallel applications go-it allowsthe programmer to choose an appropriate granularity for his applicationwithout having granularity decided or limited by a programming model, Itsupports multi-lingual applicationsthe computations that are glued togetherby our coordination model may be expressed in many different languages, Theisolation of orthogonal elements it fosters tends to make natural theincorporation into programming environments of advances along either axis.andO;Finally, the C + C model is an expressive framework for building theinteresting class of applications that incorporate features of parallel,distributed, and information management systems; and the view of coordinationas a &quot;first class phenomenon&quot; promoted by the C + C scheme simplifies ourunderstanding of the computing environment and suggests new theoreticalframeworks for modelling it more comprehensively.andP;  (These claims areexplained and defended in [2].)andM;kahn and Miller, in effect, oppose the C + C model; instead, they advocatethe use of integrated parallel languages in which a uniform model applies tocomputation and also to coordination.andP;  We have given our reasons for favoringa different approach, but this disagreement cannot be resolved on objectivegrounds.andP;  We emphasize, that despite this disagreement, we view our own workas strongly allied to Kahn and Miller's; their work has significantlyadvanced our own understanding of coordination and its importance.andM;Nicholas Carriero David Gelernter Yale University Dept.andP;  of Computer ScienceNew Haven, CT 06520-2158 carriero (at signal) cs.yale.edu gelernter (atsignal) cs.yale.edu</TEXT></DOC>