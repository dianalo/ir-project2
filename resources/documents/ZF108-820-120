<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-820-120  </DOCNO><DOCID>08 820 120.andM;</DOCID><JOURNAL>AI Expert  Sept 1990 v5 n9 p43(5)* Full Text COPYRIGHT Miller Freeman Publications 1990.andM;</JOURNAL><TITLE>Using neural nets: diagnostic expert nets. (neural networks havebeen applied to a variety of systems as diagnostic experts) (part5) (includes related article on suggested reading) (technical)</TITLE><AUTHOR>Caudill, Maureen.andM;</AUTHOR><SUMMARY>Neural networks act as diagnostic experts in several applications.andO;Researchers Oleg Jakubowicz and Sridhar Ramanujam of SUNY Buffaloare developing a system to help organize circuit board testing andidentify circuit faults.andP;  The system includes a self-organizingKohonen layer, which maps the board's symptoms to allow comparisonbetween actual board results and the expected results.andP;  The SUNYnetwork also learned time-sequential and static faults.andO;Researchers in Milan, Italy designed a neural network to helpdiagnose epilepsy.andP;  The back propagation network was tested withmiddle layers consisting of 30, 50 and 100 neurodes, and the50-neurode network learned the test set best.andP;  The researchersalso 'pruned' neurodes with low associated weights and achievedbetter results.andP;  The DASA/LARS system is composed of severalnetwork modules that diagnose faults in satellite communicationsbroadcasts.andP;  Ford researchers designed the 'leave-k-out' trainingset definition, which leaves some variables out of the trainingset to be used for testing.andM;</SUMMARY><DESCRIPT>Topic:     Neural NetworksDiagnostic AssessmentCircuit DesignTestingMedical DiagnosisSatellite Communications.andO;Feature:   illustrationchart.andO;Caption:   Jakubowicz and Ramanujan diagnostic network for digital circuitboards. (chart)andM;</DESCRIPT><TEXT>Neural networks have been applied to a variety of systems as diagnosticexpertsandM;My last article in this series (&quot;Using Neural Networks, Part 4: Making AnExpert Network,&quot; AI EXPERT, july 1990, PP.andP;  41-45) reviewed two financialexpert-network applications.andP;  This month, in keeping with the issue's theme,I'll investigate how neural networks can be used as diagnostic experts in avariety of applications.andP;  The examples are taken from the last twoInternational joint Conferences on Neural Networks (IJCNN), held in Jan. 1990in Washington, D.C.andP;  and June 1990 in San Diego, Calif.andP;  (See the suggestedreading list for references to the original papers.)andM;Rather than describe each of these half-dozen or so systems in detail, I'llidentify the most salient features of each neural-network solution.andP;  A numberof important lessons can be found in these systems that will help anyonetrying to build a neural-network solution.andP;  One of the most interestingfeatures of these applications is the variety of systems that neural networkshave been applied to as diagnostic experts.andP;  Neural networks are being usedto diagnose everything from automobilc-engine problems to satellitecommunications faults to medical illnesses.andP;  The construction and trainingapproaches are equally varied and present an interesting range of methods andnetworks.andP;  TESTING DIGITAL CIRCUITS One exceptionally interesting use ofneural networks is to diagnose failures in digital and analog circuit cards.andO;A number of research groups are working on this problem and the variation inapproach is informative.andP;  Olegjakubowicz and Sridhar Ramanujam of the StateUniversity of New York (SUNY), Buffalo, have begun developing a systemintended to help technicians identify faults in circuit boards.andO;Traditionally, technicians begin with the output of the failed board andbacktrack through the board's data flow, constantly testing and retesting,until the failure is discovered.andP;  This task is tedious and demands highlytrained and expert technicians who must be familiar with the internaloperation of every kind o board.andM;Jakubowicz and Ramanujam have developed a neural-network system that will notonly help identify the failure but also direct technicians to the nextappropriate board test.andP;  To achieve this goal, they used an unusual networkarchitecture reminiscent of a counter propagation network, as illustrated inFigure 1.andM;The input layer of the network comprises a set of binary (on-off)neurodes.andO;The input layer has one such neurode for each symptom that can be exhibitedby the tested board.andP;  In addition, a series of other input neurodes identifypins on the board observed to be good or bad by the technician.andP;  This inputlayer is then fully connected to a second, self-organizing Kohonen layer (see&quot;Neural Networks Primer, Part IV&quot; AI EXPERT, Aug. 1988, pp.andP;  61-67).andM;The Kohonen layer is used to develop a self-organizing set of feature mapsthat map the various symptoms found in the board.andP;  The Kohonen network modelsthe input data in such a way that the input patterns are clustered intogroups of symptoms.andP;  During the training of this layer, the overall output ofthe network is disregarded.andM;Once the Kohonen middle layer is trained, the weights between it and theinput layer are frozen and training begins on the network's third (output)layer.andP;  In a traditional counterpropagation network, this output layer wouldbe a layer of outstars; in the Jakubowicz and Ramanujam system, the layer isessentially a back propagation network layer that uses a delta-rule trainingprocedure.andP;  Because only a single layer obeys this training rule, actuallyback propagating the error (as a typical back propagation network would) isunnecessary.andP;  Using the delta rule (as opposed to the outstar trainingprocdure) for training yields a least-meansquared error system that hasadvantages for this particular application.andM;The training procedure for the third layer involves presenting the inputpatterns to the network and letting the Kohonen layer generate itslearned-feature map for that symptom combination.andP;  This feature map istransmitted to the output layer where the resulting network output iscompared to the desired output.andP;  The standard delta rule is then used toadjust the weights.andP;  Essentially, the output layer's task is to learn theassociation between the symptom feature maps as well as the probable causesfor those symptoms.andM;This network design was also used to learn time-sequential and static faults.andO;In an approach reminiscent of the avalanche network (see &quot;Neural NetworksPrimer, Part VI,&quot; AI EXPERT, Feb. 1989, pp.andP;  61-67), the investigators foundtheir network could learn sequential faults by changing the wayoutput-neurode activity is computed.andP;  Normally, the activity for theseneurodes would be computed as:andM;nandM;y(t+1) = f([Sigma] Wi X i)andM;i=1andM;where y is the output of each neurode at time t+ 1, xi is the incoming signalfrom the middle layer over each of the n connections, and wi is the weight onthe incoming connection.andP;  The function f is normally a sigmoid, most oftenthe function:andM;f(I) =   1andM;--------1 + e[sup.-I]andM;The SUNY researchers modified the activation function like this:andM;nandM;y(t+1) = ([Sigma] Wi x i(t) + 0.5 * y(t)andM;i=1andM;In this modification, the total stimulation of neurodes at each tick of theclock is increased by 50% of the net activity of a neurode at the previoustick.andP;  (The exact percentage can be varied by changing the 0.5 factor in theprevious formula.) In the more traditional system, activity at each tickresults from the incoming stimulation arriving at that instant; activity fromprevious time periods is assumed to completely die off between ticks so thateach time period starts fresh.andP;  In effect, previous activity affects futureactivity, essentially providing the backpropagation layer with a &quot;memory&quot; ofwhat has gone before so historical information about the system isn't lost.andO;As with the avalanche network, this simple modification makes the networkable to learn sequential as well as instantaneous patterns.andM;Testing on the Jakubowicz and Ramanujam system has only just begun, but theresults on some simple systems have been excellent.andP;  The interesting featureof this system is not however, the researchers' results, but rather thecreative modification of existing networks to design a system moreappropriate for the problem.andM;PRUNING TRAINED NETWORKSandM;A similarly creative approach is apparent in a medical diagnostic systemdeveloped by researchers in Milan, Italy.andP;  Their system is designed to aidphysicians in diagnosing epilepsy.andM;Epilepsy is not a singular, easily identified disease with specific symptoms.andO;Many variations and syndromes are lumped into the category of  epilepsy.&quot;andO;Within this collection of conditions, clusters of diseases or syndromes areconsidered to be similar.andP;  The most accepted categorization technique for theillness is complex; it involves answering a large number of questions aboutspecific symptoms, patient history, and test results on a complicated recordsheet.andM;B.andP;  Apolloni, G. Avanzini, N. Cesa-Bianchi, and G. Ronchini jointly designeda neural network to classify patients according to this categorizationscheme.andP;  Their application is a relatively straightforward backpropagationnetwork in most respects, but their results and processing techniques arequite interesting.andP;  They began with a very large binary input vectorconsisting of 814 yes-no elements taken from the answers to thecategorization questionnaire.andP;  Their available data set consisted of 134training examples of well-defined classifications, plus 22 test examples of&quot;fuzzler&quot; classifications which were used during testing to confirm that thenetwork was able to correctly generalize from the well-defined cases).andM;The Italians used standard training procedures to train severalbackpropagation networks with middle layers of 30, 50, and 100 neurodes.andO;Once this development procedure was complete, they noticed something veryinteresting.andP;  First, the 30-element middle layer wasn't up to the task; itcouldn't learn all 134 of the training-set patterns.andP;  However, of theremaining two network sizes, the 50-neurode middle layer proved to be moreadept than the larger network at learning the test set.andP;  In other words, toomany neurodes in the middle layer inhibit rather than enhance propergeneralization.andP;  This result makes sense if you consider that the middlelayer is supposed to be the feature detector.andP;  If the network has too manyfeature detectors, it may tend to &quot;memorize&quot; cases with the middle-layerneurodes instead of using them to detect more global features.andM;The researchers also tried an interesting experiment with the trainednetwork.andP;  They pruned (removed) all neurodes in which all associated weightswere less than 0.5.andP;  (The 0.5 value was chosen based on the magnitudes of thefinal weights.andP;  Removing these neurodes reduced the size of the input layerfrom 814 to 724 neurodes; the 90 pattern elements corresponding to theremoved neurodes provided no significant information about the classificationof the cases.andP;  (They also confirmed that physicians found those questionresponses to be of little help in classifying a patient's disease.) They thentook this pruned network and retrained it on the training set.andP;  Their resultsare intriguing: the pruned network actually had a better classificationrecord than the initial one on the test-data set.andP;  It was correct 80% of thetime in the specific classification it applied to each test example, and 95%of the time in the classification cluster.andM;The significance of these results is that the system is not only a very goodclassifier of epileptic illnesses but also identifies portions of thequestionnaire that are probably unimportant for classifying the illnesses.andO;The Italians are collecting a more complete nationwide database to extendtheir system and enhance its performance.andM;COMPLEX DIAGNOSTIC NETWORKSandM;Neither of the diagnostic systems I've described are operational; both arestill very much in the laboratory.andP;  But on-line diagnostic networks do exist;one was built by Fred Casselman of GTE in association with Major Jody D.andO;Jonghe Acres of the Defense Communications Agency.andP;  This system, calledDASA/LARS, is currently in operation at Ft.andP;  Detrick, Md.andM;The problem DASA/LARS addresses is the, fault diagnosis ofsatellite-communications broadcasts.andP;  In essence, the system is composed ofnine neural networks that take their input information from two key sources,a spectrum analyzer and a large database.andP;  In satellite communications, manycarrier bands carry a large number of broadcasts simultaneously.andP;  Authorizedcarriers are assigned specific broadcast characteristics, which includesfrequency, modulation, data rate, transmission filters, and so on.andP;  Thisinformation is stored in a database accessible to DASA/LARS.andM;The first step in processing the satellite broadcast status is to usespectral data from the spectrum analyzer.andP;  The complete spectral informationis processed by the various neural networks more or less in parallel.andP;  Onenetwork is assigned the task of detecting unauthorized carriers in thesystem.andP;  Preprocessing for this network consists of subtracting off theexpected characteristics of the authorized carriers.andP;  The informationremaining is passed into the network for determination of whether anyunauthorized broadcasts are included.andM;The remaining networks look for about a dozen other defects in thecommunications link.andP;  Each is trained on somewhat different combinations ofdata.andP;  In every case, however, the raw spectral data from the spectrumanalyzer is first separated into the spectral information for each of theauthorized carriers.andP;  One group of networks uses the estimated carrier-powerto noise-power-density ratio and compares it to the expected value of thesame ratio.andP;  These networks determine a series of faults such as&quot;interference from local rainfall,&quot; &quot;terminal power too high,&quot; and&quot;saturation from the terminal or satellite transponder.&quot;andM;Another network examines the 24-hour history of the satellite communications(preprocessed with a fast Fourier transform) and looks for autotrackfailures.andP;  A final group of networks inspects the difference between thecarrier's actual and expected signals and determines if any errors-such as amissing carrier or incorrect modulation or coding are present.andP;  In thesecases, the neural network involved determines if the actual signal is a &quot;goodenough&quot; fit to the expected signal.andP;  If the fit is inadequate, the carrier'sexpected signals are subtracted one at a time until the problem isidentified.andM;The most important lesson from this system is the notion of modularizingdifficult tasks.andP;  The developers didn't try to make one network learn theentire problem; instead, they broke the complex problem into small pieces sothat each network had a reasonable task to perform.andP;  The result of thiscareful system design has been a solution that performs extremely well in itsoperational tests.andP;  Even more importantly, the networks in this large systemare simply backpropagation networks.andP;  This example provides vivid proof thatneural-network technology can be used for real-world and highly complextasks.andP;  TRAINING AND TEST SETS Ford Motor Co. is a leader in the use ofneural networks for manufacturing.andP;  Researchers there have developed anengine diagnostic system that offers some interesting insight into trainingand test procedures.andP;  Kenneth Marko, L. A. Feldkamp, G. V. korius and otherFord researchers have provided important insights into the training and testset.andM;A neural network is only as smart as its training permits.andP;  The developermust ensure that the training set and training procedure are appropriate tothe problem.andP;  This includes making the training set representative of thekinds of patterns the operational network will have to recognize.andO;Furthermore, the training set must span the total range of input patternssufficiently well that the trained network can generalize about the data.andO;For example, if a network is trained to determine a person's hair color fromtraining images of blondes and redheads only, expecting it to correctlyhandle brunette images is unrealistic.andP;  While neural networks have often beendemonstrated to have extrapolation and interpolation capabilities, they mustbe trained on a wide enough set of input data to have a fighting chance ofgeneralizing from their training sets.andM;The Ford researchers found a training-set definition called leave-k-out to beparticularly effective.andP;  When a total of N example patterns are in theoverall data set, the procedure uses a series of random choices of N-kpatterns for training and k for testing.andP;  Often, the frequency of occurrenceof any given pattern is normalized so that all patterns appear withapproximately the same frequency.andP;  (Clearly, this normalization technique isappropriate only for networks using backpropagation or other similar models.andO;If applied to a Kohonen network, such frequency normalization would destroythe network's natural probability-distribution model.)andM;The Ford researchers have also found the specific case of k = I to be usefulwhen only a few example patterns exist and a training set must be developedfrom them.andP;  This situation is sometimes used as a criterion for using arule-based system rather than a neural network, but if the pertinent rulesare not well understood, the leave-1-out training procedure is a goodalternate choice since it means the network can be exhaustively trained onall N possible training/testing set combinations.andM;The Ford diagnostic network must process fairly large amounts of data in arealtime mode-typically, about 10KWords (one word equals two bytes) of datafrom about a dozen inputs during a few seconds of data recording.andP;  Some ofthe data are digital values and some are analog.andP;  Interestingly, while anexpert human technician can inspect a multisignal plot of the collected dataand determine whether the engine is operating normally, generating a workableset of rules for an expert system to do the same has been impossible.andM;The network was trained on data collected on 26 distinct induced faults inthe engine such as an open plug, broken manifold pressure sensor, pluggedinjector, and so on.andP;  The training set is made up of 16 data sets, eachconsisting of data collected from a single engine cycle  all six cylinders ofa sixcylinder engine, for example) for each failure.andP;  An equal-sized test setwas used.andM;The Ford development group used a number of network paradigms and softwarepackages and found they could achieve 100% accuracy with nearly all of them.andO;More importantly, they obtained these speed and accuracy capabilities usinghardware and software systems more or less the equivalent of equipmentalready aboard the automobile.andP;  Thus, successful deployment of on-board,real-time diagnostic systems seems quite practical using current neuralnetwork technology.andM;DIAGNOSTIC NETWORKSandM;Many other diagnostic network applications exist; the ones noted here aremerely some of those reported in recent conferences.andP;  (See the suggestedreading list.) Failure diagnosis, once the stronghold of the rule-basedexpert system, should be considered a highly appropriate application forneural networks, as well.andM;The systems described here each have important lessons.andP;  Their developershave contributed greatly to the field by sharing their stories andillustrating some of the many ways neural networks can be used to solvereal-world problems.andO;</TEXT></DOC>