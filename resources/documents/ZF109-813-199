<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF109-813-199  </DOCNO><DOCID>09 813 199.andM;</DOCID><JOURNAL>Patricia Seybold's Network Monitor  Nov 1990 v5 n11 p1(10)* Full Text COPYRIGHT Seybold Office Computing Inc 1990.andM;</JOURNAL><TITLE>Managing storage: the Epoch-1 Infinite Storage Server. (EpochSystems Inc.'s file server)(includes related article on the VortexSystems Inc. RetroChron disk controller board)</TITLE><AUTHOR>Millikin, Michael D.andM;</AUTHOR><DESCRIPT>Company:   Epoch Systems Inc. (Products).andO;Product:   Epoch Systems Epoch-1 InfiniteStorage Server (Disk drive) (Designand construction).andO;Topic:     File ServersStorage CapacityMagnetic DisksOptical DisksDisk StorageStorage AllocationBack-Up Procedures.andO;Feature:   illustrationchartgraph.andO;Caption:   The design of Epoch's server. (chart)The Infinite Storage Architecture. (chart)Epoch's staging out of files over time. (graph)andM;</DESCRIPT><TEXT>Managing Storage The Epoch-1 Infinite Storage ServerandM;REMEMBER THE FINAL scene of the first Indiana Jones movie (Raiders of theLost Ark)?andP;  We see a government functionary wheeling the crated Ark into agiant warehouse.andP;  The camera pulls back to reveal what appears to be acres ofstacked, identical crates.andP;  Once stored, we can infer, the Ark will beentombed with all the other crates and, for all practical purposes, will beunretrievable.andM;This is a pretty good analogy to the situation facing modern network users.andO;What would make it a little more akin to the network situation would be ashot of a whole series of identical warehouses being built and then filledwith crates.andM;As networks proliferate and storage requirements expand, we are having anincreasingly difficult time first in finding a place to put our data and thenfinding the data once stored.andP;  Sadly, the most common practice currentlyseems to be to go out and procure more disks.andP;  Without a storage managementstrategy, however, this rampant addition of storage capacity is a short-termand potentially costly solution.andM;THE PROBLEM.andP;  The essential problem is that massive amounts of data requiremechanisms that manage that data.andP;  Without proper storage management, usersend up running out of online storage, or harried administrators fail toimplement comprehensive backup procedures just from a lack of time, or serverperformance degrades because the disks are so full, or users and/oradministrators can't find the data once they have been archived (the Lost Arkscenario).andM;Looked at another way, network computing must solve three basic problems:andM;* Disk space managementandM;* Archival managementandM;* Backup managementandM;Although the latter two may seem similar, we can actually draw a very usefuldistinction between them.andP;  Backup management refers to the protection ofactive data through physical replication.andP;  Archival management refers to theremoval of a datafile from an active storage space to a separate, long-termrepository.andP;  Both of these require support for a retrieval function.andP;  Thenature of the retrieval process is slightly different in both cases, however,given the different intent of the two processes.andM;BACKGROUND DATA.andP;  Epoch Systems, the company whose storage management productyou will read about below, spent a good while researching the dynamics of itsparticular market: the workstation network.andP;  Epoch came up with someinteresting data based on customer and user group surveys.andM;* Workstation performance growth is rapidly outpacing the magnetic diskcapacity growth.andP;  From 1981 to 1990, workstation MIPS increased by a factorof about 100, while magnetic disk capacity growth increased by a factor ofabout 10.andM;* Network storage growth far outpaces both the growth in the number ofworkstations and in the number of system management staff.andP;  From 1987 to1990, gigabytes of network storage grew by a factor of greater than 3.5.andP;  Thenumber of workstations grew by a factor of less than 2.5.andP;  And systemmanagement staff grew by less than a factor of 2.andP;  In other words, there isnow more storage per workstation with relatively fewer administrative staffto manage it.andM;* Fifty percent of those surveyed projected that their data would increase bya factor of 7.5 from 1990 to 1995.andM;What this boils down to is a serious burgeoning problem.andP;  Because storagetechnologies haven't really kept pace with growth elsewhere, the commonsolution to the demand for data management is to add disks.andP;  This type ofsolution can result in proliferation of mount points in the network filesystem.andP;  Already strained backup procedures could break down even further,putting precious data at risk.andP;  Archiving techniques will be strained,resulting in a higher mean time to service and a rising cost of internalmanagement.andM;If a comparable situation faced the dismal discipline of economics, someonewould write a doomsday scenario book about the collapse of the world economy.andO;We, fortunately, can point to happier solutions.andM;All kidding aside, specialized network service areas such as this willprovide a great many opportunities for a number of companies.andP;  One suchcompany is Epoch Systems in Westboro, Massachusetts.andM;Epoch: Providing Storage ManagementandM;for the Network ComputerandM;Epoch Systems has taken an innovative approach to solving the problem of thecentralized management of distributed network data.andP;  Essentially, it proposesto reduce the size of the storage and backup problem by providing acombination of magnetic and optical (or tape) storage.andP;  You should notconfuse Epoch with a provider of optical subsystems.andP;  While the storage mediaare somewhat important in terms of delivering volume, Epoch's significantvalue is in the software it provides to manage the storage process.andP;  Epochuses a hierarchical storage architecture that manages optical disk technologyas a low-cost backing store to high-speed magnetic disks.andP;  The trick is thatEpoch automatically off-loads inactive data from magnetic disks into anonline optical archive.andP;  The process reduces the number of file systems thatneed to be managed and reduces the amount of data in the network thatrequires regular backup.andM;As a result, the Epoch magnetic disks never fill completely.andP;  Once the freedisk space drops below a predefined level (Epoch calls it a watermark), thesystem begins offloading inactive datafiles to optical.andP;  As a user, youhaven't the slightest clue that this is happening.andP;  Epoch's current serverconfigurations offer between 1 and 1,000GB of online data storage.andP;  When youadd in off-line storage (cartridges on the shelf) that are still listed inthe online directory, then your storage capacity is, indeed, &quot;infinite,&quot; asEpoch claims.andM;Because it is designed to solve storage management requirements in adistributed network environment, Epoch is embraving the client server model.andO;In the initial implementation of the Epoch technology, however, Epoch is onlydelivering a server component; clients have access to the storage service viathe NFS file service.andP;  (See Illustration 1.)andP;  A logical future direction forEpoch is to push some of its software out onto the desktop.andM;It uses up to 20MB of system RAM memory for file-caching to reducedisk-seeking.andM;The foundation of Epoch's solution is the Infinite Storage Architecture (ISA)shown in Illustration 2.andP;  ISA currently consists of three major services:andM;* Hierarchical Storage, provided by the Infinite Storage Manager (ISM)andM;* Online backup and recovery provided by the Backup and Recovery Manager(BRM)andM;* Volume Management, provided by the Removable Volume Manager (RVM) and theAutomated Library Manager (ALM)andM;INFINITE STORAGE MANAGER.andP;  ISM is the heart of Epoch's storage architecture,and is responsible for managing the movement of data between the variouslevels of storage.andP;  ISM's hierarchical management of storage relies on threetechnologies: RAM, magnetic disks, and optical disks (See Illustration 3).andO;ISM's movement of data from one tier to another (called staging) is policybased.andM;ISM has three major functions.andP;  It stores the definition of the stagingpolicy for controlling the movement of data.andP;  It stores the definition of therelationship between levels of storage--called staging trails.andP;  And itimplements the staging policy.andP;  (See Illustration 4.)andM;When ISM stages out a file, it writes the data in a serial bit stream tooptical, while retaining the directory information (inode) on magnetic disk.andO;(One reason for using a serial bit stream is to maximize data locally tominimize the need for cartridge exchanges and to minimize seeking.)andP;  Itupdates the directory information (in the case of Unix, the inode) and leavesa reference to the backing storage.andP;  Today, the backing store consists ofoptical or tape technology.andM;Logically, then, the file is still online and accessible.andP;  Should a userissue a request for that file, a staging fault occurs, and ISM automaticallybrings the file back from optical to magnetic, or &quot;stages the file in.&quot;andP;  (If,in these situations of enormous data storage, a file is ever taken&quot;off-line,&quot; is it ever realistically accessible again?andP;  Or is it lostforever, like the Ark in the warehouse?andP;  Epoch believes that informationtaken offline is inaccessible and thus useless.andP;  Although that is a prettystrong generality, we tend to agree, particularly in situations of large andrapidly proliferating network storage.)andM;It is important to note that the data being read from optical disks iswritten to magnetic disks and to the LAN at the same time.andP;  This has obviousperformance benefits over a serial optical to magnetic to LAN sequence.andM;ISM provides three types of staging:andM;* Bulk staging occurs automatically at configurable intervals.andP;  A dailystaging run reduces a file system's magnetic disk utilization to aprespecified low watermark (LWM) level.andM;* Event-driven staging occurs when magnetic disk utilization exceeds the highwatermark (HWM) level.andM;* Explicit staging allows users to manually stage out selective files as agroup.andM;The system administrator specifies what the zone of maximum utilization forthe file system should be, i.e., the area between the watermarks.andM;There is also a pre-stage watermark.andP;  When ISM performs bulk staging, itpre-stages additional files to bring potential magnetic utilization down tothe specified pre-stage watermark.andP;  Pre-staging writes the data to opticaldisk but does not release the space on magnetic disk.andP;  Should users requirethe pre-staged file(s), there is no delay.andP;  However, should file systemutilization exceed the high watermark, triggering staging, the disk space ofunmodified pre-staged files is immediately released, bringing diskutilization back down to the low watermark.andP;  (This is a performanceacceleration that eliminates the stage-out time.)andM;Additionally, should a huge write request push file system usage up to thelevel specified by the BSD minfree parameter, the process requesting thewrite has to wait only until ISM stages files and releases sufficient space.andO;In other servers, the offending request would cause the application toterminate in error.andM;ISM adds several extended file-control attributes that provide control overthe staging process.andM;* A locked attribute keeps the file on magnetic disk, precluding staging atany time.andM;* A convenient stage-out attribute tells the system to stage out the file atthe next convenient time.andM;* A keep attribute is similar to the convenient stage-out, but it keeps thefile data intact on the magnetic disk.andP;  In other words, it requestspre-staging at the next convenient time.andM;* A residence priority attribute specifies the relative importance of keepinga file on disk.andM;When ISM performs its bulk staging, it first calculates a residency index foreach file in the system.andP;  The index is a function of size on disk, time sincelast access, and residence priority.andM;There is also a further degree of distinction in the selection of files forstaging.andP;  When ISM first stages a file, it leaves behind the first filesystem block (today, approximately 8KB) of the file on magnetic disk.andP;  Thisblock is called a fencepost.andP;  Subsequently, ISM considers the size of thefencepost, not the size of the staged file, in calculating the file'sresidency index.andP;  In other words, by reducing the size occupied by a file onthe disk, ISM makes that file less likely to be staged a second time whenconsidered against a complete, unstaged file that has become a candidate forstaging.andP;  If ISM selects the fencepost for staging, then it simply releasesthe disk space occupied by the fencepost.andP;  The file data already is residenton optical.andM;Epoch introduced fenceposts as an optimization feature.andP;  The company hopedthat a certain class of requests could be satisfied with very little magneticresident data.andP;  For example, should a user issue a Unix command such as&quot;head&quot; or &quot;file&quot; that reads only the beginning of a file, those commands readthe fencepost without causing the rest of the file to be brought back fromoptical.andP;  When data following the fencepost is referenced, ISM stages in onlythe referenced portions rather than the entire file.andM;The staging template defines the watermark values for a given configuration.andO;Each file system has an associated template, and multiple file systems canuse the same template.andM;The ISM staging trail defines the sequence of optical volumes to which filesare staged.andP;  All files in a file system are written to a single stagingtrail, thereby being grouped together on their own set of optical stagingvolumes.andP;  ISM provides an additional mechanism for grouping files on stagingvolumes should the files be located in separate file systems.andP;  A high-levelcommand can cause certain files to be staged to a named volume, if thosefiles are not originally resident in the same file system.andP;  In other words,you can always keep logically related sets of files together on the samevolume, even if they reside in different physical file systems.andM;Epoch is a system that must be tuned to adapt to the usage patterns of eachsite.andP;  Administrators must decide what parameters work best given their workrequirements.andP;  Epoch believes that, after about three months, even takinginto account unusual activity, usage will quiesce into a fairly predictablepattern that, in turn, can be well-supported by configuring the ISM.andM;XFS.andP;  To make all this happen, Epoch has created its own file system, XFS(the eXtended File System), in which it adds some extra information to theinode.andP;  The XFS inode contains a staging ID that describes whether or not thefile is resident on magnetic storage, and, if it is not magnetic resident,then the location of the file.andP;  It is, in the words of a senior Epochengineer, a fairly simple algorithm.andP;  If location = 0, then the file ismagnetic resident.andP;  If location [is not equal to] 0, then the system looksfor the volume ID.andM;The request then goes to the Drive Multiplexer (which controls thecoordination of the logical optical volumes with physical resident andnonresident media).andP;  Should there be a mount fault, i.e., should the requiredvolume not be in the drive, the drive mux puts the XFS call to sleep andsends an error message out to the Mount Queue Manager (MQM) daemon.andP;  The MQMdaemon then handles the request, emptying a drive, moving a requested volumeto the drive, verifying the label, and then returning control to the drivemultiplexer.andP;  (In the worst case, that of a volume on a shelf someplace, theMQM notifies the operator to retrieve a certain disk cartridge or tape.)andO;Once the drive multiplexer receives control back from the MQM daemon, it thenwakes up the XFS open call, which continues.andP;  (See Illustration 5.)andM;BACKUP AND RECOVERY MANAGER.andP;  The Backup and Recovery Manager (BRM) puts aspin on the traditional modes of full or incremental backups.andP;  BRM allowsadministrators to perform magnetic-only backups, which save just themagnetic-resident portion of the files, or complete backups, which save bothmagnetic and optical-resident portions.andM;BRM is designed to provide consistent, timely backups with integrity and areasonable time of recovery (the latter being particularly important whendealing with hundreds of gigabytes of data).andP;  Among its features, the BRMoffers:andM;* Online backup of the file system.andP;  Traditional Unix backups require that afile system be taken off-line to ensure data integrity.andP;  Epoch's onlinecapabilities make it more feasible to back up with several incremental dumpsa day, improving the timeliness of the backup.andP;  (Other vendors are trying toprovide a continuous, real-time backup image of an entire system.andP;  More onthis below.)andM;* Online catalogue for backup volumes.andP;  BRM maintains an online database offiles to assist in the restore process.andP;  Every backup transaction iscatalogued.andP;  This allows administrators to avoid the time-devouring tedium ofmounting and searching through a series of volumes for the needed files.andM;* Batching file systems backup.andM;* Time- and space-efficient backups.andM;* Administrators can configure BRM through the use of backup templates thatspecify the backup of multiple file systems, the inclusion or exclusion ofspecific files, or the timed, automatic execution of backup.andM;* BRM is integrated with the Volume Manager, which tracks and mounts volumesand media as needed.andP;  This relationship also allows backup to allocate newvolumes automatically, and provides an extra level of security againstaccidental restoration of the wrong version of a file or the overwriting ofvaluable data should the wrong volume be mounted.andM;Additionally, the BRM is able to avoid using tapes that have reached the endof their usable life by keeping track of a reuse count.andM;Backup media can be either 8mm tape or optical disk.andP;  When using opticaldisk, the Volume Manager manages the automatic mounting and dismounting ofthe proper volumes.andP;  The cron command can be used to schedule the periodicbackups.andP;  Combined with the ability to back up to preallocated optical orpreloaded 8mm helical tape, cron-triggered backup provides an unattendedbackup capability even for very large file systems on the network.andM;The Recover function is designed to deliver a backup copy of a file quicklyto a user who has accidentally erased that file.andM;BSD utilities can provide a true image recovery, although that can only beapplied to an entire file system.andP;  Epoch's image recovery can be applied toany part of the file system hierarchy.andM;Epoch restores files relative to the user's current directory, regardless ofthe original location of the file.andP;  To minimize confusion, Epoch will createthe necessary subdirectory structure within the user's current directory.andP;  Inother words, if the original file path was/a/b/c/file, and the userrecovering the file is currently in directory/usr/mdm, then the recoveredfile would have a path structure of/usr/mdm/a/b/c/file.andP;  This eliminates thenecessity of going back to the original directory for the restore (andpossibly overwriting data).andM;REMOVABLE VOLUME MANAGER.andP;  The Removable Volume Manager (RVM) electronicallylabels all backup volumes and tracks the location of each volume in an onlinedatabase.andP;  All optical volumes are always virtually online, even if they arenot currently mounted or are physically removed from the jukebox unit.andO;Should a request require the reinsertion of an optical disk into the jukebox,the RVM user interface manager notifies the operator.andM;RVM provides a logical I/O layer that is inserted into the Unix block driverswitch.andP;  RVM tracks removed volumes with a volume support database.andP;  The RVMsystem multiplexes both staging and backup volumes onto actual physicaldrives.andM;AUTOMATED LIBRARY MANAGER.andP;  Architecturally, the Automated Library Manager(ALM) sits beneath the RVM and functions as a device-independent logicalinterface to all physical storage media.andP;  It uses a superset of the SCSI IIModel, and it supports optical and tape library units.andP;  Because it providesthis common interfce for storage media to the other ISA units, the ALM allowsEpoch to expand to accommodate new storage technology as it comes down theroad--even including such potentially radical offerings as the holographicstorage being developed by MCC.andM;HYPERWRITE.andP;  NFS writes can be very slow, due to the nature of the statelessprotocol.andP;  A variety of NFS accelerators are out on the market to addressthis problem.andP;  Epoch is not explicitly an NFS accelerator.andP;  However, it hasdesigned a software option that greatly increases NFS performance on theEpoch-1 server.andP;  HyperWrite is a software option for the Epoch servers thatoffers three algorithmic enhancements to reduce disk-seeking:andM;* Batching NFS write requests.andP;  HyperWrite detects multiple, simultaneouslypending NFS write requests from various clients and consolidates those as onephysical write to disk.andM;* Preallocating disk space.andP;  In sequential file writes, HyperWrite writes thedata to preallocated contiguous disk space to reduce the number of requireddisk seeks.andM;* Shadow inodes.andP;  HyperWrite creates a temporary inode near the file's newestdata blocks.andP;  As with preallocated disk space, the objective here is todecrease disk-seeking.andP;  At a convenient time, HyperWrite then copies theshadow inode to the permanent inode and discards the shadow.andM;Epoch has measured a performance improvement of up to 4 times that oftraditional file servers without any extra hardware accelerators.andM;HYPERSAVE.andP;  HyperSave is a software option that provides for very efficientcomplete backup of the server and also manages an off-site archive fordisaster recovery.andP;  As noted above, the BRM allows administrators to performmagnetic-only backups to save time.andP;  That does leave the system vulnerable toa catastrophe that damages the optical disks, however, and so doesn'teliminate the need for a complete backup of system data.andM;Tomake complete backup efficient in the large data volumes served by Epoch,HyperSave introduces another type of backup: the baseline backup.andP;  Theadministrator specifies the criteria for defining the stable files to bemaintained on a baseline backup (for example, all files unmodified for aperiod of time, or all files staged out to optical).andP;  HyperSave willautomatically update the baseline backup to add new stable files not alreadyon the baseline backup.andP;  The idea at work here is that the periodic baselineupdate doesn't consume as much time.andP;  Full backups are integrated with thebaseline backups.andP;  That is, only those files that are not current on thebaseline are backed up.andP;  Files currently in the baseline have only theirinode copied, along with a pointer to their data, on the full system backup.andM;Epoch's strategic goal is to eliminate the need to back up those files thathave not changed since the last backup.andP;  By reducing the number of files tobackup, but still retaining the ability to easily reconstruct a damaged disk(through image recovery) as well as to selectively recover lost files, theprocess of backing up hundreds of gigabytes becomes manageable.andP;  (See Table1.)andM;HARDWARE.andP;  The server can be configured with between 8 and 24 MB of systemRAM.andP;  The box uses two 25 Mhz processors: one an application processor (AP),the other, a front-end processor (FEP).andP;  The processor AP has 32 KB ofvirtual cache, and 2 KB of battery-backed RAM.andP;  The FEP has 384 KB of local,zero-wait state code RAM, a 64 KB EPROM, and an 8 KB EEPROM, and controls theSCSI driver and the LAN access.andP;  Both processors are hooked to a VME masterfor potential redundancy.andM;The optical library units (OLVs) at this point support 5.25-inch Erasable,5.25-inch WORM, and 12-inch WORM.andP;  The key in determining capacity is themixture of optical storage in a configuration.andP;  The ALM currently supportsthree types of optical media: 5.25-inch erasable and write-once media, and12-inch write once.andM;Model 21-D1.andP;  The low-end Model 21-D1 is the entry-level Epoch system,supporting a paltry 20GB of storage.andP;  The System unit comes with ISM and BRMsoftware, and the system processor with two Ethernet ports, one 760 MBmagnetic drive, one 2.3GB cartridge tape drive, and two internal slots foradditional disk drives, all mounted in a 30-inch-high rack cabinet.andP;  Theaccompanying Optical Disk Library Unit brings the RVM and ALM software, one650 MB Rewritable Optical Disk Drive, one slot for an additional rewritableoptical drive, 32 slots for rewritable optical disk cartridges, and two 650MB rewritable optical disk cartridges.andM;Model 35B.andP;  The Model 35B is a 30GB storage server for network environmentswith &quot;medium to large&quot; sets of working files.andP;  The system unit is the same aswith the Model 21, except for the addition of a file cache accelerator with8MB memory.andP;  An expansion unit provides four more 760 MB magnetic diskdrives.andP;  The OLU has one 644 MB erasable optical drive, with three slots foradditional erasable optical drives.andP;  The jukebox contains 48 slots forerasable optical disk cartridges.andP;  Ten 644 MB erasable optical diskcartridges come with the system.andM;Model 3okB/C.andP;  The Model 335B/C server uses a combination of a 5.25 erasableoptical disk library unit and a 12-inch WORM optical disk library unit toprovide support for up to 327GB of storage.andP;  This unique combination allowsfor &quot;temporary&quot; data to be stored on the erasable medium and then moved to&quot;permanent&quot; storage based upon user-specified criteria.andM;Model 1007C.andP;  By adding two more 12-inch WORM OLUs, the Model 1007C serverbrings online storage of 980GB to very large networks.andM;The Enterprise Storage Server.andP;  Epoch just announced a new configuration thatadds yet another level in the hierarchy: staging from erasable optical toWORM.andP;  This allows users to keep medium-term data on the rewritable opticalmedium, while relegating archival material to the permanent WORM medium.andP;  Thenew system allows users to keep up to 30GB of online data on rewritable andto automatically stage files out to 330GB of WORM.andP;  This system, too, canexpand to up to 1,000GB of storage.andM;NETWORK INFINITE STORAGE MANAGER.andP;  The Infinite Storage Manager, as neat asit is, is currently confined to the storage on the Epoch server.andP;  Epochrealized, though, that the next obvious step was to distribute its storagemanagement capabilities across the network.andP;  That is just what its newestproduct, the Network Infinite Storage Manager (NIS), does.andM;NIS allows storage elsewhere on the network to become part of the ISM storagehierarchy.andP;  This is a full client/server implementation.andP;  Clients can beeither workstations or other file servers; the server retains its role ofproviding a bit-stream storage service that stages to optical media.andM;NIS uses the same ISA manaement applications as the Epoch-1 server.andP;  Thereare differences in the interface of the storage management services with thefile system, however.andP;  (For example, Epoch doesn't change the inode thatresides on the clients.)andM;Each client file has an associated client store on the NIS server.andP;  More thanone client file system can map to the same server store, thereby creatingsupport for logical groupings of distributed files.andP;  (See Illustration 6.)andM;The software on the client includes both user applications and enhancementsto the kernel.andP;  Epoch uses what it calls UFS (Unix File System) wrappers.andO;Wrappers are routines that handle the ISM issues in filing before passing therequest on to the standard UFS routine.andP;  (See Illustration 7.)andP;  A request fora file calls the UFS routine first.andP;  The wrapper address is in the virtualfile switch (VFS) table in place of the UFS routine addresses.andM;The NIS client has access to two types of file staging: demand staging andperiodic staging.andP;  Demand staging is the system's response to a lack of spacetriggered either by a &quot;no space&quot; error return or the standard monitoring ofthe watermark levels.andP;  Periodic staging brings file system usage down to thelow watermark level at specified intervals.andM;Each file system contains a special file (epxattr) that contains one entryfor each inode in that managed file system.andP;  The entry contains the stagingID and the extended attributes.andP;  This was Epoch's approach to delivering thetype of hierarchical direction it has in XFS without altering the structureof the client system inode.andP;  The wrapper checks the epxattr file uponreceiving a &quot;file open&quot; request if the inode indicates that no magneticstorage is being consumed.andP;  The system also checks the magnetic space used bythe file from the original inode.andP;  Minimum space and the presence of anon-zero staging ID trigger the stage-in of a file from the server.andP;  (SeeIllustration 8.)andM;While the particular approach described above works well for Unix, Epochfaces some different challenges in trying to support other client operatingsystems.andP;  Adding NIS support for VMS, for example, is necessitating somechanges at the XQP level.andM;Dump and Restore.andP;  Epoch modified the client Dump and Restore utilities toback up the NIS file system properly.andM;Dump now has three modes.andP;  The default mode backs up everything on the clientmagnetic system.andP;  For each stagedout file, Dump makes an RPC request to theserver to determine the status of the bit file.andP;  If the server has alreadybacked up the bit file, this data is not replicated in the client dump.andP;  Ifthe server has not backed up that data, then Dump makes RPC requests to readthe bit file and includes it in the dump image.andP;  Dump also backs up theepxattr file.andM;The second mode is a Complete backup that reads all bit file data from theserver via RPC requests and then dumps everything on the file system.andM;The third mode is Magnetic-Only, which includes only the inodes and datacurrently resident on the client magnetic storage.andP;  It does not query theserver for bit file information.andM;The Restore utility takes into account these variations on dump to restorefiles from the server.andM;If an entire disk is lost, reloading the new client disk using a dump tapecreated with the default mode will restore all local magnetic files andreestablish all connections between the local disk and the bit files on theserver.andM;In the event of an accidentally deleted file(s), reloading the dump tape willeither restore the file (if it was magnetic resident) or it will schedule aserver restore if the file was backed up on the server.andM;FUTURES AND COMMENTS.andP;  Epoch has hit on an innovative solution for a seriousproblem that will only worsen as networks continue to grow, both in number ofnodes and in the amount of supported storage.andM;The Epoch servers are designed to support NFS networks, particularly Sunworkstations.andP;  Clearly, support for other operating systems and networkenvironments is desirable.andP;  Epoch says that such support for other operatingsystems is coming.andP;  (There is never any lack of laundry list items fordevelopers, particularly in an &quot;open systems&quot; world.andP;  We think Epoch isproceeding quite reasonably by targeting its immediate customer segment andworking on enhancements for that sector first--such as NIS--before shovingoff into unexplored territory.)andM;Similarly, Epoch will push its technology more in the client/serverdirection.andP;  The initial implementation of ISA was just that--initial.andP;  WithNIS, Epoch is showing that it recognizes the opportunity that theclient/server world opens up.andM;Epoch does not compete directly with NFS enhancers (although one of the sideeffects of implementing a system with HyperWrite is enhanced NFSperformance).andP;  Nor is Epoch directly competitive with optical subsystemproviders.andP;  Instead, it has discovered early on what we believe many othercompanies will find during the next decade: The advent of network computingopens up an enormous opportunity for new suppliers of niche servicesolutions.andM;Epoch is not alone in recognizing this, of course.andP;  Legato, the creators ofthe PrestoServer NFS enhancer, have also introduced a set of backup andrecovery products: the NetWorker.andP;  NetWorker backs up and recovers files frommultiple clients to a Sun server and stores them onto tape.andP;  Upon request,the files are restored to the PC in their original format.andP;  And Legatomaintains an online index.andP;  The Legato solution is not of the scale ofEpoch's, however.andP;  NetWorker retails for $4,995.andP;  An entry-level version isavailable for $2,500.andM;If you are worried about storage management on your NetWare LANs, then Epochis not for you.andP;  If you have Sun and NFS networks (with VMS workstations aswell), then Epoch should be of great interest (and importance).andO;</TEXT></DOC>