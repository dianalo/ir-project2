<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-746-136  </DOCNO><DOCID>07 746 136.andM;</DOCID><JOURNAL>Communications of the ACM  Oct 1989 v32 n10 p1174(16)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>A finite and real-time processor for natural language.andO;</TITLE><AUTHOR>Blank, Glenn David.andM;</AUTHOR><SUMMARY>A computational architecture for natural language processing usingthe Register Vector Grammars (RVG) algorithm can handlesyntactical versatility while avoiding excess grammar size andcomputational complexity.andP;  RVG architecture, compared tonondeterministic finite automation and to formalisms with morecomputational complexity, can handle nondeterminism by using asmall number of registers.andP;  An outline of enhancements involvingsubcategorization, agreement, conjunction and canonical structuresis presented, as is an analysis of the algorithm's time complexityand empirical results demonstrating RVG's efficiency.andM;</SUMMARY><DESCRIPT>Topic:     Artificial intelligenceLanguage ProcessingAlgorithmsProcessor Architecture.andO;Feature:   illustrationchartgraph.andO;Caption:   FSA for partially free-order language. (chart)Simple FSA for SVO language. (chart)Length of sentences per second and per transition. (graph)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>People process natural language in real time and with very limited short-termmemories.andP;  This article describes a computational architecture for syntacticperformance that also requires fixed finite resources.andP;  Natural languages areamazingly versatile, but not infinitely so.andP;  I start with the premise thatsyntactic performance requires only fixed finite resources.andP;  I have found aperformance processor that needs only a static state space.andP;  The processoroperates in 0(n) or real time.andP;  This is a significant improvement on the0(n,,) time available for context-free languages [10, 25, 32].andP;  Efficientperformance also depends on holding down grammar size.andP;  The processorpresented here represents syntactic versatility without incurringcombinatorial redundancy in the number of transitions or rules.andP;  It avoidsboth excess grammar size and excessive computational complexity.andM;I view the purpose of syntax as chiefly to expedite and simplify theprocessing of semantic and discourse structures.andP;  Excessive computationalcomplexity obscures this function.andP;  Some researchers have advocated ignoringsyntax altogether.andP;  But syntax always shows up somewhere, if not in a modulethat observes generalities efficiently, then in redundancies spreadthroughout a lexicon or linguistic knowledge base.andP;  Instead, I believe thatthe fixed finite resources hypothesis is a practical design principle forsyntactic engines.andM;Most modern syntactic formalisms, calling for unbounded resources, are proneto intractability [1].andP;  The designers of such formalisms accept Chomsky'sargument that finite automata cannot model the competence of native speakers.andO;A generation ago in this publication, Woods cited Chomsky to justify theunbridled recursion and manifold tests and registers of Augmented TransitionNetworks (ATNs).andP;  These facilities make his scheme &quot;equivalent to a Turingmachine in power,&quot; because &quot;the actions which it performs are 'natural' onesfor the analysis of language [34, p.andP;  600].andM;Nevertheless, there are well-known limitations on human capabilities forprocessing syntax.andP;  Miller and Chomsky recognized &quot;the obvious fact that[speakers and hearers] are limited finite devices&quot; [22, p.andP;  465].andP;  There is asmall but long-standing tradition of work modeling syntax with finiteresources, for embedding [9, 18, 19, 27, 35] and also for keeping track ofambiguity [11, 20, 23].andP;  One model [8] tries to be finite in both of thesedimensions.' Exploiting these performance limitations leads to a morepsychologically realistic and computationally efficient model of languageprocessing.andM;Let me emphasize this point.andP;  Evidence that suggests that syntacticcompetence requires more than contextfree power does not necessarily rule outperformance with finite resources.andP;  A performance model may explicitlyinclude memory limitations as part of its design.andP;  It is entirely a questionof whether syntactic performance ever calls for unbounded resources.andP;  Forexample, Pullum [24] affirms Chomsky's view that natural languages cannot beregular, citing evidence from Central Sudanic languages wherecenter-embedding is apparently more common and acceptable.andP;  The empiricalquestion is, do speakers of Sudanic languages generate sentences withunboundedly or even unusually deep center-embedding? Bresnan et al.andP;  [5]claim that crossserial dependencies in Dutch argue against describing thislanguage in terms of context-free rules.andP;  The empirical question is, canDutch speakers generate unboundedly or even unusually wide cross-serialdependencies? If not, then the fixed finite resources hypothesis is tenable.andO;If one can count up the dependencies on one's fingers, than surely one canmodel them with finite resources.andP;  Indeed, Pullum does acknowledge thathearers may indeed process sentences &quot;as if they were finite automata&quot; [24,p. 114].andM;Register Vector Grammars (RVG) are equivalent to finite state automata (FSA).andO;Implied in the name are two major innovations that allow RVG to be far moreefficient and compact than simple FSA, with respect to natural languages.andO;First the vectors.andP;  Simple FSA represent states and categories as simplesymbols-nodes and arcs in transition diagrams.andP;  Most modern syntacticformalisms abstract over category-symbols, with nonterminals and treestructures.andP;  RVG instead abstracts over state-symbols-with vectors ofternary-valued features.andP;  As we shall see, this technique helps to eliminatea great deal of redundancy from grammars.andP;  It keeps grammar size quite small.andM;Second are the registers, which keep track of alternative states.andP;  RVG isable to guarantee linear time because it pre-allocates only a small number ofthese registers.andP;  The processor supports reanalysis of structural ambiguitiesby backtracking to system states in these registers.andP;  However, the number ofregisters never grows; instead, RVG reuses them, thus systematicallyforgetting many (but not all) ambiguities, This approach puts a tight leashon nondeterminism; it also mimics human behavior.andM;This article begins by introducing the RVG architecture, comparing it firstwith its cousin, the nondeterministic finite automaton, and eventually withmore computationally complex formalisms, This is followed by a description ofhow to rein in nondeterminism with a small number of registers, and anoutline of planned enhancements to the basic architecture, with respect tohandling subcategorization, agreement, conjunction, and canonical structures.andO;The article will conclude by presenting an analysis of complexitydemonstrating that the algorithm is indeed linear, and empirical results forsmall fragments of English,andM;REGISTER VECTOR GRAMMARandM;As the fixed finite resources hypothesis implies, RVG is equivalent to anondeterministic FSA.andP;  The RVG automaton is a 5-tuple (S, C, I, F, T), whereS is a finite set of states, C is a finite set of input symbols (calledcategories), I is the initial state, F is a set of final states, and T is atransition relation mapping S x C to S.andP;  This definition is of course thesame as that of a nondeterministic FSA.andP;  The difference lies in the nature ofRVG's states and transition relation.andM;Ternary Feature VectorsandM;The states and transition relation of RVG are represented in terms of vectorsof ternary-valued features.andP;  A feature vector f is denoted by the features (fsub 1,...f sub n,) Each feature may take on one of three possible values;&quot;+&quot;, &quot;-&quot; &quot;?&quot; (&quot;on&quot;, &quot;off&quot; or &quot;don't care&quot;).andP;  A particular grammar involvesvectors of some fixed length, k.andP;  If k isandM;9, then a state vector might look like this: ++?--?-+.andM;The transition relation of RVG relies on two ternary vector operators.andP;  Giventwo vectors of ternary features, the match operator produces a Booleanresult: match (f sub i, g sub i)andM;= (TRUE if f sub 1, = g sub i or f sub i = ? or g sub i = ?andM;(FALSE otherwiseandM;match (f, g)andM;= (TRUE if match (f sub i, g sub i) = TRUE for all iandM;(FALSE otherwise.andM;For example, match (+ -?, +??) is TRUE, but match (+ + +, + ? -) is FALSE,since a + opposes a - in the this position.andM;The change operator takes two vectors and produces a third.andP;  Definite values(+ or -) in the second vector override corresponding values in the firstvector, but indefinite values (?) have no effect, Formally: change (f sub i,g sub i) = g sub i, if g sub i =+ or g sub i =andM;( f sub i, if g sub i =?andM;change (f, g) = (change (f sub i, g sub i))andM;For example, change (-+?-+?-+?,---+++???) produces - - - + + + - +?.andP;  Notethat the change operator is asymmetric: the output vector gets the definitevalues (+ or -) of the second argument.andP;  Where the second argument hasindefinite values (?), though, the values of the first argument &quot;passthrough.&quot;andM;RVG's transition relation is implemented by three data structures:andM;* A table of productions (the Synindex).andP;  Each productionandM;in the Synindex is a 5-tuple (cat, cond, change, lexflag, actions), where catis a symbol, cond and change are ternary vectors, lexflag is a characterdifferentiating different types of productions, and actions is a list ofexecutable functions.andP;  A lexflag with value I designates one production asInitFinal, The initial state I is InitFinal's change vector.andP;  The set offinal states F are just those that match InitFinal's cond vector.andM;* A list of lexical entries (the Lexicon).andP;  Each lexical enandM;try is a 2-tuple (morph, lexcat), where morph is its morphologicaldescription and lexcat is a list of category labels.andM;* The current syntactic state register (SynState).andP;  TheandM;SynState holds a ternary-valued vector, which the transition function matchesand updates.andM;We can now define RVG's transition relation.andP;  Given S, a set of statevectors, C, a set of category symbols (in the lexicon), and Synindex, a setof productions, there is a relation R on S x C to S.andP;  Relation (SynState,lexcat, SynState) is a member of R if there is a production (cat, cond,change, lexflag, actions) such that:andM;1) match(cond, SynState) is TRUEandM;2) cat = lexcatandM;3) SynState andless;- change(SynState, change)andM;Simple Rigid and Free Order LanguagesandM;Figure 1 shows a RVG for a simple Subject-Verb-Object (SVO) language:andM;Each feature in this grammar is associated with a position in the left toright order of categories.andP;  The first feature, S, associates with the subjectconstituent position, feature V with the verb position, and 0 with theobject.andP;  Thus features can control the occurrence of categories, (They neednot necessarily have to do with particular categories or positions, as weshall see.)andM;Here is how the above grammar recognizes George loves Martha.andP;  (The twovectors on each line are the state before and after each production fires.)andM;Word SynState CommentaryandM;+++ Initialize with CLOSE's changeandM;(InitFinal)andM;George +++ -++ Production SUBJ fires:andM;match(+++,+??) )TRUEandM;(matches +SandM;change(+++,??) andgt;-++andM;(new SynState with -S)andM;loves -++ --+ VERB's cond requires +V;andM;its change is -V.andM;Martha --+ --- OBJ's cond requires +O;andM;its change is -0.andM;* ---+++ CLOSE's cond requires -S andamp; -VandM;(InitFinal)andM;Figure 2 shows a simple nondeterministic finite state automaton (FSA) whichis equivalent to the above RVG Synindex.andP;  The five nodes in this diagram areequivalent to the five states reachable by the grammar of Figure 1.andP;  Notethat in the simple FSA representation, to represent the optionality of OBJ,we need to draw two CLOSE arcs.andP;  RVG avoids this redundancy because ternaryvalues provide explicitly for optionality.andP;  The cond of CLOSE, --?, matcheseither --+ or ---.andM;Another example better demonstrates the expressiveness of vectors.andP;  Supposewe want to model a partially free-order language.andP;  It allows SUBJ to occurfreely, but requires VERB to appear before OBJ, Figure 3 illustrates the RVGversion (with no change to the lexicon of Figure 1).andP;  The only differencebetween Figures 1 and 3 is that the latter relaxes one constraint.andP;  The condof VERB which had -S now has ?S.andP;  Relaxing S allows VERB to occur freely withrespect to SUBJ.andM;Compare the above with the equivalent FSA diagram shown in Figure 4.andM;The simple FSA notation leads to much redundancy for this language.andP;  Forexample, the diagram shows explicitly that SUBJ may occur first, or followVERB, or follow both VERB and OBJ, Though the number of constituents is thesame, the number of transitions have multiplied.andP;  The RVG, on the other hand,relaxes just one constraint; the number of productions does not change.andP;  Acompletely free-order language (still obligating SUBJ and VERB) requires 22transitions in diagram form.andP;  The equivalent RVG just relaxes anotherconstraint, changing OBJ's cond from -V to ?V.andM;The RVG format is thus more compact than that of simple FSA.andP;  In this case,it is better at expressing syntactic obligations, regardless of order.andP;  Inall of the grammars above, SUBJ and VERB must occur before CLOSE can fire.andO;In both Figures 1 and 3, the cond of CLOSE requires --?, Only the changevector of SUBJ and VERB can turn off the first two features.andP;  We call CLOSE aterminator category; it acts as a gate through which only acceptableutterances may pass.andM;In general, RVG is able to represent a potentially huge number of states andtransitions with a remarkably compact transition table (Synindex).andP;  RVGconstrains the number of potentially reachable states by means of aparticular configuration of features in the table.andP;  So a rigid-order grammar(Figure 1) has fewer reachable states than a free-order one (Figure 3); butgrammar size is the same.andP;  For each category there are many FSA transitionsbut just one RVG production.andM;Simple FSA are unable to pass information from state to state, except viasimple linear precedence.andP;  Individual transitions represent localconstraints; such as that SUBJ must precede VERB.andP;  It is difficult to expressdiscontinuous, non-local constraints.andP;  The only recourse is to plot separatepaths through the transition network for each non-local constraint.andO;Interaction of constraints soon leads to an explosion in the size of thegrammar.andM;In RVG the state, a vector, keeps a record of some non-local context.andP;  Notethat this does not add any computational power to FSA.andP;  Only the match andchange operators are different.andP;  Instead of insisting on exact identity, RVGallows partial matching with &quot;?&quot; values.andP;  Instead of wholesale substitution,RVG allows partial change.andP;  This modification allows feature values to &quot;passthrough&quot; productions to subsequent states-a form of constraint propagation,There is thus no need to multiply transitions through intervening states, asin simple FSA.andP;  Technically, any RVG is equivalent to some simple FSA.andP;  Butif that FSA must allow for optionality and non-local constraints, it will bemuch larger.andM;RVG throws out sentence diagrams.andP;  States are no longer simple nodes, andrules are not longer rote patterns.andP;  State vectors abstract over state nodes.andO;To be sure, the vectors of a Synindex are equivalent to some vast diagram,but for a natural language, the diagram is enormous and tangled withredundancies.andP;  Other formalisms introduce computationally expensive devicesto supplement and untangle the diagrams; RVG simply abandons diagramsaltogether.andP;  The only complexity it adds is the functional complexity of itsmatch and change operators.andM;Think of grammars as telephone switchboards or computer chips or neuralnetworks.andP;  An RVG is able to schedule a great variety of events with acompact, dense matrix of productions and features.andM;Notation for an RVG AssemblerandM;The feature keys shown in Figures 1 and 3 are not used by the RVG processoritself.andP;  Feature vectors are actually in binary form (two bits per ternaryfeature), and ternary vector operations are composed of fast bitwise logicoperators.andP;  Since match and change operate on all features at once, we canexploit the bit vector parallelism that already exists on conventionalcomputers.andM;Feature keys do make it easier to describe vectors, though.andP;  The current RVGdevelopment environment therefore allows one to create, edit and debug RVGsusing a symbolic notation, which is translated into efficient RVG &quot;machine&quot;code.andP;  2 In the following figures, the ordering-features section is a featurekey, associating labels with feature key.andP;  In Figure 5, the cond vector ofproduction SUBJ is +S - HEAD, which translates to +???- (&quot;?&quot; is the defaultfeature value).andP;  A range notation allows one to set all the features in avector segment to a value: +S.,O translates to the binary equivalent of +++.andM;The assembler also provides for macro substitution.andP;  The macros sectionassociates a symbol prefixed by &quot;#&quot; with a vector (in symbolic form).andP;  Forexample in Figure 5 macro #NPOn expands to the vector +DET..HEAD.andP;  Macros maythen appear in lieu of feature specifications in the grammar.andP;  This is asimple way to provide for generalizations, such as the features required toopen or close a phrase.andP;  The assembler is leftto-right, so it is possible tooverride values in ranges or macros.andM;Non-lexical Productions and Phrase EmbeddingandM;A lexicon is a collection of entries, each of which hold syntactic,morphological and (eventually) semantic information.andP;  The syntacticinformation is a set of categories, corresponding to productions in theSynindex.andM;A left-to-right RVG recognizer accepts a word when it finds a productionwhose cat corresponds to one of the word's categories and whose cond matchesthe SynState register; it then advances its state by updating SynState withthat production's change and consuming the word from the input string.andP;  AnRVG generator is the same with respect to matching and updating SynState,instead buffering words to an output string.andM;It is useful to allow non-lexical productions, which advance SynState but donot consume words from input (or buffer words to output).andP;  Each production'slexflag notes this distinction.andP;  &quot;L&quot; for lexical productions and &quot;N&quot; fornon-lexical productions.&quot;andM;Figure 5 demonstrates the use of a lexicon and nonlexical categories toprocess simple noun phrases (NPs).andM;RVG treats NP embedding as a special case of discontinuity.andP;  Non-lexicalproductions SUBJ and OBJ turn on features DET and HEAD.andP;  Feature HEAD acts asa switch, distinguishing clausal from phrasal productions.andP;  All of the causalproductions (SUBJ through CLOSE) must wait until HEAD is off again-adiscontinuous constraint.andP;  Meanwhile, the phrasal productions (NAME, DET andNOUN) may occur at various positions in clause structure, because they ignorethe features pertaining to clauses.andP;  E.andP;  g., NAME has ?S ?V ?O in both itscond and change.andP;  The change of NAME or NOUN turns HEAD off again,re-enabling the clausal productions.andM;This effect is not so readily achieved by simple FSA.andP;  It is tempting toallow the equivalent of SUBJ and OBJ to branch to the same set of transitionsfor NPs.andP;  The problem is, where should the network branch at the end of thephrase-to the node past SUBJ or the one past OBJ? A simple FSA loses track ofwhere it left off.andP;  The only recourse is to multiply transitions for NPs ateach possible position where they may occur! Another alternative is tointroduce recursive subnetworks-push the state of the clause level networkonto a stack, then traverse a separate phrase level subnetwork.andP;  But RVG isable to model phrase embedding without increasing computational complexity.andO;One segment of the state vector shows the identical topology for all NPs.andM;Note that this topology is part of the same flat vector.andP;  It is thusstraightforward to model free order languages that allow scrambling ofphrasal elements.andP;  For example, in Warlpiri the head and modifier of a phrasemay be widely separated in clause structure [10].andP;  Such languages usecase-marking inflections to identify the role of loose modifiers.andP;  Inlanguages like English or Latin, phrase level features screen out clauselevel productions, as in Figure 5.andP;  A phrase-terminating production updatesthe SynState to close the noun phrase for a particular role.andP;  In radicallynon-configurational languages such as Warlpiri, phrasal modifiers may recurby selectively ignoring constraints.andP;  A phraseterminating production justsignals that it has seen some role.andP;  Loose modifiers are non-local options.andM;The advantage of non-lexical productions is that they reduce redundancy oflexical categories, and thereby make grammars more compact.andP;  A grammar withnonlexical productions is equivalent to a larger one without them.andP;  Toeliminate non-lexicals, merge each one with each of the lexical productionsthat can follow it.andP;  For example, the following lexical productions couldreplace non-lexical production SUBJ in Figure 5:andM;p SUBJ-DET LandM;cond +S #NPOff change -S - DETandM;p SUBJ -NOUN LandM;cond +S #NPOff change -SandM;p SUBJ-NAME LandM;cond +S #NPOff change -SandM;Each production takes the cond of SUBJ and composes a change vector from SUBJand the production that could follow it-DET, NOUN and NAME.andP;  Since RVG allowsmultiple non-lexical productions between lexical productions, the equivalentgrammar would have to combine all allowable sequences.&quot; Non-lexicals thuseliminate a great deal of redundancy in the grammar.andM;Note that simple FSA could not use empty categories to mimic the compact RVGtreatment of NP embedding.andP;  Empty categories do not add any capacity torecord or propagate non-local constraints.andM;WH-QUESTIONSandM;A notorious example of discontinuous constraints isandM;wh-questions.andP;  These begin with wh-words (or phrases) such as who or what,and require that somewhere in the sentence there be one missing noun phrase(a gap or trace):andM;(1) Who loves Pamela?andM;(gap for subject)andM;(2) Who does Pamela love?andM;(gap for object)andM;(3) Who do the men think that Pamela loves?andM;(gap in complement clause)andM;(4) Who does George love Pamela?andM;(ungrammatical: no gap)andM;RVG models this dependency easily, as shown in Figure 6.andM;Production WH turns on feature +GAP.andP;  Production CLOSE's cond, whichdetermines possible final states, requires -GAP.andP;  So, somewhere between WHand CLOSE, some production must turn this feature off: a discontinuousconstraint.andP;  The only production which can do so is NGAP, Its cond recognizes+GAP, and its change turns it off.andP;  Most other productions have ?GAP in bothcond and change vectors, so this constraint simply passes through anyintervening productions.andP;  Thus ternary vector functionality makes constraintpropagation very efficient.andP;  Note also that vectors allow for multipleconstraints: the cond of NGAP also requires #NPOn, the segment of featuresthat open a noun phrase, so that this production can fire only when a nounphrase is possible and a gap is possible.andP;  The change of NGAP disables bothfeature GAP and all the features for a noun phrase.andM;Here is how the grammar of Figure 6 recognizes sentences (1) through (3):andM;(1) WH:who; SUBJ:NGAP:VERB:love;andM;OBJ:NAME:pamela; CLOSE:?;andM;(2) WH:who; QUES:do; SUBJ:NAME:pamela;andM;VERB:love; OBJ:NGAP:CLOSE:?;andM;(3) WH:who; QUES:do; SUBJ:DET:the; NOUN:men;andM;VERB:think; CTHAT:that; SUBJ:Name: pamela;andM;VERB:love; OBJ:NGAP:CLOSE:?;andM;The last category before each word is a lexical production; e.g., WH beforewho, VERB before love, etc.andP;  Any categories preceding the lexical categoryare nonlexical; e.g., SUBJ and NGAP before VERB.andM;Implementation of unbounded dependencies is particularly elegant.andP;  Thecomplement clause, introduced by production CTHAT, right-embeds-reusing thesame state register.andP;  The change vector of CTHAT prepares State for a newclause: +S.,AUX#NPOff.andP;  It defaults to ?GAP.andP;  Therefore the value of featureGAP passes from the matrix clause right on through to the right-embeddedcomplement clause.andP;  In sentence (3), production NGAP fires after OBJ in thecomplement.andP;  As usual, NGAP must still fire before CLOSE can.andP;  No complicatedgap-passing mechanism is necessary.andM;Adding more possibilities for noun phrase gaps is just a matter ofintroducing the corresponding possibilities for noun phrases.andP;  Allowing forprepositional wh-questions (e.g., In which box did the robot put the hammer?)andO;is a matter of an additional constraint, which will interact with differentproductions.andM;EmbeddingandM;Clause embedding was Chomsky's [7] primary evidence against finite automatafor processing natural languages.andP;  Human performance for center-embedding is,however, severely limited.andP;  For example: (4) The mouse the cat chasedsqueaked.andM;(5) The mouse the cat the dog bit chased squeaked.andP;  Embedding objectrelatives once is not unusual, but twice is boggling.andP;  Miller and Chomsky[22] acknowledge that such limitations indicate that the human sentenceprocessing mechanism must indeed be finite.andP;  (Grammatical competence, whichabstracts away from such non-linguistic considerations as stammering ormemory limitations, is non-finite.)andM;RVG allows shallow center-embedding with a trileveled state register:andM;SynStateandM;ClauseLevel -andgt; Main clause vectorandM;1st embedded vectorandM;2nd embedded vectorandM;The SynState register remains finite.andP;  Only now it has ordered levels as wellas features.andP;  RVG shifts from level to level by changing the value of theindex, ClauseLevel.andP;  A pair of actions manage clause-shifting: ShiftDown toincrement ClauseLevel and ReturnUp to decrement it.andM;The tri-leveled SynState register is similar to a processing model of Cowper[9].andP;  As in Cowper's model, RVG prefers to iterate at the same level wheneverpossible.andP;  Note that there is no limit on left- or rightembedding:andM;(6) My mother's girl friend's husband's car broke down.andM;(7) I saw a dog that chased a cat that caught a mouseandM;that ate some cheese.andM;Neither left-embedding of genitive phrases, as in (6), nor right-embedding ofcomplement clauses as in (7), need invoke clause-shifting.andP;  Once theobligatory constituents of a clause-subj ect, predicate and complementizer in(4)-have appeared, the processor can rightembed, RVG, like a simple FSA, putsno limit on edge embeddings (iteration).andM;The tri-leveled SynState does not increase complexity in any significant way.andO;There is an equivalent single-level SynState model.andP;  Recall that a segment ofthe SynState vector manages NPs.andP;  Similarly, three segments of one long, flatvector could manage three clause levels.andP;  Shifting from clause to clause, byopening and closing windows on relevant segments, would then no longerrequire any special actions.andP;  The drawback of a flat vector technique isthat, for each level, the grammar must replicate most productions, withdifferent cond and change corresponding to each clause segment.andP;  E.g., VERBOwould match and change feature VO in the main clause segment.andP;  VERB1 wouldmatch and change Vl in the first embedded segment, etc.andP;  The clauseshiftingmechanism simply eliminates this redundancy.andM;Figure 7 demonstrates center- and right-embedding of relative clauses.andP;  It isa modification of the grammar of Figure 6 (using the same lexicon).andM;Figure 7 distinguishes two ways to introduce noun phrase post-modifiers.andO;Productions MODC and MODR both set up a new clause-with #ClauseOn in theirchange vectors.andP;  MODC center-embeds, by invoking action Shiftdown, so thatits change applies to the next clause level.andP;  MODR, on the other hand,right-embedsthat is, it simply reuses the current clause level.andP;  Anotherdifference is that MODC turns on feature MTERM, which forces MODEND to fireeventually, and invoke action ReturnUp.andP;  Here is a sample parse: (8) Men whohate men that eat quiche love pizza.andM;SUBJ:NOUN:men; MODC:REL:who;andM;SUBJ:NGAP:VERB:hate; OBJ:NOUN:men;andM;MODR:REL:that; SUBJ:NGAP:VERB:eat;andM;OBJ:NOUN:quiche; NPEND:MODEND:VERB:Iove;andM;OBJ:NOUN:pizza; CLOSE:.;andM;The first relative clause center-embeds (MODC), since it occurs to the leftof the predicate, whereas the second one right-embeds (MODR), since it occursto the right of the predicate.andP;  When MODC fires, it invokes action ShiftDownand turns feature MTERM on.andP;  This feature value passes right through MODR,and eventually triggers MODEND, which invokes action ReturnUp.andM;Global constraints on movement in other formalisms have direct and simpleimplementation in RVG.andP;  For example, consider Ross's Complex NP Constraint[28], which rules out sentences like this:andM;(9) *What have you met the man who invented? Though there are two wh-words inthis sentence, one cannot fill both gaps in the relative clause-the missingsubject and object of invented.andP;  The grammar of Figure 7 models thisconstraint in terms of feature GAP.andP;  MODR's cond requires that GAP be off.andO;In other words, GAP is obligatory at each clause level.andP;  Since MODC invokesShiftDown, its change applies to a different feature GAP, at a lower clauselevel.andM;In many dialects of English, sentence (10) is grammatical but (11) is not:andM;(10) Who does George believe saw Martha?andM;(11) *Who does George believe that saw Martha?andM;That is, subject gaps may not appear after the complementizer that.andP;  One wayto model this constraint, variously called the Empty Subject Filter orSentential Subject Constraint, is with an additional feature, NOSUBJGAP.andO;Production CTHAT's change turns on NOSUBJGAP.andP;  NGAP's cond requires-NOSUBIGAP, so that it cannot fire after SUBJ, in this context.andP;  Finally,VERB's change arbitrarily turns NOSUBJGAP off again, thus enabling NGAP afterOBJ (or any other phrase position).andM;RVG represents non-local constraints (alias &quot;movement&quot; or &quot;extraposition&quot;)the same way as local ones: in terms of features whose values propagatethrough state vectors.andP;  For example, one may extrapose the object to thefront of a sentence, as in Quiche George loves! Simply add one production toFigure 7:andM;p TOPIC N cond +S #NPOff change -0 #NPOn The cond of TOPIC is the same asthat of SUBJ, allowing it occur at the beginning of the sentence.andP;  The changeis the same as that of OBJ, enabling a noun phrase andhere is the crucialpart-disabling a subsequent OBJ.andP;  The latter is a discontinuous constraint.andM;Comparison with other Syntactic FormalismsandM;Context-free phrase structure rules are almost as poor as FSA at handlingnon-local constraints.andP;  The proliferation of state-symbols that riddlessimple FSA also plagues phrase structure (PS) rules.andP;  The patterns on theright-hand sides of PS rules inherit an inability to pass information fromstate-symbol to state-symbol, other than via convoluted linear precedence.andO;Transition networks and rule patterns are good at linear constraintsbut poorat non-linear ones.andP;  Chomsky [7] noted the inadequacy of PS rules to describethe versatility of natural language syntax without loss of generality.andP;  Hetherefore introduced Transformational Grammar, relegating PS rules to a basethat generates only kernel sentences.andP;  RVG eliminates PS rules altogether.andM;Recognizing the problems with PS rules, proponents of Generalized PhraseStructure Grammar (GPSG) [13] have developed a more abstract format.andP;  Byseparating immediate dominance (ID) from linear precedence (LP) information,they get a more succinct representation of syntax, especially of free orderlanguages.andP;  ID rules specify the obligatory constituents in a local tree;this is roughly analogous to how RVG models obligatory requirements interminator categories like CLOSE.andM;Though ID/LP itself is succinct, it does little to alleviate the non-localconstraint propagation problem.andP;  The GPSG scheme still generates an &quot;objectgrammar&quot; with large local sets of PS rules.andP;  The more varied the linearsequences of a language, the larger the object grammar.andP;  Estimates of thenumber of rules in an object grammar for a natural language range as high astrillions or more [1, 30].andP;  As Shieber [30] points out, grammar size doesaffect processing complexity; efficient traditional context-free algorithmsare 0 (n sup 3[G] sub 2,), where G is the size of the grammar.andP;  He outlinesan algorithm that can parse using the much smaller number of ID/LP rules,directly.andP;  However, Barton et al.andP;  [1, pp.andP;  19l ff] show that Shieber'salgorithm is exponential in grammar size, due to expansion of ID rules.andP;  RVGavoids this problem by eschewing expansion of rules (and recursive expansionof rules) altogether.andM;Many modern syntactic formalisms have in common the paradigm of unification.andO;Basically, unification is an algorithm for matching graph structures (usuallydirected acyclic graphs).andP;  Computational linguists use this technique towrite grammars that involve propagating constraints through syntactic treesin the state space.andP;  Thus unification can be a way to tackle the non-localconstraint propagation problem.andP;  However, this adds complexity on top ofcontext free power.andP;  Features &quot;percolate&quot; up and then &quot;drip&quot; down trees;moreover, most formalisms add additional global filters or principles togovern this activity.&quot; Whereas unification matches complex graph structures,RVG just matches flat vectors.andP;  Whereas unification is monotonic (once avariable is bound it remains bound), RVG allows changes.andP;  Unification ensuresthat category-symbols (and features under them) are recoverable.andP;  Sincerecoverability is not an issue with state-symbols, the full power of thechange operator can be exploited.andP;  RVG has an unabashedly left-to-right bias.andO;It passes constraints through a changing state vector, and is willing toreuse a state register rather than expand the fixed state space.andM;ATNs use transition diagrams chiefly to represent local ordering,Discontinuous constraints typically involve setting, passing and testingmyriad separate registers.andP;  This is what gives ATNs Turing machine power.andO;Without restrictions on the use of registers, guaranteeing even polynomialrecognition is impossible.andP;  The ATN treatment of wh-questions is especiallydifficult, typically passing a &quot;hold&quot; list from network to network.andP;  Reiningin such a powerful mechanism, in accordance with &quot;movement&quot; constraints, iseven harder.andM;It is interesting that Woods [34] actually anticipated something very muchlike RVG:andM;In the absurd extreme, it is possible to reduce any transition network to aone-state network by using a flag for each arc and placing conditions on thearcs which forbid them to be followed unless one of the flags for a possibleimmediately preceding arc has been set.andP;  The obvious inefficiency here isthat at every step it would be necessary to consider each arc of the networkand apply a complicated test to determine whether the arc can be followed.andM;Indeed, RVG might look like such a &quot;one-state&quot; network, since it throws outthe diagrams.andP;  Nevertheless, an RVG processor does pass through many states,as feature values change.andP;  Woods simply did not see the possibility ofdecomposing the state(-symbol) itself Instead of both arcs and flags, RVGuses just features to forbid or permit transitions, Instead of an indefinitenumber of registers, RVG collects all constraints on order in just onecurrent state register.andP;  Instead of the &quot;complicated test&quot; that worriesWoods, RVG applies an efficient ternary vector match operation.andM;Since human syntactic performance does not call for unbounded embedding, itis possible to avoid even context-free power.andP;  Instead of the computationalcomplexity of a push-down automaton, RVG adds the functional complexity ofternary vector match and change.andP;  Instead of abstract, graph-structuredcategories, RVG has abstract, vectorized states.andP;  Its features are featuresof ordering, abstracted over the state-nodes in transition diagrams, or thestates between category symbols in rule patterns.andP;  RVG's states are thus ableto be sensitive to context-non-local constraints-without being&quot;Context-sensitive&quot; in the automata-theoretic sense.andP;  7andM;BOUNDARY BACKTRACKINGandM;Structural ambiguity is a crucial problem for natural language processing.andO;It is typically treated as a nondeterministic search problem, testing variousstructural alternatives until accepting one (or more) interpretation.andP;  Commonmethods of dealing with ambiguity backtracking, pseudo-parallelism or chartparsing-put no bound on the search space.andP;  Unbounded search is, however,computationally wasteful and cognitively unrealistic.andM;The fact that RVG is equivalent to FSA does not necessarily solve thisproblem.andP;  A nondeterministic FSA still involves a search through a searchspace that grows, in the worst case, exponentially.andP;  There are wellknownmethods to convert any nondeterministic FSA into a deterministic one.andP;  Thisconversion may, however, explode grammar (machine) size exponentially.andO;Because RVG already eliminates combinatorial redundancies by explicitlyallowing for optionality and nonlocal constraints, conversion to simpledeterministic FSA is neither trivial nor desirable.andM;Instead, RVG tolerates nondeterminism, by putting strict limits on it.andP;  Thetechnique is boundary backtracking.andM;RVG watches each major syntactic boundary (such as opening or closing aclause or phrase).andP;  At each boundary-crossing, it stores its system state inan associated register.andP;  If the processor ever gets stuck, it can backtrackto a previous system state-but only to one actually held in a register.andO;There is an array of boundary registers.andP;  The fixed size of this array puts acap on the growth of the state-space for nondeterministic search.andM;Each register associates with a major boundary in left to right syntax.andP;  Hereis a set (not necessarily definitive) of boundaries:andM;Curr (the current state)andM;Word (when a word is processed)andM;OpenClause(when a new clause opens)andM;MidClause (when about to process main predicate)andM;CloseClause(when clause right-embedding is possible)andM;Phrase (when a NP opens, or closes left ofandM;predicate)andM;Since RVG allows center-embedding only to a finite depth, it maintainsOpenClause through Phrase by ClauseLevel.andP;  The processor automatically addsthe clause level number to the boundary name at run time: i.e.andP;  OpenClause0,OpenClausel, OpenClause2, MidClause0, etc.andM;The AlgorithmandM;If RVG were completely nondeterministic, its algorithm would be almost thesame as that of a nondeterministic FSA.andP;  The only difference is ternaryvector match and change.andP;  Boundary backtracking is a control schema fornondeterministic search within a fixed state space.andP;  It has three basicaspects:andM;1) Local parallelism.andP;  From word to word, the processor tries all syntacticinterpretations.andP;  It searches depthfirst, looking for all possible sequencesof non-lexical productions leading to a lexical production for a given word.andM;2) Saving states.andP;  The grammar must explicitly specify when to save states inboundary registers.andP;  Associated with a few productions are save actions.andP;  Forexample, production OPENC invokes action save OpenClause.andP;  A save action putsthe current system state with the boundary name or a temporary SaveList.andP;  Theprocessor copies these states into the named boundary registers when itreaches a lexical category.andP;  In preparation for the next word, it alsoautomatically updates the Curr and Word registers.andM;3) Backtracking.andP;  Access to boundary registers is Last In First Out, mediatedby a separate array of boundary subscripts, Resume.andP;  The processor tries tocontinue from states in boundary registers until no more are left.andO;Initially, and after each word, it finds the Curr register.andP;  If this statefails (no production matches), or if a production successfully completes asentence, the processor gets the next available state via Resume.andP;  Thus ittries to report all possible interpretations.andP;  The limit on this behavior isthat save actions reuse state registers.andM;ExamplesandM;The phenomenon of garden-path sentences suggests that there are severe limitsin human performance for keeping track of ambiguities.andP;  For example:andM;(12) The horse raced past the barn fell,andM;People do not readily understand this sentence, even though there is aperfectly grammatical interpretation.andP;  The problem is the category ambiguityof raced.andP;  It could be either intransitive (serving as main predicate) ortransitive (serving as a passive post-modifer of horse).andP;  Most people preferthe intransitive reading; they are thus &quot;led down the garden path,&quot; andcannot recognize the correct interpretation.andM;When the RVG processor crosses the same boundary twice, it reuses theassociated register.andP;  It therefore loses any state already held in thatregister.andP;  This allows the state space to remain bounded; it can also lead togarden-path effects.andP;  Suppose that race has three syntactic categories, NOUN,VINTRANS and VTRANS.andP;  The processor chooses the first one that fits thecurrent context.andP;  In this case, because of the verbal inflection of raced, itskips NOUN in favor of VINTRANS.andM;Between horse and raced the processor goes through two boundary productions.andO;The first production, closing a noun phrase to the left of the predicate,triggers action save Phrase.andP;  Before accepting the main predicate, anotherproduction triggers action save MidClause.andP;  After these, lexical productionVITRANS fires.andP;  Before processing to the next word, the processorautomatically saves states from the current SaveList into boundary registers.andO;In this case there are three: Phrase0, MidClause0 and Word.andM;After the processor accepts the preposition past, and starts another nounphrase, it again saves in register Phrase0.andP;  It therefore loses the statepreviously stored in this register-the state closing the noun phrase betweenhorse and raced.andP;  So, when the processor gets to fell, and backtracks, itwill not find the passive postmodifier interpretation of raced anywhere.andO;Note that the post-modifier interpretation is not available in MidClause0,since by then the processor had already closed off the noun phrase.andM;Contrast the processor's behavior for this sentence: (13) The horse found bythe barn fell.andM;Here, found is also ambiguous, between active or passive transitive.andO;Choosing the active interpretation first, the processor closes off the NP,and saves states in Phrase0 and MidClause0.andP;  This path gets stuck when itreaches by, where it expects a direct object.andP;  So the processor backtracks.andO;In this case, there has been no intervening NP.andP;  The processor finds thepassive interpretation of found in register PhraseO, and successfullyanalyzes it as a post-modifier.andM;Sentence length does not necessarily limit boundary backtracking.andP;  (Thus BBcontrasts with the Sausage Machine of [8].) Hence it has no problems withsentences like these:andM;(14) Have the boys take the exam.andM;(15) Have the boys taken the exam?andM;(16) Have the boys who have a reputation for playingandM;hookey taken the exam?andM;To be sure, there may be backtracking.andP;  Suppose have has categories QUES andIMP.andP;  The processor will recognize sentence (14) directly.andP;  For sentences(15) and (16), it will backtrack to the OpenClause0 boundary, reanalyze takenas IMP, then accept the rest of the sentence directly.andM;Local parallelismandM;The boundary backtracking algorithm, notwithstanding the name, actuallycombines aspects of backtracking and parallel search.andP;  Both use boundedresources.andP;  Local processing (of words) is parallel and exhaustive; nonlocalprocessing (of constituents) is serial and preferential.andP;  In accordance withrecent psycholinguistic research [31], search for lexical entries is paralleland exhaustive.andP;  In addition, search for all local syntacticinterpretations-any non-lexical productions leading to a lexical productionfor a given word-is also parallel and exhaustive.andP;  This search fornon-lexical interpretations is called local parallelism.andP;  The bound on localparallelism is related to the number of non-lexical productions in thegrammar; in practice, it is quite small.andP;  Nonlocal processing is serial.andO;When necessary, it backtracks to states held in boundary registers.andP;  Thebound on nonlocal processing is the number of registers.andP;  (It is nonlocalprocessing that would otherwise lead to a combinatorial explosion in statespace.)andM;The following examples illustrate the role of local parallelism:andM;(17) Is the block sitting on the table?andM;(18) Is the block sitting on the table red?andM;At first glance sentence (18) looks very similar to (12), the garden-pathsentence.andP;  Again, the processor must choose between either a main predicateor a postmodifier interpretation of a verb.andP;  The difference is, raced isambiguous between two distinct categories (intransitive vs.andP;  transitive),whereas sitting involves just one category.andP;  The ambiguity of (18) is just amatter of non-lexical interpretation, The processor must determine if sittingis the main verb, as in (17), or a postmodifier, as it turns out in (18).andO;The processor looks for all possible local non-lexical interpretations for agiven lexical category, and saves all boundaries traversed along the way.andO;Between block and sitting, the main predicate reading of sitting saves statesin PhraseO and MidClause0.andP;  In parallel, the post-modifier reading of sittingsaves a state in MidClausel.andP;  When the processor reaches red, backtracking isable to resume from register MidClausel.andM;The difference between sentences (12) and (18) is subtle.andP;  The garden-pathsentence involves a tensed intransitive verb (the first lexical category ofraced), which cannot function as a post-modifier.andP;  It crosses boundaries onlyat the main clause level-PhraseO and MidClause0.andP;  Sentence (18), on the otherhand, involves a progressive verb (sitting), which can function as either amain verb or a post-modifier.andP;  It crosses boundaries at both the main andfirst embedded clause levels, in parallel.andP;  Thus it is possible to resumesentence (18), but not garden-path sentences like (12).andM;This model affects the way one categorizes verbs.andP;  Consider another example:andM;(19) Was the book read to the children interesting?andM;Many dictionaries list read as either transitive or intransitive.andP;  Why, then,doesn't sentence (19) lead to a garden path? The answer is that read is atransitive verb with an optional truncation of its object (like eat, cook,etc.).andP;  On this analysis, read does not actually have an intransitivecategory; instead it has just one major category, plus a subcategory thatoptions a missing, implied object.andP;  (Subcategorization is discussed in anupcoming section.)andM;An advantage of local parallelism is that it could without much difficulty bemodified to accommodate non-syntactic criteria.andP;  The syntactic componentmakes predictions about preferences by the order of productions-analogous toarc-ordering in ATNs [33].andP;  We have seen that race prefers intransitivebefore transitive; a grammar of English models the effect of minimalattachment [11,15] by ordering a phrase-closing production before productionsthat open post-modifiers.andP;  Category preferences are not the whole story, ofcourse; semantics or intonation can bias theattachment preferences as well.andO;In the right context, even garden-path sentences become comprehensible.andP;  Forexample:andM;(20) There were two horses in the field.andP;  The horseandM;raced past the barn fell.andM;RVG provides for actions associated with productions, already used to shiftclause level and save states.andP;  Other actions may perform tests on semanticstructure to provide on-line guidance to a parser or generator.andP;  Thoughsyntax is autonomous in the sense that it has its own data structures, therecan be interaction between syntax and semantics during processing, especiallynear phrase and clause boundaries (see [2, 6, 12]).andP;  In an interactiveprocessor, while trying local attachments in parallel, syntax proposes andsemantics disposes, Suppose we hook a referential semantics module up withRVG.andP;  Let production DEFEND, which fires just before closing a definite nounphrase, invoke referential analysis for the phrase.andP;  If there is anunambiguous referent for the description so far, reference succeeds.andP;  (In thenull context, reference succeeds, finding unambiguously nothing.) For thecontext of (20), reference for the horse fails, because there are two equallyplausible referents.andP;  DEFEND fails, so the processor looks for thepostmodifier interpretation instead.andM;Boundary Backtracking vs.andP;  Bounded LookaheadandM;Boundary backtracking is an alternative to the bounded Lookahead scheme ofMarcus [16].andP;  His processor also posits limited resources for resolvingstructural ambiguity.andP;  It builds up partial constituents in a Lookaheadbuffer having a small, fixed number of cells, When the buffer gets full, theprocessor must commit itself to an interpretation; it allows no backtracking.andO;By this account, garden-path sentences occur when the processor is forced tomake such a commitment, incorrectly.andM;As Church [8, p.andP;  57] notes, &quot;in some sense, [bounded] backup, Lookahead andparallelism are all very similar.&quot; Whereas unbounded backtracking saves allchoice alternatives at every choice point, bounded backtracking wouldpresumably fix the size of the backtracking stack.andP;  It is not clear, however,that there is any arbitrary limit that would both account for garden-pathsentences and still process the temporarily ambiguous sentences that peopledo understand.andM;Boundary backtracking, on the other hand, does not rely on arbitrary boundson artificial data structures (such as a stack or a buffer).andP;  Instead, it ismotivated by perceptual boundaries in syntax.andP;  There is much psycholinguisticevidence for such boundaries in human sentence processing.andP;  Click studies[12] and eye-fixation studies [6] indicate that reading comprehensionactivity increases at phrase and clause boundaries.andP;  Structurally ambiguousunits retard processing time up to clauseclosing boundaries, beyond whichthey do not [2].andM;Marcus' model makes no provision for noticing legitimate structuralambiguities, e.g., They are flying planes.andP;  In a footnote, Marcus doessuggest that his processor could flag any output that is potentiallyambiguous, but &quot;some external mechanism will then be needed to force theinterpreter to reparse the input, taking a different analysis path&quot; [20, p.andO;13].andP;  Instead of ruling out nondeterminism, boundary backtracking reins itin.andP;  It remains sensitive to some (but not all) ambiguity.andP;  So it discoversthe second interpretation of They are flying planes after it returns to theMidClause0 register.andP;  Boundary backtracking does forget many ambiguities, asin garden-path sentences like (12).andP;  Whether humans retain just thoseambiguities that the boundary backtracking model does is an interestingempirical question.andP;  The computational import is that there will be far fewerpossibilities for a processor to consider.andM;Some readers may object that a model that simply rejects garden-pathsentences is too stringent.andP;  Indeed, Marcus notes that native speakers canunderstand garden-path sentences with conscious effort, &quot;A higher levelproblem solver uses a set of grammatical heuristics .andP;  .  .  to discoverwhere the processor went astray&quot; [20, p.andP;  205].andP;  As with globally ambiguoussentences, Marcus' solution smacks of a homunculus.andP;  The &quot;external mechanism&quot;or &quot;higher level problem solver&quot; is far more powerful than the sentenceprocessor itself, since it is able to &quot;force&quot; the processor to behavedifferently.andP;  Church [8] suggests adding an ad hoc horse-racing rule to thegrammar.andM;The boundary backtracking model suggests a simple and general heuristic.andO;Suppose we allocate one more register, Extra.andP;  The processor can then accepta gardenpath sentence by saving, in Extra, a state from another boundaryregister-just as it is about to be reused.andP;  For example, when the processorfails to recognize sentence (12), it can start over, only this time copyingthe contents from PhraseO to Extra before it gets reused.andP;  It can thenbacktrack to Extra as it would to a state in any other boundary register.andO;This strategy adds no significant complexity to the algorithm.andP;  It has thevirtue of allowing the processor's coverage to degrade gradually, in a mannersimilar to human performance [8].andM;Finally, the boundary backtracking algorithm is reversible, Boundariessupport re-starts for natural language generation as well as parsing.andP;  Whenspeakers stammer or rephrase their speech, they appear to resume atboundaries [29).andP;  Boundary registers can help a computational generator byproviding a small number of definite states at which failed attempts mayresume,andM;AdjunctionandM;RVG's approach to right-embedding, discussed the previous section onwh-questions, predicts the awkwardness of sentences like these:andM;(21) Mary sang a song that she had learned in EuropeandM;before the war to her children.andM;(22) I called the guy who smashed my brand new car aandM;rotten driver.andM;Sentence (21) right-embeds upon reaching the complementizer that.andP;  By then,all obligatory constituents in the matrix clause have appeared.andP;  Later, theprocessor cannot adjoin to her children to the matrix clause.andP;  This policy issimilar to Kimball's principle of early closure [15], or Cowper's &quot;PokerPrinciple&quot; [9]-once the obligatory cards are on the table, one cannot pickthem up again.andM;Though sentences (21) and (22) are awkward, they are certainlycomprehensible.andP;  Cowper notes that the lost clause &quot;must be retrieved fromsome kind of less immediate memory storage in order for the last constituentto be added [9, p.andP;  46], She does not explain the nature of this auxiliarystorage.andP;  It is, I suggest, a boundary register.andP;  When the processorright-embeds, it also saves its state in register CloseClause0.andP;  Ifnecessary, it can retrieve this state later.andP;  This behavior is similar toright association [8, 11 ].andP;  The RVG processor prefers to attach material tothe current clause, but if necessary can explicitly resume a clause from aCloseClause register, in order to add a late adjunct or optional argument,Explicit resuming is a failure-driven strategy.andP;  Unlike backtracking, it doesnot actually return to a prior position in the input stream.andP;  Instead, itprocesses the current word with an earlier syntactic state-at the end of anearlier matrix clause.andM;PLANNED IMPROVEMENTSandM;Thus far this article has presented a basic architecture for syntax as ascheduling system, ordering sequences of events in time.andP;  (Indeed, it is ageneral-purpose automata, with applicability for scheduling any comparablycomplex sequence of events [3].) There are of course many other issues forsyntax, which is only part of natural language as a whole.andM;SubcategorizationandM;The current scheme only provides for major categories.andP;  Subcategories couldbe represented by multiplying major categories, but it is useful to avoidsuch redundancies.andP;  For example, the verb put requires a locative phrase-onecannot say *I put it.andP;  We would rather not have to introduce distinctionsbetween all the various kinds of verbs and their complements at the majorcategory level.andM;A better approach is to factor out subcategories, as non-lexical productionsthat precede the major lexical productions.andP;  In other words, subcategoryproductions will be semi-lexical.andP;  Like non-lexical productions,subcategories do not consume (or produce) words; nevertheless, lexicalentries must be able to specify their subcategories.andP;  Each lexical entry willhave a category set (implemented as a bit vector): the set of productionsthat may fire up to this word.andP;  For example, the lexical entry for put willinclude in its category set (among other things) a non-lexical subcategoryLOCREQ.andP;  This production fires before the lexical production for the verb.andO;The change vector of LOCREQ turns on a feature in the state register, +LOC.andO;The clause terminator production, CLOSE, requires that this feature be turnedoff, which only a production recognizing a locative phrase can do.andM;Adding category sets to the lexicon constrains nonlexicals generally.andP;  Forexample, phrase-opening productions require words that could legitimatelyopen phrases-determiners, adjectives, nouns, etc., but not tensed verbs,prepositions, etc.andP;  Specifying constraints in this form can fine-tune thedescription and performance of grammars.andP;  Category sets also provide a way toguarantee that a non-lexical production will fire at most once between words.andO;This guarantee eliminates the possibility of infinite cycles between words,and makes local parallelism tractable.andM;AgreementandM;RVG is unusual in that features control precedence relations.andP;  In most otherformalisms, features enforce agreement, e.g., determiner and head noun mustagree in number.andP;  RVG could include agreement features in its state vectors.andO;DET-PL turns on a feature +PL which rules out NOUN-SG, etc.andP;  This would leadto some redundancy among productions, though, multiplying every combinationof productions by every possible way they might agree.andP;  A better approach isto manage agreement features separately.andP;  In addition to ordering vectors,each state register configuration includes morphosyntactic agreement vectors.andO;Generalized actions combine agreement values from inflections and lexicalentries with values in the agreement vector.andP;  Such actions cut acrosssyntactic categories and thus avoid duplication.andP;  So long as the agreementregister is finite and reusable, there is no significant increase ofcomplexity.andP;  (The current architecture already provides for an efficienttreatment of inflectional morphology and idioms [17]).andM;ConjunctionandM;Boundary backtracking suggests a systematic way to handle conjunction thatavoids a great deal of redundancy.andP;  Conjuncts typically attach at boundaries:andM;(23) Joe loves Sue and Bob.andM;WordandM;(24) Joe loves Sue and her husband.andM;PhraseOandM;(25) Joe loves Sue but dislikes Bob.andM;MidClause0andM;(26) Joe loves Sue but he dislikes Bob.andM;ClauseOandM;(27) Joe loves Sue and Bob Martha.andM;Clause0andM;As with adjunctions, processing conjunctions may involve trying to explicitlyresume from states available in boundary registers.andP;  Sentence (25), forexample, resumes from the State stored in the MidClauseO boundary, justbefore loves.andP;  To be sure, conjunction may involve more than resuming astate.andP;  As sentence (27) suggests, a conjunction production should alsooption possibilities for ellipsis-in terms of ordering features.andM;StructuresandM;The current system produces as output a linear trace of the productions firedfor each possible recognition of a sentence.andP;  Building diagnostic traces doesnot add any significant complexity to the recognition algorithm.andP;  It simplyconcatenates a record for each production fired to the trace.andP;  Boundaryregisters maintain a copy of a current trace as part of a stateconfiguration.andP;  The fixed finite resources hypothesis obviously restricts thekinds of diagnostic structures RVG can possibly build.andP;  It cannot buildindefinitely center-embedded tree structures, nor can it keep track of allconceivable parses for ambiguous sentences.andP;  These restrictions are notonerous, though.andM;While helpful for grammar development, linear traces are not grammarstructures, nor are they intended as such.andP;  Rather, actions associated withproductions can do the work necessary to support interpretation.andP;  The currentmodel already provides for actions to handle center-embedding and savingstates in boundaries.andP;  To these we are adding a repertoire of actions forcompositional interpretation.andM;The notion of using actions associated with productions to supportinterpretation is a common one, found for example in ATNs.andP;  The problem hereis to limit the complexity of these actions.andP;  The modus operandi with respectto actions is that they, too, abide by the fixed finite resources hypothesis.andO;The syntactic categories and ordering positions of natural languages areclosed classes.andP;  That is why RVG can schedule productions with a smallgrammar and a fixed finite register configuration.andP;  Similarly, affixes,pronouns and grammatical roles are also closed classes.andP;  This suggests thatagreement, anaphora and predicate-argument calculus are also susceptible toprocessing by a small repertoire of generalized actions and a fixed finiteregister configuration.andP;  While long-term memory is presumably quite large andnot necessarily real time in response (that's why people need external memoryaids), short-term memory is quite limited and thus real time.andP;  The key toreal-time sentence processing is determining the structure of this short-termregister configuration,andM;Our approach draws on ideas from other researchers who have sought to limitcomputational performance capability to finite state.andP;  Church [8, p.andP;  66]dealt with the problem of keeping track of ambiguous prepositional phrases.andO;Rather than maintaining separate interpretations for each possible attachment(which could lead to an exponential growth in state space), Church advocateda pseudo-attachment strategy.andP;  To quote his thesis: &quot;YAP has a marked rule topseudo-attach (attach both ways) when it sees both alternatives and it cannotdecide which is correct.&quot; Martin et al.andP;  [21, p.andP;  279] apply this idea toseveral notoriously ambiguous constructs, including reduced relatives,conjunction and nounnoun modification.andP;  &quot;The approach taken here is toflatten the syntactic structure of these phrases.andP;  .  .  .  In this way, theparser will not waste time trying all possible bracketings; it will becontent with a canonical one that represents them all.&quot; Basically, thisamounts to a strategy of least commitment: if there is a locally unresolvableambiguity, then allow an alternative category that explicitly leaves thingsundetermined, This attitude is crucial in a processor with restrictedresources.andP;  To take a simple example, the words the and sheep are ambiguousor undetermined, depending on how one looks at them, with respect to number.andO;If they are ambiguous, the processor must consider singular and pluralpossibilities separately.andP;  If they are undetermined, on the other hand, itdoes not have to decide.andP;  The processor generates just one canonicalrepresentation, which it may refine subsequently,andM;ANALYSIS OF COMPLEXITYandM;What is the time complexity of RVG? There are two considerations: 1) the sizeof the processor and 2) the size of the grammar.andP;  The RVG performance modelmakes important improvements in both of these dimensions: processor size isfixed as a small number of boundary registers, and grammar size is held downby ternary vector functionality.andM;Processor size is the number of boundary registers.andP;  Grammar size is thenumber of categories, c, which must be considered at each state.andP;  Factoringout c, it can be shown that boundary backtracking is linear with respect toinput.andM;Consider the simple case where just a single boundary register allowsbacktracking to grammatical boundary B.andP;  In a sentence of n words (call themw sub i, W sub 2, .....andP;  , w sub n,), B can be crossed from 0 to n times.andO;Suppose B is crossed exactly once, at arbitrary word w sub i:andM;w sub l,w sub 2.....w sub i-1, w sub (b over 1)....w sub nandM;Words w sub l through w sub (i-1) will be considered just once (since nostates are saved for backtracking prior to word w sub i).andP;  Words w sub ithrough w sub n, can be considered, in the worst case, c times, where c isthe number of alternatives stored in the boundary register associated with B.andO;The worst case occurs when, for each of the first c - 1 alternatives in B,the recognizer goes all the way to the last word of the sentence (w,) beforefailing and backtracking to word w sub i:andM;T = t(w sub l .andP;  .  .  W sub (i-1)) + C*t(w sub i...andP;  w sub n) less or equalandM;c*t (w sub 1....w sub n)andM;T is 0(n), or proportional to the length of the inputandM;string, whatever c might be.andM;Now consider the general case, where B is crossed x times, 0 less or equalthan x less or equal to n.andP;  Here is where the policy of reuse comes in, everytime B is crossed.andP;  Let w sub i1,W sub i2 ......, W sub ix, (1 less or equalil andless; i2 andless; .andP;  .  .  andless; ix andless; n) be the words at which B is crossed:andM;W sub i .andP;  .  .  W sub (il-1) W sub B/il .andP;  .  .  W sub (i2-l)W sub B/i2andM;W sub B/ix......W sub nandM;Again, words w sub i through w sub(il-1) are considered just once, since nostate is saved prior to the first crossing of B. Words w sub il through Wsub(i2-1)may be considered, in the worst case, c sub l times, where c sub lis the number of alternatives saved in B.andP;  The worst case occurs when, foreach of the c 1 - 1 interpretations, w sub il throughW sub(i2-1) areconsidered before a failure occurs (at i2 - 1).andP;  The processor must thenbacktrack to word w sub il in order to get the next interpretation.andP;  Notethat words w sub il through Wsub (i2-1) can be considered no more than c subl times.andP;  As soon as Wi2 is accepted, backtracking to any word prior to W subi2 will be impossible, since a new State will then be saved in B,&quot;forgetting&quot; any state from before W sub i2.andP;  Similarly, Wi2 through W sub(i3-1) are considered, in the worst case, c sub 2 times (where c sub 2 is thenumber of alternatives saved at word w sub i2).andP;  And so on through words wsub ix to w sbu n, which can be considered no more than cx times.andP;  Thus, themaximum recognition time is:andM;T = t(w sub l .andP;  .  .  w sub (il-1) + cl*t(w sub i1 .andP;  .  .  W sub (i2-1)andM;+ c2*t(w sub i2 ...andP;  w sub i3-1) +....andP;  + Cx*t(w sub ix ...W sub n,)andM;less or equal c*t(w sub l .andP;  .  .  w sub n,), where c' = max[c1, c2, ...andP;  cx]andM;When only a single boundary register is used, recognition time is 0 (n).andM;The case of b boundary registers is a further generalization of this result.andO;Registers BR sub 1 through BR sub b (ordered as some permutation of BR sub 1,BR sub 2, BR sub 3, .andP;  .  .  , BR sub (b-1, depending on the order in whichthe associated boundaries are crossed) each appear in the Resume array atmost once.andP;  For each of ci alternatives saved in BR, (the first registerpushed into the Resume array), the recognizer may consider at most c sub 2interpretations saved in BR2 (the second register pushed into Resume).andO;Similarly, for each interpretation in BR sub 2, the recognizer can considerat most c sub 3 interpretations saved in BR3, and so forth.andP;  Thus, themaximum number of times any word w sub i may be considered when using bboundary registers is cl*c2*c3* .andP;  .  .  *cb.andP;  For a sentence of n wordsrecognized with b boundary registers:andM;T less or equal to cn = 0(n), where c = clc2* .andP;  .  .* cb Therefore, with bboundary registers, recognition time is still 0(n).andM;The significance of this result is that it avoids the potentially exponentialblowup associated with simple unbounded backtracking, which never &quot;forgets&quot;any states.andP;  Note that local parallelism is quite tractable (so long asgrammar size is), but global parallelism is not, at least not for arbitrarilylong, complicated sentences.andM;The second dimension is grammar size, which also has a significant impact onprocessing time [1).andP;  RVG is efficient with respect to grammar size in tworespects.andP;  First, it eliminates a great deal of redundancy found informalisms that overly commit to strict linear precedence.andP;  Thus the numberof productions is comparatively small.andP;  As it should be: the syntacticcategories of a language are a closed class.andP;  With p productions (each withtwo vectors) and f features, an RVG of size p*2 *f can represent a statespace with at least f sup f, FSA transitions.andP;  (The number of FSA transitionsmay be even greater if there are iterative productions, with no correspondingincrease in size for the equivalent RVG.) Second, constraint propagationthrough state vectors sharply reduces the size of the reachable state space.&quot;andM;Here is how Reed [25, p.andP;  38] describes the efficiency of RVG:andM;The vector of one of the RVG states may absorb, representationally, manyadditional productions.andP;  Thus, the RVG-based parser offers the efficiency ofbeing less sensitive to the branching factors of grammars.andM;Holding down branching factors is a direct consequence of holding downgrammar size.andM;What about the number of features? The number of ordering features f shouldalso be small: the left-to-right ordering positions of a language are anotherclosed class.andP;  We may estimate f as the sum of L + N, where L is the numberof local constituent positions and N is the number of non-local constraints.andO;L has to do with the linear precedence of categories; e.g.andP;  in English,subject appears before predicate, quantifiers precede adjectives, etc.andP;  Notethat L is somewhat smaller than the number of categories, since manycategories share the same temporal position.andP;  For example, many categoriesmay appear in the predicate position-verbs, prepositions, adjectives, etc.andO;Instead of many state-nodes, there is just one feature representing thisabstract position.andP;  N has to do the non-local constraints betweenconstituents, such as gapping and constraints on gapping.andP;  Note that it ispossible to eliminate redundancy here as well.andP;  We have seen, for example,that the Complex NP Constraint requires no extra features.andP;  Both L and N arethus reasonably small, so f will be as well.andP;  Crucially, there is nocombinatorial redundancy here.andM;EMPIRICAL RESULTSandM;I have conducted computational experiments to quantify the efficiency of theRVG algorithm.andP;  The design of these experiments is similar to those of Tomita[32, chap.andP;  6] and Reed [25].&quot;,andM;The experiments investigate the relationship of parsing times to sentencelength, sentence ambiguity and grammar size.andP;  Parsing time, for Tomita, isCPU time minus time for garbage collection.andP;  Measuring time in this manner,though of interest, is problematic, since much depends on CPU capabilitiesand details of implementation.andP;  Moreover, garbage collection is a significantfactor.andP;  The RVG architecture generates no garbage, whereas the Earley,Tomita, and Reed algorithms have considerable memory overhead.andP;  (To getpolynomial time they keep track of all partial constituents in memory.)andO;Therefore, following Reed [25] , I also measured abstract machine time-thenumber of state changes.andP;  Grammar size and memory requirements are alsoinstructive in evaluating efficiency as well as learnability ofarchitectures.andM;Two experiments compared th RVG formalism with efficiency context-freeparsers.andP;  The first one involves a grammar and sentence set (1 less or equalto n) obtained by the following schema:andM;det noun verb det noun E.g., The robot saw a cat in the parkandM;(prep det noun) sup n-1andM;with a telescope .andP;  .  .andM;Tomita showed that parsing time for this algorithm grows polynomially withrespect to sentence length.andP;  I predicted linear growth for RVG with boundarybacktracking.andP;  The first experiment confirms this prediction, both in termsof CPU and abstract machine time (see Figures 8 and 9).andP;  The secondexperiment involved implementing an RVG grammar with the same coverage asTomita's Grammar III, for which he provides a testbed of 40 sentences(Appendix G).andP;  These sentences, taken from computer science textbooks,average 11.2 words in length, and involve a variety of syntactic structures:relative and reduced relative clauses, gerunds, infinitive and thatcomplements, sentential, conjuncts, noun-noun modification, etc.andP;  Thisexperiment further corroborates RVG's efficiency (see Figures 10 and 11).andP;  Interms of CPU time, RVG is performing about an order of magnitude faster thanTomita's algorithmin compiled C on an IBM AT versus interpreted Lisp on aDEC-20.andP;  In terms of abstract machine time, RVG is performing one to twoorders of magnitude faster (based on statistics from [25].andM;RVG gains efficiency by a) ruling out spurious ambiguities by constraintpropagation and b) losing potential interpretations to reuse.andP;  The RVG modelweeds out many structurally plausible but uninteresting interpretations.andO;Indeed, of the forty sentences in Tomita's testbed, only one is reported asambiguous: Time flies like an arrow, The parser gets the proverb in whichtime is a noun as well as an analysis in which time is an imperative verb (bybacktracking to the OpenClause boundary).andP;  It does not report interpretationsin which time is a denominal or adjective modifying flies.andP;  Thesepossibilities are available locally, while in the noun phrase, but are lostby the time the parser reaches the end of the sentence.andP;  Getting the rightinterpretation will, to be sure, require integration of syntax and semantics.andO;Semantics must choose the preferred interpretation while it is available,locally.andM;Perhaps as significant as its efficiency in time is that the RVG algorithmpre-allocates all memory resources, statically.andP;  This eliminates the overheadof dynamic memory allocation and garbage collection.andP;  Polynomial algorithmssuch as Earley's and Tomita's have a high memory overhead, maintaining achart or graph for all possible interpretations.andP;  Hence the RVG algorithmlooks especially promising for real-time applications running in relativelysmall machines.andM;RVG eschews transition diagrams and phrase structure rules, as well as thetrees which become great forests in Tomita's algorithm, It is significantthat Tomita's grammar badly overgenerates, ignoring issues of agreement andgapping.andP;  These are of course difficult to express in terms of simplecontext-free rules, but straightforward in RVG.andP;  Tomita's grammar has about220 rules, while the RVG version has 60 rules, each with two vectors of 30features.andP;  An important prediction is that whereas context-free grammars willget enormous as coverage increases, the size of an RVG will grow very slowly.andO;Other formalisms trade more computational complexity for less grammar size.andM;Both the features and productions of RVG are susceptible to parallelism.andP;  Thecurrent RVG processor, running on conventional computers, already exploitsfeature parallelism.andP;  It sees all the features in a vector at once, as awhole, just as a simple FSA sees all the bits in a state-symbol, whole.andP;  Theonly difference is that RVG looks at the bits with a ternary rather than abinary logic.andP;  The operators of the ternary vector logic look at all featuresin a vector in parallel.andP;  On binary computers, ternary vectors areimplemented as a pair of bit vectors, and ternary operators exploit thelow-level parallelism of bit-wise operators, e.g., on a 32-bit machine, 32features at a time.andP;  (Such micro-parallelism is of course plausible in neuralmachines as well.)andM;Because RVG represents more information about syntactic state in the stateitself, it is able to be quite compact in its representation of naturallanguage grammars, and hence quite efficient in processing them.andM;Acknowledgments.andP;  RVG originates in unpublished work by A.E.andP;  Kunst, to whomI am also grateful for help with this article.andP;  Brad James helped develop theanalysis of complexity.andP;  Thanks also to Edwin J.andP;  Kay for help withsubsequent drafts.andO;</TEXT></DOC>