<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-239-900  </DOCNO><DOCID>08 239 900.andM;</DOCID><JOURNAL>UNIX Review  March 1990 v8 n3 p42(7)* Full Text COPYRIGHT Miller Freeman Publications 1990.andM;</JOURNAL><TITLE>A faster data delivery. (fiber-optic Unix networking)</TITLE><AUTHOR>Partridge, C.andM;</AUTHOR><SUMMARY>The Fiber Distributed Data Interface (FDDI) standard promises todramatically increase data transfer rates possible on Unixnetworks.andP;  FDDI products are now available that offer speed of100M-bps, and a few areas offer 1G-bps fiber links.andP;  Issuesinvolved in making the new standard useful are discussed.andP;  FDDIspeeds are difficult for a system to keep up with and complex tointegrate with other networks.andP;  Operating-system protocols havehistorically been a bottleneck in data flow, but new TCP/IPimplementations capable of 600G-bps data transfer have beendemonstrated.andP;  Finding a fast interface is now a major difficulty.andO;Integrating FDDI with existing networks can be awkward becauseEthernet bridges and FDDI bridges transmit media access controllayer addresses in reverse bit order.andP;  FDDI is not fast enough forsome applications.andP;  Cell switching and other new techniquespromise 1G-bps networks in the future.andM;</SUMMARY><DESCRIPT>Topic:     Fiber Distributed Data Interface StandardFuture of ComputingUNIXNetwork ManagementData Transfer RateWide Area NetworksLocal Area Networks.andM;</DESCRIPT><TEXT>A FASTER DATA DELIVERYandM;Many computer networking technologies now coming on the market allow for datatransmission at speeds far exceeding those of existing, installed systems.andO;To begin with, the Fiber Distributed Data Interface (FDDI) products nowavailable offer data-transfer rates of 100 megabits per second--a tenfoldimprovement in throughput over Ethernet.andP;  Faster wide-area networks are alsobecoming more affordable.andP;  T1 links have largely replaced 56-kilobit links inmany wide-area networks.andP;  And microwave links have been used to build the NewEngland Academic and Research Network (NEARnet), a 10-megabit metropolitannetwork in Boston.andP;  In a few localities, it is now possible to buy fiberlinks capable of one-gigabit-per-second data rates for little more than theprice of a T1 link.andM;Given that network speeds are taking this leap forward, and products thatcontribute to or utilize higher speeds are finding wider use, now is a goodtime to examine the implications of &quot;faster&quot; networks and the technology thatsurrounds them.andP;  In the first part of this article we will examine FDDIspecifically, and consider the challenges to be faced in making FDDI usefulover the next few years.andP;  In the second part we will look at the much fasternetworks currently in the lab, and discuss some of the issues involved inbringing them out of the lab and into the market during the 1990s.andM;The Fast Network of Today.andP;  Because it is a developing standard and iscommonly implemented in chip-sets already, it is safe to assume that FDDIwill become the most widely adopted high-speed network technology over thenext few years.andP;  Accordingly, it makes sense to look at the benefits--and,more importantly, the challenges--that FDDI presents.andP;  (For a detaileddescription of FDDI, see Steve Cooper's article, &quot;FDDI and the NextGeneration of LANs&quot;, UNIX REVIEW, Vol.andP;  7 No.andP;  2.)andM;The key benefits of FDDI are clear.andP;  Thanks to its use of optical fiber, itoffers a dramatic increase in throughput and a substantial improvement inerror rates over previous networking schemes.andP;  Furthermore, FDDI isculturally compatible with popular IEEE 802 LANs, and thus offers high speedin a relatively familiar package.andM;Some of the challenges FDDI presents are still being discovered asresearchers, vendors, and users begin to implement systems based on it.andP;  Thefollowing problems have already appeared:andM;* Keeping up with FDDI speeds.andP;  Moving data at 100 megabits per second is achallenge through most layers of the networking stack.andP;  If users are to seeanything close to the theoretical FDDI transfer speeds, our networkingsystems must be tuned to achieve high speeds throughout those layers.andM;* Integrating FDDI with other networks.andP;  FDDI has not appeared in anetworking vacuum.andP;  Most sites have several installed networks and plan toadd FDDI to the mix.andP;  The problems of integrating FDDI into currentnetworking environments are somewhat more complex than originally expected.andM;* FDDI isn't fast enough.andP;  A paradox of FDDI is that while it challenges usto improve our systems to keep pace with it, there are some computing systemsfor which the 100-megabits-per-second rate of FDDI is much too slow.andM;Keeping Up With FDDI Speeds.andP;  Figure 1 shows how the seven layers of the OSInetwork stack are often distributed across applications, the operatingsystem, and the network interface.andP;  For a system to achieve optimum FDDItransfer speeds, all three modules must be capable of supporting high datarates.andM;Until about a year ago, it was widely believed that the operating system wasthe key bottle-neck in the flow of data through this triad.andP;  Typicalimplementations of transport and network protocols like TCP/IP and TP4/CLNPachieve speeds that are often only one-third to one-fifth the nominal10-megabit-per-second capacity of an Ethernet.andP;  Based on this evidence,several researchers (not unreasonably) concluded that protocols like TCP andTP4 must be too &quot;heavyweight&quot; to achieve high speeds, and began to work onnew, &quot;lightweight&quot; protocols intended to reduce protocol-processing times.andO;Some of these newer lightweight protocols are designed to be simple enough toput into hardware.andM;Last year, however, Van Jacobson of Lawrence Berkeley Labs demonstrated anoptimized TCP/IP implementation that allowed a diskless Sun-2 workstation touse the entire effective bandwidth of an Ethernet (about 9 megabits persecond, taking headers and timing constraints into account).andP;  The TCP code inthis implementation averages fewer than 20 instructions per TCP segment.andP;  Atabout the same time, Dave Borman announced interim results of his work tooptimize TCP/IP on a Cray supercomputer--the system had achieved a stunningrate of over 600 megabits per second, with further room for improvement.andM;The most important result of Jacobson's and Borman's announcements has been asubstantial change in views about where the networking bottlenecks lie andwhat it means to make a protocol fast.andP;  Based on their work, this authorestimates that new RISC CPUs in test production could support a TCP/IPimplementation that sent and received data at speeds in excess of one gigabitper second.andP;  Furthermore, researchers working on lightweight transferprotocols have found that in order to offer a serious performanceimprovement, they must develop protocols that can be implemented within afinger-count's worth of instructions.andM;In current research, considerable attention is focused on the performance ofthe network interfaces themselves, the board, or in some cases, simply theconnector and chip-set that attach a system to the network.andP;  In his work,Jacobson discovered that some Ethernet chip-sets could not deliver10-megabit-per-second throughput--their circuitry was sufficiently complexthat they could not move data off the network and through to the operatingsystem at full Ethernet speeds.andP;  Interface speeds may be a problem for FDDIsystems, as well; engineers working with some of the FDDI chip-sets havealready mentioned that parts of the chip-sets are &quot;slow&quot; and cannot achieve100-megabit-per-second throughput.andP;  Another related problem is that manycomputer buses aren't capable of supporting a device doing such rapid directmemory access.andP;  So, even if the interface is up to speed, it may not be ableto feed the operating system fast enough.andP;  In other words, the battle tomaintain FDDI speeds may be lost even as data enters a system.andM;If we get past the twin hurdles of finding a fast interface and properlytuning the protocols supported in the operating system, there is still oneremaining impediment: the application.andP;  In many systems, the higher layers ofthe protocol stack are built into the applications.andP;  And, surprisinglyenough, we still don't know very much about how these higher layers perform.andM;The only area of the higher layers that we understand with regard to itsperformance is the data-format conversion part of the presentation layer.andO;Data-format conversion is a process critical to the transfer of binary data(integers, arrays, and records, for example) between machines with differenthardware formats.andP;  Clearly, at some point in the transfer path, data in thesending machine's format must be converted to data in the receiving machine'sformat.andP;  In practice, this conversion is typically done with the use of anintermediate format--an external data format--which is understood by the bothmachines.andP;  The sending machine converts its data into the external format andthen sends the data to the receiving machine, which converts it from theexternal format to its own internal format.andP;  Provided the data-conversionrules are well defined, the meaning of all data should be preserved throughthe two conversions.andM;A key problem for those people interested in performance is deciding which ofthe various external data formats to use.andP;  The OSI external data format,known as Abstract Syntax Notation One (ASN.1), is widely considered the mostflexible and general of the data formats, but it is also believed to be twiceas expensive to process as the other major formats, including XDR, NDR, andCourier.andP;  Application designers continue to struggle with the question ofwhether the added flexibility of ASN.1 is worth the potential performancecosts.andM;Integrating FDDI with Existing Networks.andP;  Because FDDI is IEEE 802-compliant,it was originally thought that integrating FDDI with other networks would beeasy--both routers and 802 bridges would function, and any remainingintegration problems would largely solve themselves.andP;  Now that FDDI hasbecome a reality, however, engineers have discovered that it is not as easyto link to other networks as was first thought.andP;  In particular, there areproblems with bridges.andM;There are two ways to link two networks together.andP;  One method useslayer-three routers (also known as gateways), which connect networks byforwarding data sent using layer-three protocols.andP;  Routers can only forwardlayer-three protocols that they understand.andP;  Bridges act as transparent,layer-two connectors: packets on one side of a bridge are forwarded to theother side based on their layer-two (Media Access Control, or MAC) address.andO;Thus, bridges are a sort of universal link, capable of moving packetsindependent of the layer-three protocol encapsulated inside them.andP;  Bridgesand routers move packets from one network to another at roughly the samespeeds, so that choice between them is largely a matter of the localenvironment.andM;Originally it seemed that, when adding FDDI to an existing mix of 802networks (especially Ethernet), the best method for integration would be abridge.andP;  A bridge would allow the FDDI network to be inserted seamlessly as,for example, a high-speed backbone among many corporate networks.andO;Unfortunately, two critical problems have appeared.andM;First, FDDI transmits the MAC address in a bit order that is the exactreverse of Ethernet's.andP;  In other words, what Ethernet reads as the first bitof the address, FDDI reads as the last bit.andP;  So, if a packet is exchangedbetween an Ethernet host and an FDDI host, the receiver will incorrectlyinterpret the addresses.andM;Initially, people thought it might be possible to solve this problem byhaving the bridge flip the addresses in the packet headers as they went by.andO;But flipping in the bridges doesn't work because some protocols, like theAddress Resolution Protocol (ARP), carry addresses in their data, where thebridge can't see them.andP;  A better solution, suggested by researchers atDigital Equipment Corp., is to have all operating systems use canonical 802bit order (that is, Ethernet bit order) and have the FDDI interfaces covertfrom the canonical bit order used by the system into the FDDI bit order usedon the wire.andP;  In other words, the bit order in FDDI addresses would become alocal matter of how bits are put on the wire.andM;A second problem involves maximum transmission unit (MTU) sizes, which differacross the various 802 networks.andP;  In the case of an FDDI-Ethernetconnection--which appears to be the bridge most vendors want to build--FDDIhas an MTU over three times the size of Ethernet's.andP;  As a result, it ispossible that a host on an FDDI network may send a packet too large for abridge to put on an Ethernet.andM;A couple of solutions to this problem have been proposed--neither of themsatisfactory.andP;  One approach is to force hosts on the FDDI part of the networkto limit themselves to packets no larger than the Ethernet MTU.andP;  The problemwith this is that, to achieve high bandwidth, it is better for hosts to usethe largest packet size possible, so that the cost of the packet header andper-packet processing is amortized over the largest amount of data possible.andO;By limiting hosts on the FDDI network to samller packet sizes, we would bemaking it harder for them to achieve high data rates.andP;  And we have probablyput those hosts on the FDDI network (rather than on the Ethernet) preciselybecause they need the higher speeds!andM;The second proposed approach is to have higher-layer protocols detect whichhosts are on the Ethernet and which are on the FDDI network, and vary themaximum packet size depending on the destination of the data.andP;  While ignoringthe complexities of maintaining information on the multiple packet sizes onthe network, this approach still has the attendant problem that it depends onhigher-layer protocol support.andP;  However, the key advantage of using bridges,which we want to preserve, is their independence from higher protocols!andM;Note that we can avoid the problem of MTU size entirely by using our FDDInetwork as a backbone network only.andP;  Provided we never put any hosts on theFDDI network, and never connect two FDDI networks to an Ethernet, the MTUproblem will never appear.andP;  But this would be a very limited application ofFDDI.andM;Indeed, the problem of MTU size is sufficiently difficult that some expertshave suggested that the whole idea of bridging FDDI to other 802networks--and to Ethernet in particular--is impracticable, and that we shoulduse routers instead.andP;  Because they are designed to interconnect heterogeneousnetworks, routers must support the ability to adapt to different MTU sizesusing network-level fragmentation and reassembly.andP;  Newer routers are alsocapable of switching more than one protocol (IP, OSI CLNP, and XNS amongthem) at once, so with a little luck you may be able to use a router toswitch all the traffic among your LANs.andM;FDDI Isn't Fast Enough.andP;  Earlier we discussed problems encountered in gettingcomputer systems to keep up with FDDI's 100-megabits-per-second data rate.andO;It is important, however, to observe that there are some systems for whichFDDI is too slow.andP;  Here are a few examples to consider:andM;* Cray's TCP/IP networking capabilities enable it to send data at a rate ofover 600 megabits per second.andP;  Assuming a Cray really has need to send orreceive data at anywhere close to that speed, FDDI is much too slow.andM;* High-quality graphics are becoming an increasingly big part of scientificcomputing.andP;  But one second's worth of the bits required to fill an HDTVscreen amounts to over one gigabit!andP;  Even after compression (which can reducedata size quite a bit) FDDI still isn't fast enough.andM;* Some pundits predict that by the mid-1990s, the standard disklessworkstation will boast a 100-MIPS RISC CPU with 64-bit data paths.andP;  Assumingthe system has memory that keeps approximate pace with its processor (anadvance that admittedly may not be feasible), the workstation will have thepotential to consume over 50 gigabits of data per second.andP;  Although there isreason to believe that advanced caching techniques will prevent theworkstation from having to send to or request from the network anywhere nearso much data, it appears that FDDI may sometimes be too slow to keep up withthe workstation's data demands.andM;To support these kinds of data requirements, researchers are actively workingon higher-speed networks that are capable of achieving data rates of onegigabit per second and faster.andP;  In the next section, we take a brief look atthe kind of technologies currently being examined.andM;The Fast Networks of Tomorrow.andP;  Moving data at one-gigabit-per-second speedsand faster turns out to pose many challenging problems in all facets ofnetworking.andP;  Currently, you'll find researchers in general agreement on onlyone point: it is possible to send data at rates of multiple gigabits (and, intheory, terabits) per second over long distances using lasers and opticalfiber.andP;  Any other matter related to high-speed networks is probably still asubject of polite or heated debate.andP;  Enough research has been done in a fewareas, however, that we are beginning to get an idea of how networks withsuch high data rates might be built.andM;One such area is cell-switching.andP;  Essentially, cell-switching is amultiplexing technique in which 1000-bit packets or cells, rather thanindividual bits, are multiplexed.andP;  Cells are routed through a network offiber-optic links by high-speed, parallel switching devices.andP;  As it turnsout, cell-switching appears to be a very convenient multiplexing method forhigh-speed networks, and parallel switches capable of switching cells overfiber-optic links at multiple-gigabit-per-second speeds are already beingprototyped.andP;  Some of these parallel switches are quite versatile, capable ofhandling both synchronous and asynchronous (connection-oriented andconnection-less) traffic.andM;In addition, cell-switching holds the promise of allowing us to constructunified networks in which voice, video, and regular data can be transmittedover the same network.andP;  Currently, we tend to segregate these types of datainto different networks, such as the telephone system, the cable TV systems,and the nationwide computer data networks.andP;  Such segregation is clearly notoptimal.andM;In the first place, segregating data types necessitates the existence ofmultiple overlapping national or international networks, each of which musthave its own support organization.andP;  Installing multiple, distinct networks toserve the same area is clearly not cost-efficient.andP;  If it were possible toinstall one general-purpose network to serve all needs, in principle everyonewould be able to get their particular network services at a lower cost.andO;Because it is so versatile, cell-switching offers some hope of achieving suchgeneral-purpose networks.andM;Secondly, it is no longer clear whether this segregation affords us anyadvantage, especially as the differences among types of data continue toblur.andP;  As one pundit has said, &quot;If you look real close, it's all digital.&quot;andO;If all of our data is digital, it is a bit surprising that we need distinctdigital networks to transmit what is in each case fundamentally a bunch ofbits.andM;Separate networks were originally considered desirable because differenttypes of digital data were perceived as having markedly differentrequirements.andP;  For example, computer data is typically very sensitive to loss(you don't want your bank-deposit record lost in transit) while voice trafficis relatively insensitive (a little loss usually causes a transient bit ofnoise on the line).andP;  Conversely, computer data has fewer real-timerequirements than voice.andP;  These differences may still derail hopes of asingle, unified data network.andP;  But even if the unification effort fails, itseems likely that cell-switched networks will play a big role in futurehigh-speed networks.andM;Another area to look for interesting developments in high-speed networks isin distributed computing systems.andP;  Gigabit-speed networks may revolutionizedistributed systems by making it easy to share vast amounts of data veryquickly.andP;  At the same time, gigabit-speed networks present a very realchallenge: sharing small amounts of data over high-speed networks isn't verycost-effective.andM;This problem of the effects of high-speed networking is best illustratedusing an analogy.andP;  Suppose we could make cars that travel 100 times as fastas they do now (the difference would be roughly equivalent to that betweenthe speed of today's ubiquitous Ethernets and a gigabit-speed network) whilemaintaining the same economical gas mileage.andP;  Imagine further that weimproved our road system so that you could drive safely at these new highspeeds.andP;  Clearly your life would change.andP;  The drive from Boston to SanFrancisco would take under an hour (even with traffic); with the help of thetime-zone change, you could comfortably attend an early-afternoon meeting inBoston and return to your office in San Francisco to do a few hours' morework before going home.andP;  Taking the kids to visit your folks in Florida wouldbe an easy drive on a Sunday afternoon, no matter where you lived in the US.andM;But there's a problem with this scenario.andP;  Just as accuracy of human responsewould inhibit safe travel at such speeds, the laws of physics prohibit usfrom transmitting data faster than the speed of light.andP;  But today's networksalready transmit data at rates close to the speed of light.andP;  Thus, since wecan't make the data move appreciably faster through the wires, the only wayto get improved data rates is to send more data per unit of time.andP;  To extendour car analogy to reflect a comparable effect, what we would really end updoing is making cars bigger, capable of carrying 100 times as many people oras much baggage for the same gas mileage, but unable to go much faster.andP;  Theimprovement, while still profound, would be of a much different nature.andM;How does this story relate to distributed computing?andP;  As it turns out,there's an odd relationship between the acceleration of data consumption andthe acceleration of data transmission: computers continue to get faster inthe same sense that cars get faster; but networks are only going to getfaster in the sense that cars would get bigger.andP;  So computers will continueto process larger amounts of data faster and faster, and the only waynetworks will be able to deliver those larger amounts of data in a timely wayis by sending ever larger amounts at a time.andM;Present-day distributed systems are generally not designed to encourage thesending of large amounts of data.andP;  Consider the popular abstraction of aremote procedure call.andP;  Typically, systems use RPCs to get relatively smallamounts of data--a block of a file here or a page of memory there.andP;  Returningto the car analogy: on a gigabit network, using RPC calls to transfer data isakin to moving all your house furnishings across the country in the back seatof your sports car, one piece at a time.andP;  We will have to find ways to enabledistributed systems to move much larger quantities of data in onetransfer--entire files (perhaps an entire filesystem), or all the contents ofa remote system's memory.andM;FDDI is here, and while it presents some technical concerns that require ourattention, FDDI should fit pretty comfortably into our current networkingenvironment.andM;Gigabit-per-second networks are waiting in the wings, and we will need themsoon.andP;  But their dramatically higher speeds pose some difficult problems thatmust be solved before they can be widely deployed.andP;  The 1990s will be achallenging time for people involved in data networking.andM;BibliographyandM;A.S.andP;  Acampora and M.J.andP;  Karol, &quot;An Overview of Lightwave Packet Networks&quot;,IEEE Network (January 1989).andM;D.andP;  R. Boggs, J.C.andP;  Mogul, and C.A.andP;  Kent, &quot;Measured Capacity of an Ethernet:Myths and Realities&quot;, Proceedings of ACM SIGCOMM '88.andM;D.andP;  Borman, &quot;Implementing TCP/IP on a Cray Computer&quot;, Computer CommunicationReview (April 1989).andM;D.andP;  D. Clark, V. Jacobson, J. Romkey, and H. Salwen, &quot;An Analysis of TCPProcessing Overhead&quot;, IEEE Communications (July 1989).andM;C.andP;  Partridge, &quot;How Slow is One Gigabit Per Second?&quot;, Computer CommunicationReview (January 1990).andM;Craig Partridge is a scientist at BBN Systems and Technologies Corp., wherehe is affiliated with a project on gigabit network architectures.andP;  He is alsothe editor of ACM SIGCOMM's Computer Communication Review and a member of theInternet Engineering Steering Group.andP;  He received his bachelor's and master'sdegrees from Harvard University.andO;</TEXT></DOC>