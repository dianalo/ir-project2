<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-305-728  </DOCNO><DOCID>08 305 728.andM;</DOCID><JOURNAL>Release 1.0  March 16 1990 v90 n3 p1(13)* Full Text COPYRIGHT EdVenture Holdings 1990.andM;</JOURNAL><TITLE>Information refining. (software to summarize or perform semanticanalysis on textual data) (includes related articles on languageand meaning and how Reuters classifies stories)andM;</TITLE><DESCRIPT>Topic:     Information Storage and RetrievalSoftware PackagesSemanticsTaxonomyClassification SystemsLanguage Analysis.andO;Feature:   illustrationchart.andO;Caption:   Semantic analysis. (chart)andM;</DESCRIPT><TEXT>INFORMATION REFINING With the proliferation of e-mail, electronic publishing,bulletin boards, on-line documentation and the like, the volume of textdistributed and stored electronically is overwhelming.andP;  We have some tools toretrieve individual pieces of text if we know what we're looking for, but weneed better ways to categorize texts and even summarize their meaning.andP;  Wehave covered many of them over the past few years; here's a more conceptualoverview of the issues and some additional, more recent examples.andP;  (See alsoRelease 1.0, 87-9, 87-11, 88-1, 89-3, 89-7, 89-12, 90-2.)andM;Information is intangible and reusable; its value lies in what you make ofit.andP;  The value added by refining information can be far greater than that ofthe raw information itself.andP;  Consider: A name and address is a useful nuggetof data.andP;  That same name and address mentioned casually in a letter isinformation that needs refining.andP;  Finally, that name and address in context-- for instance, in a letter indicating that the individual may be in themarket for a two-story brownstone -- is potentially the msot valuable of all.andO;(There's a fourth possibility: The letter is to a friend and should not berefined: Each nuance has great meaning; he's telling her he's about to moveout.andP;  But we're concerned mostly with commercial information here.)andM;Our topic, &quot;information refining,&quot; is a term coined by John Clippinger ofStarr King Communications (now moving to Coopers andamp; Lybrand to work on thisarea).andP;  More or less, it is the process of converting information ore intosomething useful -- the &quot;refining&quot; of textual information into morestructured information that can be represented and manipulated by computers.andO;(Analysis of data in databases is another matter, and analysis of sound andimages is still mostly beyond us in a commercially realistic sense.)andM;Information can be refined by people -- editors, for example -- but a numberof techniques ranging from string search to so-called natural-languageunderstanding can automate or at least assist the process.andP;  The more regularthe initial format and content, the more efficient the tools can be; the moreirregular, the more value can be added by refinement.andP;  Information can beclassified, organized, filtered/selected and combed for relevant facts.andP;  Oncerefined, the output could be a database of financial transactions or earningsreports or a list of job candidates or a selection of articles for a human toread or a set of grammatical errors for correction.andP;  It could trigger (andprovide input for) the execution of some action, such as entering anappointment in a schedule and sending a letter of confirmation, selecting anews item from a stream of uninteresting news, sending a letter of apology toan angry customer or transferring $2.9 million to an overseas account.andP;  Orselected pieces of it could be assembled into a personalized newsletter ormarketing piece (see Release 1.0, 90-2).andP;  Items concerning the same subjectcan be linked to each other; cross-references can be representedelectronically.andM;We're dealing here with practical problems; see the box across for adiscussion of &quot;meaning.&quot;andP;  The goal of classification or understanding oftexts for our purposes is simply to represent a text as useful, structuredinformation so that it can be manipulated electronically.andP;  In short, thesystem doesn't need to understand the language; it just needs a way torepresent what is relevant about its topic or its meaning.andP;  It strips thetext to find concepts it recognizes and the relationships among them.andM;Topic and summary: Cast and plotandM;The tools divide into two basic but overlapping areas -- ones that deal withwords and concepts, for text classification and filtering, and ones that tryto derive summaries of the content (or &quot;meaning&quot;), generally by parsingsentences and recognizing standard &quot;scripts&quot; or story lines.andP;  In short, thereare ones that determine the text concerns a boy and a girl, and ones that canfigure out the story line -- whether the boy won or lost the girl.andP;  Both canbe useful; the parsers tend to work in far more limited domains, with a smallrange of possible story lines -- for example, mergers and acquisitions,earnings reports, disaster reports for an insurance company, purchase orders,sports scores.andM;Underlying each representation is a model.andP;  The model may be as simple as adatabase or a list of index terms by which text chunks are classified, or ascomplex as a set of rules, a semi-hierarchical tree, a semantic net, anobject-oriented database or a set of situations and analogies.andP;  The model forfiltering and classification systems is generally some set of relationshipsamong lexical items; the model for the parsers is the story lines (or simplycorrect sentences, in the case of grammar/style checkers, which check to seeif the text follows rules but generally make no attempt to determinecontent).andP;  You could say that classifiers attempt to fit each item as aconcept into a model, whereas the summarizers try to create a model ofinteracting concepts for each chunk.andP;  Of course, meaning being what it is,both the means and the results may involve a combination of these things (amixture of concepts, in fact).andM;The four sections following explore four approaches, three for textclassification and the final one for summarization:andM;I - simple queries, no model;andM;II - simple queries, complex model of relationships among concepts;andM;III - complex queries (using rules and parsers), any kind of model; andandM;IV - complex model for representation of individual text items, not just therelations among items.andM;I -- Tools for determining topics (simple queries, no model)andM;The basic tool is plain old string search for text retrieval: you index text-- words, phrases, etc.andP;  -- and then you search by those strings.andP;  (Theindexing has no conceptual role; it's simply a list of all the possiblewords/phrases in the text base, with pointers to where they can be found, andis a convenience for finding and counting words.)andP;  In a simple Booleansystem, a query is just a list of words whose presence indicates the concept.andO;Most systems allow you to edit and store queries and give them names, whichis a convenience for the user.andP;  You can also build more complex Booleanqueries, specifying combinations or the presence of one word within a givendistance from another.andP;  (See Release 1.0, 88-1.)andM;Similarity rankingandM;Next, you can build more complex queries, which may weight words according totheir relative frequency in the overall textbase and the target chunk oftext, co-occurrences of words, and other parameters.andP;  &quot;Similarity&quot; rankingfiguratively builds an N-dimensional space, in which each chunk of text is avector, with its dimensions determined by the number of occurrences of eachword.andP;  (Each word represents one dimension; most text chunks have 0 length inmost dimensions.andP;  Alternatively, you could create a one-dimensional vectorfor each text chunk; each position in the string is a word, and a 1 in anyposition indicates that the text chunk contains the word referred to by thatposition in the string.andP;  This system doesn't weight words at all, but canalso be useful.andP;  See Release 1.0, 88-1.)andM;A variety of formulas are used to determine the &quot;similarity&quot; of any textchunk to any other; figuratively, how close are the vectors to each other?andO;Do they point in the same direction?andP;  And so on.andP;  Each kind of formula willproduce slightly different results, but in the end similar articles arelikely to be so identified.andP;  The vectors can represent either the entiredatabase, an individual text chunk, or the concept/query itself.andP;  This allowsfor a simple form of so-called natural-language query, where the systemregards all the neutral words as noise, and pays attention to those thatdiscriminate one topic from another (in italics in these examples):andM;Will OS/2 ever reach its potential and beat out UNIX?andM;What's the name of the river in Budapest?andM;When did Juan meet Alice?andM;You won't get a direct answer, but you can probably find the answer in theresponses.andP;  The use of &quot;potential&quot; in the first example might pull in somestories about electric circuits, but those can be discarded.andP;  The bettersystems allow for refinement of queries by selecting or rejecting particulartexts the system finds and running a query again -- in essence, the user issaying &quot;weight the words in these texts more&quot; or &quot;weight them negatively.&quot;andM;Most of the industrial-strength systems now use some variant of thesetechniques, including Individual Inc., Dow Jones' DowQuest (which runs on theConnection Machine), and other learn-as-you-go systems which refine queriesby adjusting the weightings of words.andP;  They are all more or less based on thework of Gerard Salton at Cornell University.andM;Missing modelandM;However, the complexity and power of the query mechanism has little to dowith the model of the concept space in which a particular text chunk may beplaced.andP;  The query mechanism has to do with matching the text to a concept;the second has to do with how the concepts themselves are organized.andP;  Allthese systems basically offer one-to-one measures of closeness between theconcept and the target items.andM;There's no model beyond the user's own -- an alphabetical listing, say, andwhatever meaning he assigns to the names he gives to the concepts.andP;  You'rejust measuring the match or the closeness of the queries to each concept.andP;  Inthis context, a &quot;concept&quot; is the input to which target chunks of text arematched: It could be &quot;'Juan' or 'Alice' and 'budget',&quot; or &quot;Please liststories on French wines,&quot; or simply a single word in a book index.andP;  Thus abook index is a list of the concepts covered in the book (and is verydifferent from an electronic index which lists all the words in the book).andM;II -- Structured semantic space (simple queries, complex model of concepts)andM;Now we come to the notion of organizing the concepts into a model.andP;  You maydecide you want more than just answers to queries or rankings of similarity.andO;You want a model of the domain you're studying, and a way of placing the textchunks you examine within it.andP;  In essence, you've got a model of the world,and queries (simple or complex) that identify text chunks concerning thevarious concepts in that world.andP;  Then you can use the query technologiesdescribed above to link target items to concepts in a model of the domain.andM;A thesaurus indicates what words are linked to each other; a semantic netwith numerical values or types on the links between concepts, as in the workof Vladimir Pokhilko (see &quot;Software as medium,&quot; Release 1.0, 89-11), holdseven more value.andP;  Other possible structured models that might be relevant, ifnot universal, are a chronology or a table of contents or a course syllabus.andO;A writer wanting to categorize source material according to a book outlinewould find a book outline useful; a student studying for an exam or teacherpreparing a course could match texts to the syllabus.andM;Note: Hypertext is what you get when you link things.andP;  When you link thingsin some automated way, as described above, you usually get something morestructured and predictable -- such as a hierarchy.andP;  Human-generated hypertextmay be a brilliant piece of work, or it could be a lot of unconnectedrubbish.andP;  If you allow a lot of processes, or a lot of editors or readers youmay get a self-organizing system, a text base with some valuable structurethat couldn't be predicted (cf.andP;  hypertext publishing, Release 1.0, 89-7, ora self-structuring support system such as Answer's Apriori, Release 1.0,89-7, or &quot;agenda-setting,&quot; Release 1.0, 90-2).andM;Verity: Model matches queriesandM;An interesting tool in this context is Verity's Topic, a premier example of asystem that helps users to build hierarchical taxonomies (&quot;topics&quot;) and therelated queries.andP;  It builds and displays relationships between topics andsub- and super-topics.andP;  In Topic, the queries are hierarchically related justas the concepts (or nodes) are: If the concept of &quot;the Soviet Union&quot; includes&quot;Sverdlovsk,&quot; &quot;Kiev&quot; and &quot;Moscow,&quot; then the query for &quot;Moscow&quot; &quot;includes&quot; thewords in the queries for &quot;Sverdlovsk,&quot; &quot;Kiev&quot; and &quot;Moscow,&quot; with weightingsassigned by the builder-user.andP;  You could of course request articles about theSoviet Union but not about Kiev.andM;In fact, Topic is not a true hierarchy but (potentially) a more complexdirected acyclic graph; a node can have multiple parents.andP;  For example, thequery and the concept for &quot;Moscow&quot; could be a node both under &quot;cities&quot; andunder &quot;Soviet Union.&quot;andP;  On the text display, you see only three levels at atime, but if you walk down the Topic tree on a high-resolution Sun display,you may end up seeing duplicate nodes displayed in several locations.andP;  Thusit has implicit rules and weights in the way it combines nodes, but it doesnot easily support the kind of rules with variables (&quot;acquired&quot; followed by acompany name) described below.andM;III -- Complex queries (with rules) for classifying texts by conceptandM;We originally figured that sheer word statistics will always suffice todetermine concepts: You can find a story without the precise words you mightuse if you simply look for co-occurrences and similarity rankings.andP;  Forexample, not every story about France will have the word France in it, butthere will be similarities of people, places, named in it to other storiesabout France.andP;  So if you have a good enough query of weighted words, you candetermine any topic.andP;  With a Connection Machine, for example, you can simplylook for articles similar to a target.andP;  The lack or presence of any singleword isn't deciding; it's a fuzzy sort of match.andP;  It just seems very costlyin terms of resources to employ rules and parsers.andM;Two problems: One, the Connection Machine is still fairly expensive, eventhough (or because?) the software itself is simple.andP;  Two: Ultimately,statistical methods are less precise and less extensible.andP;  Concepts areexplicit (1), and can be quite complex.andP;  From a business perspective, it'sultimately more comforting to define a topic explicitly than to point to anarticle and say, &quot;something like this.&quot;andP;  Moreover, because the rules areexplicit and compact, they are easier to interpret, modify or extend, and tointerpret.andP;  It allows you to define concepts by rules (what a computer doesand is) rather than by lists (&quot;computers&quot; is IBM, Apple, Compaq, Tandem,Cray).andM;Finally, rules are more reusable, since you can change the variables in arule and reuse it, whereas in a word-based approach, you have to create awhole new set of words for each new concept.andP;  For example, compare &quot;Juan,Alice, Fred, Sasha, Brian, Ann, Bill, Bill, Gwen, Arkady, Julia...&quot; with thisrule: &quot;Any person whose employer is Fancy Functions.&quot;andP;  The rule can easily beedited to apply to workers at another company simply by changing the companyname rather than by building a new list; it also doesn't require updatingevery time the boss gets angry and fires someone, or whenever the humanresources director puts a new relative on the payroll.andP;  Rules give you theextensibility that differentiates intelligence from string-recognition,however many strings you may recognize.andP;  In the end, rules are generallyeasier to validate, debug and extend than a statistical approach, althoughlists may be easier to generate and work quite well in simple cases.andM;Measures of accuracyandM;Accuracy is a function of recall -- what percentage of the total matches didyou find? -- and precision -- how many of the ones you found were in factmatches?andP;  What constitutes 100 percent, of course, may be arguable, just asdoctors don't always agree on diagnoses, and tax experts and IRS personneldon't always agree on tax items.andP;  With traditional word-list systems the sumof precision and recall generally totals about 100, whereas with Reuter'srule-based Text Identification System (2) each alone approaches 100 (at leastin the example cited above, for a given domain and after considerable work).andM;Proper names provide a good illustration of the issue.andP;  String-search systemsare very good at finding them -- with 100 percent precision (as long as youinclude synonyms like Big Blue or Ma Bell).andP;  However, the goal is to findstories where they are the focus, rather than a passing mention.andP;  In tests atReuters, for example, TIS got high recall (98 percent) for categories linkedto specific names but lower precision (84 percent), because text items wereflagged whether or not the name was relevant.andP;  &quot;'You're cuter than LizTaylor,' he murmured,&quot; may not imply a story about the film industry.andP;  The135 economic categories, by contrast, were tougher to recall (89 percent)than for precision because of the breadth of the concepts involved and thelarge variety of ways to express them.andP;  Could you flag &quot;Rhymes with Rich&quot; asa story about the hotel business?andP;  That was Newsweek's title for its cover onLeona Helmsley.andM;The Reuters system described across provides such fine discrimination,assigning stories to a variety of carefully constructed classifications.andP;  Bycontrast, Desktop Data is using simpler string search (using customer-definedstrings and information-vendor story classifications as available) on a pc toselect news articles received in real-time over an FM band.andP;  It has signed up15 customers since its launch last month, at $15,000 or so a year, but thatnumber is growing quickly, with Wall Street the quickest to sign up for now.andO;Desktop's NewsEDGE works in real-time more as an alerting mechanism, and isdesigned to be easily modifiable; the user can simply enter a new keyword.andO;But it suffers from traditional trade-offs, typically with high recall andlow precision -- or a lot of extraneous stories that the user can simplyignore.andM;IV -- Determining story lines: Complex models of individual text itemsandM;All the systems above classify text items, so that you can get appropriatepieces of text about any concept defined in your concept model.andP;  But moreinteresting is the notion of summarization, so that you can get precisely theinformation you need, rather than articles about it.andM;Determining a model of the content, as opposed to a model of the concepts inwhich in item can be located, is a tough but worthwile task.andP;  If you want toknow when Federal Express zapped ZapMail, for example, you don't want a bunchof articles about ZapMail; you want a single date -- the date you can'tspecify in a string-search because that's the information you lack.andP;  Or ifyou want to know what restaurant chains Feed'n'Frenzy acquired, you'd ratherget a list than a pile of articles that include &quot;Feed'n'Frenzy&quot; and the words&quot;acquired,&quot; &quot;bought&quot; and &quot;merged,&quot; including one about the purchase of a newFrench-fry facility.andP;  In other words, you want your information not justfiltered but also digested (or refined), with the contents of these variousredundant or irrelevant articles reduced to a model that you can query forprecisely what you need.andM;The basic tools for finding meanings are a variety of pattern-recognizingparsers.andP;  They know grammar rules -- the stuff of traditional naturallanguage/AI -- and they also have models of the domain: scripts,relationships among concepts, standard actions and transactions, etc.andM;One part of the system recognizes linguistic constructs -- verbs and nouns,phrases and antecedents, synonyms and complex clauses.andP;  Grammar rules dealwith parts of speech that the system recognizes through lists and other cues.andO;They also have a customized lexicon of domain-specific synonyms for variouscalculations, field names, and application commands; e.g., salary = pay xhours worked, emp.no = employee number, etc.andP;  (3)andM;Another part of the system looks for domain-specific scripts, or generic&quot;meanings,&quot; such as acquisitions, transactions, or whatever the content mayandM;be   For example: &quot;Country Cousins was acquired by MetroToys for $49andM;million.&quot;andP;  &quot;Please wire $200,000 to Mr. X in Nicaragua.&quot;andP;  &quot;Please book a roomat the Berkeley in London.&quot;andP;  &quot;Earnings fell 10 percent.&quot;andP;  (But we're surethey'll make it up next quarter!)andM;Some commercial productsandM;For example, Natural Language Incorporated's Connector is a tool forconnecting such scripts (as represented by nouns and verbs) with a databasethat holds the model -- i.e.andP;  the relationships implied by the scripts.andO;I.e., &quot;X pays Y&quot; implies a transaction, with an amount, a date, and an object(an action or a thing) that was paid for.andP;  The Connector helps auser/buildder to define terms (pay, salary, particular products that thepeople in this domain might be buying and selling, invoice, bill, due, etc.)andO;and build the model they pertain to.andP;  Each transaction is one line in thedatabase, but the system can answer related queries by joining variousrecords.andP;  For example, &quot;Does anyone who works for a company in Chapter nXIowe us money?&quot;andP;  This system might combine internal company records with adatabase populated by external financial news, set up to collect informationabout companies whose employees we do business with (among other things).andM;The results of such analysis can fill a database or knowledge base, generatea financial transaction, or trigger a letter of response.andP;  The database canalso be used to answer queries or as a source of data for traditionalapplications, as in SCISOR, below.andP;  At this point most such systems are inearly stages or are considered a source of proprietary advantage by theirowners and not discussed; one exception is several installations of CognitiveSystem's ATRANS tool, first installed in July 1986 at Citibank.andP;  (CognitiveSystems was founded in 1981 by Roger Schank, a former Yale professor whopropounded the notion of conceptual dependencies, including scripts, orstandard models of behavior: Juan gives an object to Alice; Alice pays moneyand takes it; ownership is transferred.andP;  Or, Michael is hungry; Michael eatsfood; Michael is no longer hungry.)andM;ATRANS monitors incoming telexes for money-transfer instructions, and is inuse at Citibank, Irving Trust, and Generale Bank in Brussels, among otherplaces.andP;  It has an extremely limited domain -- telexes for money-transfers --but it is able to save banks considerable time and data-entry.andP;  And for banksmore than most of us, time is money.andP;  Paul Callens, assistant head of bankingoperations for Generale Bank, for example, says that the system identifiesand correctly handles 80 to 85 percent of telexed payment orders, whichnumber 120 to 150 a day.andP;  The system automatically generates formattedS.WI.F.T.andP;  messages (Society for Worldwide Interbank FinancialTelecommunications, a banking standard).andP;  About 15 percent are kicked backand once in a while a mistake occurs in the generated messages, so it'snecessary for a human to compare the original telexes to the ATRANS output.andO;Still, use of ATRANS allows the bank to accept properly formatted telexes aslate as 10 am, whereas other messages must come in by 8.30 to be handled thesame day.andP;  (4)andM;GE's SCISOR: A database of financial newsandM;SCISOR stands for System for Conceptual Information Summarization,organization and Retrieval, a project at General Electric's Randamp;D Center inSchenectady, NY.andP;  Developed by Lisa Rau and Paul Jacobs, SCISOR builds adatabase about mergers and acquisitions, using stories from financial wires.andO;Stories are first classified as about, not about or maybe about a takeover,in a four-stage process that starts with headline analysis.andP;  For example,&quot;Dow Jones Stock Averages&quot; and other recurring headlines are automaticallydiscarded.andP;  Then the system uses a keyword filter, looking for words such as&quot;buy,&quot; &quot;takeover&quot; and &quot;acquisition&quot; that indicate a story about a merger oracquisition.andP;  Next, it applies pattern matches, looking for both positivepatterns such as &quot;acquired [COMPANY] for $[AMOUNT],&quot; or negative matches,such as &quot;sale of [COMPANY] unit,&quot; indicating a divestiture rathe than amerger.andP;  Finally, if the system is still unsure it does linguistic andconceptual analysis, knowing that the object of a takeover is a company.andM;At this stage, SCISOR also starts to define the information that will be usedto fill the database.andP;  Take the complex sentence illustrated above: &quot;Reveresaid it had received an offer from an investment group to be acquired for $16a share, or about $127 million.&quot;andP;  By determining that the investment group ismaking the offer, SCISOR decides that it is the acquirer, and that Reveremust be the company being acquired.andP;  It can also distinguish the per-shareprice from the total value of the acquisition.andM;SCISOR does this through the cooperation of two modules, TRUMP (forTransportable Understanding Mechanism Package) and TRUMPET (for TRUMPExpectation Tool).andP;  TRUMP is a general-purpose parser and semanticinterpreter; it finds the filter words, and knows how to interpret them.andP;  Itknows (for example) if an offer is from an object, the object is making theoffer.andP;  TRUMPET, on the other hand, contains the domain-specific model: Itknows that a unit making an offer is the acquirer, and matches words toconcepts.andP;  This is generally sufficient to generate a full-fledged databaseentry, as shown in the form at the bottom of the illustration across.andP;  Andthe system can also handle questions in much the same way, parsing thequestions and deriving the answers from the database.andM;Last year, Rau and Jacobs and colleague George Krupka took the basictechnology of SCISOR to build a second system for classifying Navyintelligence messages as part of a demonstration project for a DARPAconference, and were able to construct a high-accuracy database generator inone person-month.andP;  (How accurate is, so to speak, classified.)andP;  The team hasalso started working with other GE units on possible commercial applicationsof SCISOR, but that is preliminary.andM;SCISOR runs on Sun SPARCstations and takes about 10 seconds per story,compared to TIS's 5 seconds on MicroVAX servers ATRANS takes a second or lessto parse brief messages on a mainframe, about 10 seconds on a LISP machinesand 15 to 30 seconds on a VAX.andP;  Of course, these tasks are completelydifferent and performance is hardware-dependent, but these times indicatethat the technology makes sense for commercial use.andP;  All the systems arewritten in LISP.andM;Other components of meaningandM;Other components of meaning are even harder to determine, and are generallysupplied by a human.andP;  They include:andM;Context: Who's talking?andP;  When?andP;  Where?andP;  About what?andP;  For example, Ha1 istalking about spreadsheets.andP;  Start by defining words such as here and now.andM;Intent: The goals of speaker and listener.andP;  &quot;The cat is in the kitchen.&quot;andP;  Whyis he telling you this?andP;  Should the cat be brought into the living room?andP;  Isit waiting for a walk?andP;  Is it eating the turkey?andP;  Or is that where itbelongs?andP;  &quot;You look great today!&quot;andP;  Why did he say this?andP;  Not a problem toaddress (electronically, commercially) at the moment.andM;Speech transactions: Acts, not just meanings.andP;  &quot;I'll get it to you tomorrow.&quot;andO;&quot;We'll buy them?&quot;andP;  &quot;Please file this report.&quot;andP;  Action Technologies' TheCoordinator asks users to identify their speech transactions more or lessexplicitly, and uses those tags to generate a database that monitors thestatus of interpersonal obligations: Who owes an answer or a work product towhom?andP;  Who has requested something but not received an answer?andM;Overall, we're still at an early stage in dealing with natural text, althoughwe can already go far with structured text that looks natural.andP;  &quot;We stilldon't realize how complex it is,&quot; says Clippinger.andP;  &quot;We're fish; we'reswimming in it!&quot;andM;The moral is: When you use natural lanuage, you need to understand whatyou're talking about, with a model, a (n implicit) context/domain.andP;  Thenatural language part is easy!andM;(1) Yes, concepts are explicit, but &quot;moods&quot; are not.andP;  For some purposes, suchas assessing opinions or morale, evaluating moods can be valuable, but that'sbeside the point here.andM;(2) In a complex assignment of intellectual property, Reuters owns theoriginal rule base it got from Carnegie as well as the specificimplementation (TIS), an extended rule base and communications interfaces,for modification and reuse within Reuters.andP;  But Carnegie Group owns andlicenses the underlying categorization and pattern-matching engine, TextCategorization Shell.andP;  If Reuters were to remarket TIS, it would oweroyalties for TCS to Carnegie.andM;(3) Without the domain-specific content such parsers are useful forgrammar-checking, but you can't have &quot;meaning&quot; without a model of a specificdomain -- and some intention.andP;  Theoretically you could avoid beingdomain-specific by having a model of the whole world and everyone'sintentions, but let's get real!andM;(4) On the other hand, messages in the S.W.I.F.T.andP;  syntax itself can bereceived until 11 am.andP;  You'd think that this is such a limited domain withsuch well-defined transactions (or scripts) that most bankers would simplyadopt S.W.I.F.T.andP;  syntax to save themselves trouble -- and kill the marketfor ATRANS.andP;  In fact, S.W.I.F.T.andP;  has been around since the Seventies but itis still used by only half the world's banks, albeit for 75 about percent oftransactions.andP;  Think of the implications for Electronic Data interchange.andO;Seems like natural language will be with us for a long time!andO;</TEXT></DOC>