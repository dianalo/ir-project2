<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-059-206  </DOCNO><DOCID>07 059 206.andM;</DOCID><JOURNAL>Communications of the ACM  Feb 1989 v32 n2 p183(12)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>High level knowledge sources in usable speech recognition systems.andO;(technical)</TITLE><AUTHOR>Young, Sheryl R.; Hauptmann, Alexander G.; Ward, Wayne H.; Smith,Edward T.; Werner, Philip.andM;</AUTHOR><SUMMARY>MINDS is an integrated speech recognition system developed atCarnegie-Mellon that combines natural language processing withreal-time speech comprehension in problem solving dialogues.andO;While a 'listening typewriter,' which would transcribe whatever ithears with only a short delay, is still a far off goal, progressin bringing it to full development is being made.andP;  An overview ofthe MINDS systems is provided.andP;  It functions in a resourcemanagement domain, featuring information obtained from a databaseof facts about US Navy ships.andP;  MINDS is only a start in theintegration of speech recognition and natural language processing.andO;The experiments described demonstrate the effectiveness of addedconstraints on the recognition accuracy of the speech system.andM;</SUMMARY><DESCRIPT>Topic:     Problem SolvingIntegrated SystemsVoice RecognitionExpert SystemsNatural Language Interfaces.andO;Feature:   illustrationtable.andO;Caption:   Reduction in branching factor and search space. (table)Recognition performance. (table)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>HIGH LEVEL KNOWLEDGE SOURCES IN USABLE SPEECH RECOGNITION SYSTEMSUnderstanding speech is a difficult problem.andP;  The ultimate goal of all speechrecognition research is to create an intelligent assistant, who listens towhat a user tells it and then carries out the instructions.andP;  An appararentlysimpler goal is the listening typewriter, a device which merely transcribeswhatever is hears with only a few seconds delay.andP;  The listening typewriterseems simple, but in reality the process of transcription requires almostcomplete understanding as well.andP;  Today, we are still quite far from theseultimate goals.andP;  But progress is being made.andM;One of the major problems in computer speech recognition and understanding iscoping with large search spaces.andP;  The search space for speech recognitioncontains all the acoustic associated with words in the lexicon as well as allthe legal word sequences.andP;  Today, the most widely used recognition systemsare based on hidden Markov models [HMM].andP;  In these systems, typically, eachword is represented as a sequence of phonemes, and each phoneme is associatedwith a sequence of phonemes, and each phoneme is associated with a sequenceof states.andP;  In general, the search space size increases as the size of thenetwork of states increases.andP;  As search space size increases, speechrecognition performance decreases.andP;  Knowledge can be used to constrain theexponential growth of a search space and hence increase processing speed andrecognition accuracy.andP;   Currently, the most common approach to constrainingsearch space is to use a grammar.andP;  The grammars used for speech recognitionconstrain legal word sequences.andP;  Normally they are used in a strict left toright fashion and embody syntactic and semantic constraints on individualsentences.andP;  These constraints are represented in some form of probabilisticor semantic network which does not change from utterance to utterance.andM;As we move toward habitable systems and spontaneous speech, the search spaceproblem is greatly magnified.andP;  Habitable systems permit users to speaknaturally.andP;  Grammars for naturally spoken sentences are significantly largerthan the small grammars typically used by speech recognition systems.andP;  Whenone considers interjections, restarts and additional natural speechphenomena, the search space problem is further compounded.andP;  These problemspoint to the need for using knowledge sources beyond syntax and semantics toconstrain the speech recognition process.andM;There are many other knowledge sources besides syntax and semantics.andO;Typically, these are clustered into the category of pragmatic knowledge.andO;Pragmatic knowledge includes inferring and tracking plans, using contextacross clausal and sentence boundaries, determining local and globalconstraints on utterances and dealing with definite and pronominal reference.andO;Work in the natural language community  has shown that pragmatic knowledgesources are important for undertanding language.andP;  People communicate toaccomplish goals, and the structure of the plans to accomplish them are wellunderstood.andP;  When speech is used in a structured task such as problemsolving, pragramatic knowledge sources are available for constraining searchspaces.andM;In the past, pragmatic dialogue level knowledge sources were used in speechto either correct speech recognition errors or to disambiguate spoken inputand perform inferences required for understanding.andP;  In these systems,pragmatic knowledge was applied to the output of the recognizer.andM;In this article we describe an approach for flexibly using contextualconstraints to dynamically circumscribe the search space for words in aspeech signal.andP;  We use pragmatic knowledge to derive constraints about whatthe user is likely to say next.andP;  Then we loosen the constraints in aprincipled manner.andP;  We generate layered sets of predictions which range fromvery specific to very general.andP;  To enable the speech system to give priorityto recognizing what a user is most likely to say, each prediction setdynamically generates a grammar which is used by the speech recognizer.andP;  Theprediction sets are tried in order of most specific first, until anacceptable parse is found.andP;  This allows optimum performance when users behavepredictably, and displays graceful degradation when they do not.andP;  Theimplemented systems [MINDS] used these layered constraints to guide thesearch for words in our speech recognizer.andP;  For our recognizer, we use amodified version of the SPHINX large vocabulary, speaker independent,continuous speech recognition system.andP;  The MINDS spoken language dialoguesystem developed at Carnegie-Mellon University applies pragmaticknowledge-based constraints as early as possible in the speech recognitionprocess to eliminate incorrect recognition choices and drastically reduce thespeech system error rate.andM;The main problem in speech recognition is the enormous complexity involved inanalyzing speech input.andP;  Variations in pronunciation, accent, speakerphysiology, emphasis and characteristics of the acoustic environmenttypically produce hundreds of different possible phonome classifications foreach sound.andP;  In turn, the many phoneme classifications can result in manypossible word hypotheses at each point in the utterance.andP;  All of these wordchoices can then be combined to yield hundreds of sentence candidates foreach utterance.andP;  The resulting search space is huge.andP;  Yet a speech system isrequired to filter out all incorrect candidates and correctly recognize anutterance in real time.andP;  Different approaches have been used in the past tolimit the exponential explosion of the search space and trim thecomputational complexity to a more manageable level.andM;In an attempt to reduce complexity, the speech recognition problem has beensimplified along different dimensions.andP;  The first speech recognition systemswere tailored to specific speakers only.andP;  This reduced much of the speechsignal variation due to speaker characteristics such as sex, age, accent andphysiological characteristics.andP;  The early systems also only recognized veryfew words.andP;  This reduction in vocabulary eliminated much confusion duringrecognition, especially if the words were all acoustically different.andP;  Toavoid the problem of slurred and coarticulated words, the early systemsrequired that each word be pronounced separately and that the speaker pauseslightly between words.andP;  A final simplification of the speech problem was toartificially limit the number of different words that could be used at anyone point.andP;  Similar to the choices available in a series of menus, thespeaker could only use one of a few words at any place in the utterance.andO;This technique depends on the previously uttered sentential context to reducethe so-called branching factor.andP;  The effective vocabulary at each point ismade much smaller than the overall vocabulary available to the system.andM;Speech technology has made great strides in the recent past.andP;  We are now in aposition where we can progress beyond systems that merely type out a sentencewhich was read to it for demonstration purposes.andP;  The speech recognitionresearch focus has shifted toward integrated spoken language systems, whichcan be used by people trying to accomplish a task.andP;  For the rest of thisarticle we will only be concerned with speaker independent, large vocabulary,connected speech recognition systems.andM;THE NEED TO INTEGRATE SPEECH ANDandM;NATURAL LANGUAGEandM;Speech recognition techniques at the word level are inadequate.andP;  Error ratesare fairly high even for the best currently available systems.andP;  TheCarnegie-Mellon SPHINX system is considered to be the best speakerindependent connected speech recognition system today.andP;  But even the Sphinxsystem has an error rate of 29.4 percent for speaker independent, 1,000 wordconnected speech recognition, when recognizing individual words in sentenceswithout using knowledge about syntax or semantics.andP;  Clearly we need someforms of higher level knowledge to understand speech better.andP;  This is thekind of knowledge that has been used for years by researchers concerned withnatural language understanding of typed input.andM;Just using the modules developed by the typed natural language understandingcommunity is not as simple as it may seem.andP;  There are a number of veryspecific demands to a speech system interface which differ from a typedsystem interface.andP;  Many of the techniques for parsing typed natural languagedo not adapt well to speech specific problems.andP;  The following pointshighlight some of the unique speech problems not found in typed naturallanguage.andM;Probability Measures.andP;  There is nothing uncertain about what a person hastyped.andM;The ASCII characters are transmitted unambiguously to the program.andP;  Thespeech system has to deal with many uncertainties during recognition.andP;  Ateach level of processing, the uncertainties compound.andP;  The result of eachprocessing step is usually expressed in terms of some probability orlikelihood estimate.andP;  Any techniques developed for typed natural languagesystems need to be adapted to account for different alternatives withdefferent probabilities.andM;Identifying words.andP;  The speech system usually has many words hypothesized foreach word actually spoken.andP;  The word identification problem has severalcomponents, some of those include:andM;* Uncertainty of the location of a word in the input.andP;  For almost everyacoustic event in the utterance, there are words hypothesized that start andend at this event.andP;  Many of these words overlap and are mutually exclusive.andM;* Multiple alternatives at every word location.andP;  Even at the correct wordlocation, the system is never certain of what word is actual input.andP;  A listof word candidates is usually hypothesized with different probabilities.andM;* Word boundary and juncture identification.andP;  Often words are coarticulatedsuch that their boundaries became merged and unrecognizable.andP;  Some milk is aclassic example of a phrase where the two words overlap without a clearboundary.andM;Phonetic Ambiguity of Words.andP;  Many words sound completely alike and theorthographic representation can only be distinguished from larger context.andO;This is the case for ice cream and I scream.andM;Syllable Omissions.andP;  Spoken language tends to be terse.andP;  Because people areso good at disambiguating the speech signal, speakers unconsciously omitsyllables.andP;  An example of this is frequently found in the pronunciation ofUnited States which is reduced to sound like unite states in everyday speech.andM;Missing Information.andP;  The speech system will occasionally fail to recognizethe correct word completely.andP;  Even though the speaker may have said the wordcorrectly, it cannot be hypothesized from the acoustic evidence.andP;  There willbe too many other word candidates that receive a better score and the correctword will be left out.andM;Ungrammatical Input.andP;  If miss-typing is the kind of phenomenon a standardnatural language system has to deal with, speech systems encountermiss-spoken words and filled pauses like ah and uhm which further complicaterecognition.andP;  Natural human speech is also more likely to be ungrammatical.andM;The effect of all these differences requires speech recognition systems todeal with many more alternatives and a much larger search space than typednatural language systems.andP;  Therefore all techniques that have been developedby the natural language processing community must be restructured if they areto be adapted for the speech specific problems.andP;  They especially must beadapted to deal with the huge search spaces that result from the magnitude ofthe problem if these knowledge sources are to be used to assist in actualspeech recognition.andM;Uses of Knowledge in Speech Recognition SystemsandM;In the past, speech systems have used a variety of different kinds ofknowledge sources to reduce the magnitude of the search space.andP;  The followinglist describes the major information sources used by different speechsystems.andP;  We restrict ourselves to enumerating the knowledge sources abovethe level of complete words as they apply to sentences, dialogues, user goalsand user focus.andM;* Word Transition Probabilities.andP;  If one wants to use knowledge of more thanone word, the obvious solution is to use two words.andP;  By analyzing a large setof training sentences, a matrix of word pairs is constructed.andP;  The sentencesare analyzed individually, without regard to dialogue structure, focus ofattention or user goals.andP;  The resulting word pair matrix indicates whichwords can follow an already recognized word.andP;  A further extension of thismethod used likelihoods of transitions encoded in the matrix instead of justbinary values.andP;  Not only do we know which word pairs are legal, but we alsohave an indication of how likely they are.andP;  Empirically derived trigrams ofwords have also been used.andP;  Here a matrix is computed which, when given asequence of the two preceding words, indicates which words can immediatelyfollow at this point.andP;  Variations on the word transition probabilityestimates using Markov modeling techniques have been used by.andP;  A minormodification of this approach uses word categories instead of words.andP;  Wordcategories are independent of the actual vocabulary size and require lesstraining data to establish the transition probability matrix.andP;  While thisapproach does well to reduce the amount of search that is required, there isstill much information missing in the triplets of allowable words.andM;* Syntactic Grammars.andP;  A syntactic grammar first divides all words intodifferent syntactic categories.andP;  Instead of using transition probabilitiesbetwen word pairs or triplets, a syntactic grammar specifies all possiblesequences of syntax word categories for a sentence.andP;  Network grammars seem tobe the most efficient representation for this type of constraint, since fastprocessing times are crucial in a speech system.andP;  Other grammar parsingrepresentations are not as efficient when faced with the large numbers ofcandidates in a speech recognition situation.andP;  While the grammar may bewritten in a different notation, it can usually be compiled down to a networkfor the actual speech processing.andP;  The big drawback of these grammars is thatthey are difficult to construct by hand.andP;  They also assume the speaker willproduce an utterance which is recognizable by the grammar.andM;* Semantic Grammars.andP;  Semantic grammars have been the most popular form ofsentential information encoded in speech recognition systems.andP;  The grammarrules are similar to those of syntactic grammars, but words are categorizedby a combination of syntactic class and semantic function.andP;  Only sentencesthat are both syntactically well formed as well as meaningful in the contextof the application will be recognized by a semantic grammar.andP;  Semanticgrammars express stronger constraints than syntactic grammars, but alsorequire more rules.andP;  These grammars are also easily representable asnetworks.andP;  Compared to the syntactic-grammars above, they are even moredifficult to construct by hand.andP;  Nevertheless, most speech systems which usehigher level knowledge have chosen to use semantic grammars as their mainsentential knowledge source.andM;Some speech recognition systems emphasized semantic structure whileminimizing syntactic dependencies.andP;  This approach results in a large numberof choices due to the lack of appropriate constraints.andP;  The recognitionperformance therefore suffers due to the increased ambiguities.andP;  None ofthese systems proposed to use any knowledge beyond the constraints withinsingle sentences.andM;* Thematic Memory.andP;  Barnett describes a speech recognition system which usesa notion of history for the last sentences.andP;  The system keeps track ofpreviously recognized content words and predicts that they are likely toreoccur.andP;  The possibility of using a dialogue structure is mentioned byBarnett, but no results or implementation details are reported.andP;  The thematicmemory idea was picked up again by Tomabechi and Tomita, who demonstrated anactual implementation in a sophisticated frame-based system.andP;  Both speechrecognition systems use an utterance to activate a context.andP;  This context isthen transformed into word expectations which prime the speech recognitionsystem for the next utterance.andM;* History-based Models.andP;  Fink, Biermann and others implemented a system thatused a dialogue feature to correct errors made by a small vocabulary,commercial speech recognition system.andP;  Their module was strictlyhistory-based.andP;  It remembered previously recognized meaning (i.e., semanticstructures) of sentences as a finite state dialogue network.andP;  If thecurrently analyzed utterance was semantically similar to one of the steredsentence meanings and the system was at a similar state of the dialogue atthat time, the stored meaning can be used to correct the recognition of thenew utterance.andP;  Significant improvements were found in both sentence and worderror rates when a prediction from a previous utterance could be applied.andO;However, the history-based expectation was only applied after a wordrecognition module had processed the speech, in an attempt to correctrecognition errors.andM;* Strategy Knowledge.andP;  Strategy knowledge was applied as a constraint in thevoice chess application of Hearsay-I.andP;  The task domain was defined by therules of chess.andP;  The &quot;situational semantics of the conversation&quot; were givenby the current board position.andP;  Depending on these, a list of legal movescould be formulated which represented plausible hypotheses of what a usermight say next.andP;  In addition, a user model was defined to order the moves interms of the goodness of a move.andP;  From these different knowledge sources, anexhaustive list of all sentences possible was derived.andP;  This list ofsentences was then used to constrain the acoustic-phonetic speechrecognition.andP;  Hearsay-I went toof ar in the restriction of constraints.andP;  Aclassic anecdote tells of the door slamming during a demonstration and thesystem recognizing the sentence: &quot;Pawn to Queen 4&quot;.andP;  Hearsay-I applied itscontraints in an extremely limited domain and overly restricted what could besaid.andP;  Nevertheless, the principles of using a user model, task semantics andsituational semantics are valid.andM;* Natural Language Back-Ends.andP;  Several speech recognition systems claim tohave a dialogue, discourse or pragmatic components.andP;  However, most of thesesystems only use this knowledge just like any typed natural languageunderstanding system would.andP;  The speech input is processed by a speechrecognition module which uses all its constraints up through the level ofsemantic grammars to arrive at a single best sentence candidate.andP;  Thissentence is then transformed into the appropriate database query, anaphoricreferences are resolved, elliptic utterances are completed and the discoursemodel is updated.andP;  All these higher level procedures are applied after thesentence is completely recognized by the speech front-end.andP;  There is nointeraction between the natural language processing modules and the speechrecognizer.andM;Natural Language ResearchandM;There has been much research on discourse, focus, planning, inference andproblem solving strategies in the natural language processing community.andO;Some of the research was not directly carried out in the context of naturallanguage systems, but describes methods for representation and analysis ofthese issues.andP;  We will briefly review the key principles which influenced thedesign of the MINDS spoken language dialogue system.andM;Plans.andP;  The utility of tracking plans and goals in a story has been wellestablished.andP;  A number of researchers have described the utility ofidentifying a speaker's goals to disambiguate natural language sentences andto provide helpful system responses.andP;  Similarly, Cohen and Perrault havedeveloped a plan-based dialogue model for understanding indirect speech acts.andO;A program called PAM showed how an understanding of a person's goal canexplain an action of the person.andP;  The goal and subsequent actions can also beused to infer the particular plan which the person is using to achieve thisgoal.andP;  Additionally, Wilensky developed methods for dealing with competinggoals and partial goal failures.andP;  The ways in which a plan can be broken downinto a hierarchical set of goals and subgoals was originally demonstrated bySacerdoti.andM;Problem Solving.andP;  Newell and Simon were key influences in the study of humanproblem solving.andP;  Among other things, they showed how people constantly breakgoals into subgoals when solving problems.andP;  Their findings, as well as muchof the other research done in this area illustrate the function of user goalsrepresented as goal trees, and traversal procedures for goal trees.andM;Focus.andP;  Focus determines a set of relevant concepts for a particularsituation.andP;  Grosz found that natural language communication is highlystructured at the level of dialogues and problem solving.andP;  She showed how thenotion of a user focus in problem solving dialogues is related to apartitioning of the semantic space.andP;  Focus can also provide an indication howto disambiguate certain input.andP;  Additional work by Sidner confirmed the useof focus as a powerful notion in natural language understanding.andP;  Sidnersuccessfully used a focus mechanism to restrict the possibilities of referentdetermination in pronominal anaphora.andM;Ellipsis.andP;  Elliptical utterances are incomplete sentences which rely onprevious context to become meaningful.andP;  Methods for interpreting ellipticalutterances were studied in depth by Frederking.andP;  He used a chart-basedrepresentation to remember fragments of preceding sentences which weresuitable complements for elliptic phrases.andM;User Domain Knowledge.andP;  Chin showed how the knowledge of the user about thedomain can influence the expectations and behavior of a system.andP;  In addition,he described ways in which the user expertise could be inferred by thesystem.andM;THE MINDS SYSTEMandM;The main problem in speech recognition is the enormous complexity involved inanalyzing speech input.andP;  As search space size increases recognitionperformance decreases and processing speed increases.andP;  The value of a reducedsearch space and stronger conntraints is well known in the speech recognitioncommunity.andP;  Reducing the search to only the most promising word candidates bypruning often erroneously eliminates the correct path.andP;  By applyingknowledge-based constraints as early as possible, one can trim theexponential explosion of the search space to a more manageable size withouteliminating correct choices.andP;  Previously we delimited the key knowledgesources incorporated into the MINDS system to provide constraint.andP;  Now webriefly overview the entire MINDS system and enumerate the primaryinnovations of this new approach.andP;  The approach employs knowledge basedconstraints to reduce the exponential growth of the search space the speechrecognizer must analyze.andM;To demonstrate our new approach in speech recognition, we have built MINDS, aMulti-modal.andP;  INteractive Dialog System.andP;  It allows a user to speak, type andpoint during a problem solving session with the system.andP;  The system outputsinformation in a variaty of media.andP;  It produces a natural language answer tothe user's question as well as displaying relevant information on a multipleraster display screens.andP;  These screens display the current information invarious contexts emphasizing different aspects of the dialogue history andthe current world situation.andP;  The MINDS system operates in real time.andM;MINDS works in a resource management domain, featuring information obtainedfrom a database of facts about ships in the United States Navy.andP;  The basicproblem scenario involves a damaged ship performing a particular task.andP;  Thesystem user must determine the impact of the damage on the mission and thendetermine whether the damaged ship should continue in degraded condition orshould be replaced by a different ship.andP;  If a replacement ship is beingconsidered, the user must locate one with similar capabilities and a shipwhich will have minimal impact on other mission operations.andP;  An excert from asample interaction transcript can be found in Figure 1.andM;For the purpose of this article, MINDS can be viewed as a speaker-independentcontinuous speech recognition system that uses dialogue knowledge, usergoals, plans and focus to understand what was said in its naval logisticsproblem solving domain.andP;  The system uses this higher level knowledge ofdialogues in addition to a representation of user's domain knowledge topredict what the current user will talk about next.andP;  The predictionsdrastically reduce the search space before the sentence and word detectionmodules even begin to analyze the speech input.andM;In every general terms, we can describe the main operations of the MINDSsystem as a continuous loop.andP;  First, the system generates a set ofpredictions based on the last user query, the database response and the stateof the dialogue.andP;  Then, the predictions are translated into a semanticgrammar.andP;  The active lexicon is restricted to only cover words which are partof the predictions.andP;  The next user query is parsed using the dynamicallycreated grammar and lexicon, and the user's question is displayed forverification.andP;  The database response to user request is then presented inoutput modalities.andM;Innovations of the MINDS SystemandM;The MINDS system represents a radical departure from the principles of mostother speech recognition systems.andP;  The key innovations of the MINDS systeminclude the following:andM;1.andP;  Use of a combination of knowledge sources including discourse anddialogue knowledge, problem solving knowledge, pragmatics, user domainknowledge goal representation, as well as task semantics and syntax in anintegrated system.andM;2.andP;  All of the knowledge is used predictively.andP;  Instead of applying theknowledge to correct an error or resolve ambiguities after they occur, theknowledge isd applied in a predictive way to constrain all possibilities asthey are generated.andM;3.andP;  The constraints generated by the system are immediately applied to thelow-level speech processing to reduce the search space.andP;  We use thepredictive constraints to eliminate large portions of the search space forthe earliest acoustic-phonetic analysis.andM;4.andP;  In case the predictions fail, the system provides a principled way ofrecovery when constraints are violated.andP;  If the constraints are satisfied,recognition is more accurate and faster.andP;  However, if some of our predictionsare violated the system does not break--it degrades gracefully.andM;5.andP;  In addition to speech input, the MINDS system allows pointing andclicking as well as typed modes of interaction.andP;  The user may use the mouseto select objects displayed graphically on the screen.andP;  For those users whoare uncomfortable speaking to a computer, anything that could be spoken canalso be typed on a keyboard.andM;MINDS exploits knowledge about users' domain knowledge problem solvingstrategy, their goals and focus as well as the general structure of adialogue to constrain speech recognition down to the signal processing level.andO;Pragmatic knowledge sources are used predictively to circumscribe the searchspace for words in the speech signal.andP;  In constrast to other systems, we donot correct misrecognition errors after they happen, but apply ourconstraints as early as possible during the analysis of an utterance.andP;  Ourappraoch uses predictions derived from the problem-solving dialogue situationto limit the search space at the lower levels of speech processing.andP;  At eachpoint in the dialogue, we predict a set of concepts that may be used in thenext utterance.andP;  This list of concepts is combined with a set of syntacticnetworks for possible sentence structures.andP;  The result is a dynamicallyconstructed semantic network grammar, which reflects all the constraintsderived from all our knowledge sources.andP;  To avoid getting trapped bypredictions which are not fulfilled, we generate them at different level ofspecificity.andP;  When the parser then analyzes the spoken utterance, the dynamicnetwork allows only a very restricted set of word choices at each point.andO;This reduces the amount of search necessary and cuts down on the possibilityof recognition errors due to ambiguity and confusion between words.andP;  Thebottom line is that the MINDS system uses as much knowledge as possible toachieve accurate speech recognition and help the user complete his taskefficiently.andM;The Predictive Use of KnowledgeandM;Predictions are derived from what we know about the current state of thedialogue.andP;  This knowledge is then refined to constrain what we expect theuser will actually say.andP;  In some sense predictions cover everything we expectto happen, and exclude events which are unlikely to happen.andP;  To be able tocreate predictions, we have three very important data structures in thesystem:andM;A knowledge base of domain concepts.andP;  In this data structure we represent allobjects and their attributes in the domain.andP;  The representation uses astandard frame language which provides the capability to express inheritanceand multiple relations between frames.andP;  The domain concepts also representeverything that can be expressed by the user.andP;  Each possible utterance willmap into a combination of domain concepts which constitute the meaning ofthat utterance.andP;  This representation of meaning is also used to generate thedatabase queries from the utterance.andM;Hierarchical goal trees.andP;  The goal trees represents a hierarchy of allpossible abstract goals as user may have during the dialogue.andP;  The goal treesare composed of individual goal nodes, structured as AND-OR trees.andP;  Each goalnode is characterized by the possible subgoals it can be decomposed into anda set of domain concepts involved in trying to achieve this goal.andP;  Theconcepts associated with a goal tend to be restricted from previous dialogcontext.andP;  These restrictions on concept expansions are dynamically computedfor each concept.andP;  The computation is based upon principles for inheritingand propagating constraints based upon their embedding and are often referredto as local and global focus in the natural language literature.andP;  The goaltree not only defines the goals, subgoals and domain concepts, but also thetraversal options available to the user.andP;  A goal node's associated conceptscan be optional or required, single use or multiple use.andP;  If a concept isoptional, it is possible but not necessary for a user to apply this conceptin the current problem-solving step.andP;  If a concept is defined asmulti-usable, then a user could refer to it several times in differentutterances during the current problem solving step.andM;A User Model.andP;  A user model represents domain concepts and the relationsbetween domain concepts which a user knows about.andP;  These models arerepresented as control structures which are associated with goals in the goaltree.andP;  The control structures express which goals may be exclusive becausethe user can infer the information in one goal once the other, exclusive goalhas been completed.andP;  Other goals may be optional because the user isunfamiliar with the domain concept or its potential importance in deriving asolution to the current problem.andP;  Additionally, the control structurescontain probabilistic orderings for conjunctive subgoals.andP;  Hence, the usermodel provides the system with potential traversal options which are morerestrictive than the traversal options provided by the other knowledgesources.andM;These three complex data structures are currently only constructible byhand--based on a detailed and careful analysis of the problem-solving taskitself.andM;We will now try to explain how the knowledge is used by the MINDS system totrack the progress of a user during a problem-solving session.andP;  We will alsoshow how predictions in the form of domain concepts are generated during adialogue.andM;When an input utterance and its database response have been processed, wefirst try to determine which goal states were targeted by the presentinteraction.andP;  Determination of activated goal states is by no meansunambiguous.andP;  During one interaction cycle, several goal states may becompleted and many new goal states may be initiated.andP;  Similarly, it ispossible that an assumed goal state is not being pursued by the user.andP;  Todeal with these ambiguities, we use a number of algorithms.andP;  Goals that havejust been completed by this interaction and that are consistent with previousplan steps are preferred.andP;  Based on the information we have available, weselect the most likely plan step to be executed next.andP;  If the current goal isnot complete, then our most likely plan step will attempt to complete thecurrent goal.andP;  If the current goal is satisfied, we identify the next goalstates to which a user could transit.andP;  The result of tracking the goal statesis a list of potential goals and subgoals which a user will try to completein the current utterance.andP;  Additionally, since there are always many activegoals which may or may not be hierarchically embedded, we also maintain alist of all active goals.andP;  Hence, the procedures described above are used fordetermining the best, most likely goal state a user will transit to.andP;  Togenerate our most restrictive predictions, we then restrict the most likelygoal state further by taking the constraints from the user model.andP;  The nextprediction layer ignores the user model and is derived only from the best,most likely next goal.andP;  Finally, additional less restrictive sets ofpredictions are derived from currently active goals which are at higherlevels of the goal tree.andP;  This procedure continues until all active goals areincorporated into a prediction set.andP;  The goals all have an associated list ofconcepts in the task domain.andP;  These are the concepts a user will refer towhen trying to satisfy the current goal.andM;For example, in a goal state directed at assesssing a ship's damage, weexpect the ship's name to appear frequently in both user queries and systemstatements.andP;  We also expect the user to refer to the ship's capabilities.andO;The predicted sentence structures should allow questions about the featuresof a ship like &quot;Does its sonar still work?&quot;andP;  Display the status of all radarsfor the Spark&quot; and &quot;What is Badger's current speed?&quot;andM;Some domain concepts which are active at a goal tree node during a particulardialogue phase have been partially restricted by previous goal states.andP;  Therepresentation of the domain concepts associated with goal nodes provides amechanism to specify what prior goal can restrict the current concept.andP;  Theserestrictions may come either from the user's utterances or from the systemresponses.andP;  Thus each goal state not only has a list of active domainconcepts, but also a set of concepts whose values were partially determinedby an earlier goal state.andM;In our example, once we know which ship was damaged, we can be sure allstatements in the damage assessment phase will refer to the name of that shipor its hull number only.andP;  In addition to the knowledge mentioned earlier, wealso restrict what kinds of anaphoric referents are available at each goalnode.andP;  The possible anaphoric referents are determined by user focus.andP;  Fromthe current goal or subgoal state, focus identifies previously mentioneddialogue concepts and answers which are relevant at this point.andP;  Theseconcepts are expectations of the referential content of anaphora in the nextutterance.andM;Continuing our example, it does not make sense to refer to a ship as &quot;it&quot;before the ship's name has been mentioned at least once.andP;  We also do notexpect the use of anaphoric &quot;it&quot; if we are currently talking about a group ofseveral potential replacement ships.andM;Elliptic utterances are predicted when we expect the user to ask aboutseveral concepts of the same type after having seen a query for the firstconcept.andM;If the users have just asked about the damage to the sonar equipment of aship, and we expect them to query about damage to the radar equipment, wemust include the expectation for an elliptic utterance about radar in ourpredictions.andM;Expanding Predictions into NetworksandM;After the dialogue tracking module has identified the set of concepts whichcould be referred to in the next utterance, we need to expand these intopossible sentence fragments.andP;  Since these predicted concepts are abstractrepresentations, they must be translated into word sequences which signifythat appropriate conceptual meaning.andP;  For each concept, we have precompiled aset of possible surface forms, which can be used in an actual utterance.andP;  Ineffect, we reverse the classic understanding process by un-parsing theconceptual representation into all possible word strings which can denote theconcept.andP;  A predicted concept can be quickly un-parsed into all its possiblesemantic network grammar subnets.andM;In addition to the individual concepts, which usually expand into nounphrases, we also have a complete semantic network grammar that has beenpartitioned into subnets.andP;  Each subnet expresses a complete sentence.andP;  Asubnet defines allowable syntactic surface forms to express a particularsemantic content.andP;  For example, all ways of asking for the capabilities ofships are grouped together into subnets.andP;  The semantic network is furtherpartitioned into separate subnets for elliptical utterances, and subnets foranaphora.andP;  The semantic grammar subnets are precompiled to allow directaccess for processing efficiency.andP;  The terminal nodes in the networks areword categories instead of words themselves, so no recompilation is necessaryas new lexical items in existing categories are added to or removed from thelexicon.andM;The final expansion of predictions brings together the partitioned semanticnetworks and the predicted concepts which were translated into their surfaceforms.andP;  Through a set of cross-indices, we intersect all predicted conceptexpressions with all the predicted semantic networks.andP;  This operationgenerates dynamically one combined semantic network grammar which embodiesall the dialogue level and sentence level constraints.andP;  This dynamicallycreated network grammar is used by no parser to process an input utterance.andM;To illustrate this point, let us assume that the frigate &quot;Spark&quot; has somehowbeen disabled.andP;  We expect the user to ask for its capabilities next.andP;  Thedialogue tracking module predicts the &quot;shipname&quot; concept restricted to thevalue &quot;Spark&quot; and any of the &quot;ship-capabilities&quot; concepts.andP;  Single anaphoricreference to the ship is also expected, but ellipsis is not meaningful atthis point.andP;  The current damage assessment dialogue phase allows queriesabout features of a single ship.andM;During the expansion of the predicted concepts, we find the word nets such as&quot;the ship,&quot; &quot;this ship,&quot; &quot;the ship's,&quot; &quot;this ship's,&quot; &quot;its,&quot; &quot;Spark&quot; and&quot;Spark's.&quot;andP;  We also find the word nets for the capabilities such as &quot;allcapabilities,&quot; &quot;radar,&quot; &quot;sonar,&quot; &quot;Harpoon,&quot; &quot;Phalanx,&quot; etc.andP;  We thenintersect these with the sentential forms allowed during this dialogue phase.andO;Thus we obtain the nets for phrases like &quot;Does [it, Spark, this_ship,the_ship] have [phalanx, harpoon, radar, sonar],&quot; &quot;What [capabilities, radar,sonar] does [the_ship, this_ship, it, Spark] have,&quot; and many more.andP;  Thissemantic network now represents a maximally constrained grammar at thisparticular point in the dialogue.andM;Recognizing Speech Using Dynamic NetworksandM;We use the SPHINX system [19] as the basis for our recognizer.andP;  SPHINXsamples input speech in centisecond frames.andP;  Based on the LPC cepstrumcoefficients, each frame is then mapped into one of 256 prototype vectors.andO;Vector-quantized speech is also used to train Hidden Markov Models (HMMs) forphonemes.andP;  HMMs are trained from a corpus of approximately 4200 sampleutterances.andP;  Each word is represented in the dictionary as a single sequenceof phonemes.andP;  The models for words are pre-compiled by concatenating the HMMsfor each phoneme in a word.andP;  During recognition, SPHINX performs atime-synchronous beam search known as the Viterbi algorithm, matching wordmodels against the input.andM;In the MINDS system, we use the active set of semantic networks to controlword transitions instead of the word-pair constraints normally used by theSPHINX system.andP;  The search begins at the set of initial words for all activesubnets.andP;  This set includes only currently active words from the dynamicallycreated lexicon for this utterance.andP;  As the search matches a word from theinput utterance, it transits along the arc in the grammar represented by thatword.andP;  A score is assigned to each path in the beam, indicating how well theinput is matching the HMMs in the path.andP;  Paths falling below a thresholdscore are pruned.andP;  The dynamically created semantic network is used to allowonly legal word transitions.andP;  The network does not affect the score of a pathbut simply restricts words which can continue a particular path.andP;  If nostring of words is found which matches the HMMs better than a certainthreshold score, a different grammar and lexicon from a more general set ofpredictions must be used to re-process the utterance.andP;  After the spoken inputhas been processed, the word string with the best score is passed back to thesystem for parsing.andM;When Predictions FailandM;There are a number of assumptions built into the use of predictions.andP;  If auser conforms to our model of a problem-solving dialogue, the advantages areclear.andP;  However, we must consider the case when some assumptions areviolated.andP;  There are two points to consider when predictions fail: We mustfirst be able to identify the situation of failed predictions and then find away to recover.andM;In the MINDS system, the first point is accomplished without extra work.andO;When the user speaks an utterance which was not predicted, the speechrecognition component usually fails to produce a complete parse.andP;  The spokenwords do not match the predicted words and receive low probability scores.andO;This may not always be as easy in other recognition systems.andM;As a mechanism for recovery from failed predictions, the MINDS system alwaysproduces several sets of predictions for each utterance.andP;  These sets ofpredictions range from very specific to very general.andP;  For the most specificpredictions, the system uses all the possible constraints.andP;  Each successiveset of predictions then becomes more general.andP;  The number of levels ofconstraint relaxation depends on the goal tree structure at that point.andO;Predictions are made more general by assuming additional goal nodes areactive.andP;  Eventually we reach a level of prediction constraints which isidentical to the constraints provided by the full semantic grammar with allpossible words.andP;  We now can parse any syntactically and semantically legalutterance, disregarding all dialogue considerations.andP;  Beyond that we can onlyrelax the constraints to a point where any word can be followed by any otherword.andP;  This would be necessary if a user spoke an utterance that was notcovered by the grammar.andP;  In this case, we must rely on heuristics during thesemantic interpretation of the utterance to provide a correct meaning anddatabase query.andP;  Details of this procedure are described in [38].andM;When the speech recognition module fails to parse at a particular level ofconstraints, the next set of predictions is used to reparse the sameutterance until a successful parse is obtained.andP;  If the user is cooperativeand within our predictions, recognition accuracy will be high and responsetime immediate.andP;  As the system backs up over several levels of constraints,the search space of the recognition module becomes larger and processing timeincreases while accuracy drops.andP;  However, the system never experiences acomplete loss of continuity when predictions are violated.andM;EVALUATION OF PROGRESSandM;Many systems developed by researchers in the artificial intelligencecommunity lack a rigorous evaluation.andP;  While the individual systems mayincorporate brilliant ideas, it is rarely shown that they are in some waybetter than other systems based on a different approach.andP;  If the research ina field wants to make progress, that progress must be made visible andmeasurable.andM;In the field of speech understanding one clear measure of success isrecognition accuracy.andP;  Recognition accuracy can be measured in terms of wordaccuracy, sentence accuracy as well as semantic accuracy.andP;  Word accuracy isdefined here as the number of words that were recognized correctly divided bythe number of words that were spoken.andP;  In addition to the number of correctwords, we also record the number of insertions of extra words by therecognizer.andP;  This number is otherwise not reflected in the percentage ofcorrect words.andP;  Recognition accuracy thus takes into account deleted wordsand word substitutions (i.e., &quot;its&quot; was spoken but &quot;his&quot; was recognized.)andP;  Onthe other hand, error rate reflects insertions, deletions, and substitutions.andO;If the speech recognizer makes minor errors in recognizing an utterance, butthe underlying meaning of the utterance is preserved, the utterance isconsidered to be recognized semantically accurate even though some words wereincorrect.andP;  Semantic accuracy therefore is the percentage of sentences withcorrect meaning.andP;  In our system, a sentence is considered semanticallycorrect if the recognition produces the correct database query.andM;To test the ability of the MINDS system to reduce search space and improvespeech recognition performance, we performed two experiments.andP;  The firstexperiment assessed search space reduction caused by predictive use of allpragmatic knowledge sources.andP;  The second experiment measured improvement inrecognition accuracy rates resulting from the use of layered predictions.andO;Both studies used a test set of data which was independent from the trainingdata used to develop the system.andP;  This means that the utterances anddialogues processed by the system to obtain the experimental results had notbeen seen previously by the system or the developers.andM;Our test data consisted of 10 problem solving scenarios.andP;  These were adaptedversions of three actual transcripts of naval personnel solving problemscaused by a disabled vessel.andP;  The personnel must determine whether to delay amission, find a replacement vessel or schedule a repair for a later date.andO;They use a database to find necessary problem solving information.andP;  Inaddition, we created seven additional scenarios by paraphrasing the originalthree.andP;  The test scenarios contained an average of nine sentences with anaverage of eight words each.andP;  An excerpt of a dialogue sequence is given inFigure 1.andM;The training data had consisted of five different problem solving scenariosfrom transcripts of naval personnel performing the same basic task.andP;  Thetraining scenarios were used for developing the user models.andP;  Dialoguephases, goals and problem solving plans were derived from an abstractdescription of the stages and options available to a problem solver.andP;  Theabstract plan descriptions had been provided by the Navy.andM;Since our database was different from the one used in gathering the originaltrancripts, we were forced to adapt all scenarios.andP;  Lexical items which wereunknown to our system were substituted with known words.andP;  Shipnames,locations, capabilities, mission requirements, etc.andP;  were changed to beconsistent with our database.andP;  We feel these adaptations had minimal impacton the integrity of the data and did not alter the problem solving structureof the task.andP;  The lexicon for this domain contained 1,000 words.andM;Reduction of Search Space and PerplexityandM;Since the magnitude of the search space is such a critical factor in speechrecognition, one measure of success is the reduction in search space providedby a system.andP;  To measure the constraint imposed by the knowledge sources weuse two measures: perplexity and search space reduction.andP;  Perplexity is aninformation theoretic measure that is widely used in speech systems tocharacterize the constraint provided by a grammar.andP;  Perplexity consists ofthe geometric mean of the number of nodes which are visited during theprocessing of an utterance.andP;  In our case, we use the semantic networkgrammars to calculate the number of word alternatives which the system has toconsider.andP;  Test set perplexity is computed specifically for actualutterances.andP;  After we compute the alternatives for a word in the utterance,we assume the system recognizes this word correctly and continues by onlycomputing the alternatives which directly follows this word in the grammar.andO;A more detailed justification of this measure is given in [17].andP;  The size ofthe search space is calculated by raising the sentence perplexity value tothe number of words in the sentence.andM;Our first experiment was designed to test the perplexity and search spacereduction resulting from applying pragmatic constraints.andP;  To measure thereduction in perplexity and search space we collected test set perplexitymeasurements for each of the parsed sentences in two conditions.andP;  The firstcondition represented the constraints provided by the complete semanticgrammar networks with the full vocabulary available.andP;  The second conditionmeasured perplexity for the most specific set of predictions that could beapplied.andP;  The estimate for the second condition is the perplexity obtained bymerging the successful prediction level with all of the more specific butunsuccessful levels of constraints.andP;  Otherwise, the results would bemisleading whenever the predictions were not fulfilled.andM;As seen in Table I, by applying our best constraints, test set perplexity wasreduced by an order of magnitude, from 2 979.2 to 17.8 while search spacesdecreased by roughly 10 orders of magnitude.andM;Improvements in Recognition AccuracyandM;To evaluate the effectiveness of using predictions on recognition performancewe used 10 speakers (8 male, 2 female) who had never before spoken to thesystem.andP;  To assure a controlled environment for these evaluations, eachspeaker read 20 sentences from the adapted test scenarios provided by theNavy transcripts.andP;  Each of these utterances was recorded.andP;  The speechrecordings were then analyzed by the MINDS system in two conditions.andP;  Thefirst condition ignored all constraints except those provided by the completesemantic grammar.andP;  In other words, all possible meaningful sentences wereacceptable at all times.andP;  The second condition used the MINDS system with themost specific set of predictions appropriate for the utterance.andM;To prevent confounding of the experiment due to misrecognized words, thesystem did not use its normal speech recognition result to change state.andO;Instead, after producing the speech recognition result, the system read thecorrect recognition from a file which contained the correct set ofutterances.andP;  Thus, the system always changed state according to a correctanalysis of the utterances.andM;The results can be found in Table II.andP;  The system performed significantlybetter with the predictions.andP;  Error rate decreased from 17.9 percent to 3.5percent.andP;  Perhaps just as important is the nature of the individual errors.andO;In the condition with the most specific successful predicitons, almost all ofthe errors (insertions and deletions) were made on the word &quot;the.&quot;andP;  Anotherlarge proportion of errors consisted of substituting the word &quot;his&quot; for theword &quot;its.&quot;andP;  Furthermore, none of the errors in the &quot;with predictions&quot;condition resulted in an incorrect database query.andP;  Hence, semantic accuracywas 100 percent on this sample of 200 spoken sentences.andM;CONCLUSIONSandM;It is obvious that the MINDS system represents only a beginning in theintegration of speech recognition with natural language processing.andP;  We haveshown how one can apply various forms of dialogue level knowledge to reducethe complexity of a speech recognition task.andP;  Our experiments demonstratedthe effectiveness of the added constraints on the recognition accuracy of thespeech system.andP;  We have also demonstrated that specific predictions can failand the system will recover gracefully using our mechanism for graduallyrelaxing constraints.andM;For this domain, we hand-coded all the goal trees and grammars into theknowledge sources of the system.andP;  For larger domains and vocabularies itwould be desirable to automate the process of deriving the goal trees andgrammars during interactions with the initial users.andP;  There is much more workneeded on automatic modeling of human problem solving processes based onempirical observation.andM;We do not claim that these exact results should be obtainable in any domainor any task.andP;  Rather it was our intent to demonstrate the usefulness ofdialogue level knowledge for speech recognition.andP;  Future spoken languagesystems dealing with larger domains and very large vocabularies will be welladvised to consider incorporating the kinds of mechanisms described in thisarticle.andO;</TEXT></DOC>