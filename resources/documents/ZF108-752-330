<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-752-330  </DOCNO><DOCID>08 752 330.andM;</DOCID><JOURNAL>Communications of the ACM  August 1990 v33 n8 p50(22)* Full Text COPYRIGHT Association for Computing Machinery 1990.andM;</JOURNAL><TITLE>Knowledge and natural language processing. (KBNL knowledge basednatural language) (technical)</TITLE><AUTHOR>Barnett, Jim; Knight, Kevin; Mani, Inderjeet; Rich, Elaine.andM;</AUTHOR><SUMMARY>KBNL (Knowledge Based Natural Language), a unique processingsystem that cleanly separates world knowledge from linguisticknowledge, is described.andP;  KBNL uses its knowledge base to aid inlexical acquisition.andP;  It consists of 'Lucy,' a knowledge-basedEnglish understanding system; 'Koko,' a knowledge-based Englishgeneration system, and 'Luke,' a lexical acquisition tool thatbuilds a robust dictionary for use by Lucy and Koko.andP;  All threesystems are based on a common model of mapping expressions ontoknowledge based representations.andP;  KBNL uses the CycLrepresentation language to describe the meaning of expressions inthe language used by the knowledge base.andP;  Applications of KBNLinclude querying the Cyc knowledge base itself as well asfull-text retrieval systems and machine translation.andM;</SUMMARY><DESCRIPT>Topic:     Knowledge-Based SystemsNatural Language InterfacesArtificial intelligenceExpert SystemsTechnology.andO;Feature:   illustrationtablechart.andO;Caption:   Knowledge about reading. (table)Knowledge about eating. (table)KNBL: Lucy, Koko, and Luke. (chart)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>KNOWLEDGE AND NATURAL LANGUAGE PROCESSINGandM;People use natural languages, such as English, to communicate.andP;  There areseveral good reasons for getting computers involved in that process.andP;  Thefitst reason is that there is an abundance of natural language material.andO;Computers are useful tools for organizing that material, retrieving it, andtranslating it into other languages.andP;  The second reason is that people needto communicate with machines and people find natural languages natural.andO;Although there is a school of thought that holds that only people caneffectively use natural language and thus it is inappropriate to bringcomputers into this arena (see [160]), there is already evidence thatprograms that manipulate language in various ways can be useful.andP;  In thisarticle, we will describe our approach to doing this.andP;  The key to thisapproach is our treatment of relationships between linguistic structures andthe objects they describe.andM;Language andandM;Knowledge about theandM;WorldandM;Language is a tool for communicating about the world.andP;  It is an effectivetool because there aare natural relationships between linguistic constructsand structures in the world that we use language to describe.andP;  For example:Simplistically, in the world there are objects and actions; correspondinglyin language there are nouns and verbs.andP;  In the world events occur in atemporal sequence; in language actions are (or can be) marked for tense.andM;The goal of computational natural language processing it twofold: to createcomputational representations of the relationships that hold between languageand some computational model (e.g., a knowledge base or a database schema) ofthe world; and to exploit those relationships to understand and generatelanguage as appropriate to some set of tasks.andP;  The scope of such system is,of course, limited by the scope of the domain model they use.andM;Unfortunately, although the mappings between language as a whole and theworld we talk about are natural and have evolved to be effective, there isnot a priori reason to believe that mappings between a small domain model andthe corresponding slice out of a language are natural at all.andP;  In fact, asubstantial body of experience in computational natural language processingover the last twenty years suggests that they are not.andM;As an example, consider the sentences:andM;Alan's birthday is November 13.andM;Alan was born on November 13.andP;  Suppose that we have a very limited domainmodel, in which we know about people and some of their attributes, includingtheir birthdays.andP;  Then both of these sentences mean the same thing.andP;  Buttheir linguistic structures are very different and the process by which theyare coerced into the same representation is necessarily ad hoc.andM;Now suppose we are exploiting a richer domain model in which there is anexplicit representation both for people and their birthdays and of childbirthevents (as well as of the relationships that necessarily exist between thetwo).andP;  The first sentence can now be interpreted as describing an attibute ofa person (using a fairly straightforward mapping scheme) and the second canbe interpreted as describing the temporal position of a childbirth event(with another straightforward mapping scheme).andP;  The fact that the twosentences &quot;say the same thing&quot; is a property not of our natural languageinterpretation system but of our model of the world, which provides the basisfor inferring the content of one from the content of the other.andM;From a computational point of view, this approach offers two big advantages.andO;The first is that is substantially reduces the complexity of the naturallanguage system, which no longer must generate baroque, ad hoc mappings.andP;  Ofcourse, we have accomplished this at the expense of increase complexity inthe domain modeling system.andP;  But in fact, that is the second advantage ofthis approach.andP;  It produces a more powerful domain model, which can in turnbe used to support other reasoning tasks in addition to natural languageprocessing.andM;Next consider the sentence:andM;Sue reads Melville.andM;Suppose that our knowledge base contains the concepts shown (clearlysimplified) in Figure 1.andP;  HermanMelwille is a person.andP;  MobyDickNovel is apiece of written material whose creator is Melville.andP;  Reading is a set (ofreading events); it is a subset of the more general set of all events.andP;  Eachreading event has a reader and an object (the thing that was read).andO;Associated with each of these last two attributes is a selectionalrestriction, which imposes a constraint on the values that the attribute cantake on.andP;  These constraints correspond to our knowledge of reading eventsthat can occur in the world.andP;  Only people can read; only written material canbe read.andP;  Assume that all the relevant knowledge that we have is shown in thefigure, and, in particular, that Person is not known to be a subclass ofWrittenMaterial (although it may be a subclass of such things as Mammal andAnimal, which are irrelevant for this example).andP;  Given this knowledge, it isnot possible to form a consistent interpretation of our sentence, sinceMelville is not WrittenMaterial and thus cannot be the value of the objectslot of a Reading event.andP;  The obvious solution to this problem is to create anew concept WorksByMelville and install it as a subclass of WrittenMaterial.andO;Then we can add a second meaning of the word &quot;Melville&quot; that points to thisnew concept.andP;  This solution works, but it is very specific.andP;  This same thingwill have to be done over and over, for example to handle sentences such as,&quot;Sue reads joyce,&quot; or &quot;Bob listens to Mozart.&quot;andM;A better solution is to add to our system the following linguistic rule: Thename of the creator of an object can be used to refer to the works he or shecreated.andP;  Then, assuming that the notion of creator exists in the knowledgebase, this rule will be able to generate a whole class of metonymous (1)meanings automatically as they are needed.andP;  A system that has this rule willbe substantially less brittle than one in which only the specific metonymoususes that have been entered by hand are available.andM;Now consider the sentence:andM;David ate the turtle.andM;Assume that the only relevant information contained in our knowledge base isshown in Figure 2.andP;  We again have the problem that we cannot produce aconsistent semantic interpretation because neither Turtle nor Animal is knownto be a subclass of Food.andP;  There are two obvious ways to fix this problem,each of which has serious drawbacks:andM;* Make the class Turtle (or perhaps its superclass Animal) a subclass ofFood.andP;  There are three problems with this solution.andP;  The first is that itblurs the distinction between living and dead things and thus reduces ourability to do common-sense inference; an animal--the turtle--can move, butfood--a bowl of turtle soup--cannot:  what do we conclude about somethingthat is both an animal and a food?andP;  The second difficulty is that a wholeanimal is not in fact food.andP;  In particular, in this case, we assume that theshell was not eaten.andP;  The third is that, just as in the Melville example,there is no generality; we will need a separate account of why &quot;tea&quot; canrefer to the bush or the beverage.andM;* Create by hand a new concept TurtleMeat and define a second meaning of theword &quot;turtle&quot; to be TurtleMeat.andP;  This solution works but it too has nogenerality.andP;  We would have to do the same thing with every other animal.andM;A better solution to our problem has neither of these limitations.andP;  In thisnew approach, we add two facts to our system:andM;* A fact about the world: The flesh of almost any animal can be food if theeater is hungry enough.andM;* A fact about language: The word for any naturally occurring entity canusually be used to refer to the most common products derived from it.andM;This fact about the word is true independently of whether language is beingused.andP;  It is a fact that anyone stranded in the forest needs to know.andM;This fact about the language applies quite broadly.andP;  It explains all of thefollowing meaning pairs:andM;* coffee, the plant coffee, the beverageandM;* Cotton, the plant cotton, the fabricandM;* chicken, the animal chicken, the foodandM;There are, of course, exceptions to this rule.andP;  For example:andM;* flax, the plant linen, the fabricandM;* pig, the animal pork, the foodandM;But there are a relatively small number of such exceptions and they arelisted in the dictionary, so it is easy to encode them into our system.andM;By separating linguistic knowledge and world knowledge in this way, we cancreate a system that is both more robust linguistically and more powerful asa problem solver than we would have if we collapsed the two kinds ofknowledge into an ad hoc dictionary of special-case meanings.andP;  Thisseparation does not mean that the knowledge base may not know about thelanguage.andP;  In fact, we will later argue that such knowledge is necessary tofacilitate acquisition.andP;  The important consideration is that the linguisticand domain knowledge be presented in different parts of the knowledge base:in short, the class Turtle and the word &quot;turtle&quot; must be representedindependently.andM;As a final example, consider the sentence, &quot;Joan walked to the store andbought two candy bars.&quot;andP;  What does this sentence &quot;mean&quot;?andP;  There are severalpossible answers, including:andM;1.andP;  (literally) Joan walked to the store and bought two candy bars.andM;2.andP;  Joan was able to walk.andM;3.andP;  Joan paid for two candy bars.andM;4.andP;  Joan did not buy three candy bars.andM;The first of these is the literal meaning of the sentence.andP;  The othersrepresent facts that are derivable from the literal meaning using knowledgethat we all have.andP;  Further, the person who wrote the original sentenceprobably intends for people who read it to derive those additional facts.andP;  Inthat sense, the meaning of the sentence includes the derived facts.andM;When we begin to think about a computational model of meaning, however, it isimportant to distinguish between literal meaning and additional facts thatmay be supposed to follow from it.andP;  Although the literal meaning may be vague(literal here does not mean precise), it provides a starting point that canbe used by other knowledge in the knowledge base (KB) to derive theadditional facts.andP;  It is the job of the natural language system to producethe literal meaning.andP;  Once that is done, additional reasoning, which mayexploit both facts about the world and facts about language is typicallyused, may apply.andP;  If we fail to produce the literal meaning first, then thereis no way to separate knowledge about language from knowledge about theworld.andM;In this example, fact 2 is an entailment of the literal meaning, by which wemean that there is no consistent model of the KB in which the literal meaningis true and the entailed expression is not.andP;  Fact 3 is a default assumptionthat is based on what we know about the world.andP;  Although it could be false(for example if Joan's sister actually handed over the money), it willnormally be assumed to be true in the absence of any evidence to thecontrary.andP;  Fact 4 is an implicature that is based on what we know about howlanguage is typically used.andP;  Since we generally try to be helpful andinformative when we speak [23], our readers assume that, if a strongerstatement than the one we made had been true, we would have said so.andP;  Thus,if Joan had bought three candy bars, we would have said that.andP;  This is anexample of a particular kind of implicature called scalar implicature [27].andM;In this article, we describe a Knowledge-Based Natural Language (KBBL) systemwhich is based on the kind of rules described in our descussions of theseexamples.andP;  It presumes the existence of a world model that is broad enough toallow a clean distinction between knowledge about the world and knowledgeabout how language relates to the world.andP;  We are exploiting the CYC system[39, 40] as that domain model.andP;  So we define natural language understandingin KBNL to be the process of mapping from an English expression to anexpression in CYC's representation language, CycL.andP;  Correspondingly, wedefine natural language generation to be the process of mapping from a CycLexpression to a natural language one.andM;Although there is a long tradition of work in knowledge-based Englishunderstanding systems, world knowledge is treated differently in KBNL than inthese other systems in two very important ways.andP;  The first is that KBNL isconcerned not with narrow KBs but with broad ones.andP;  We are interested inarticulating not a collection of ad hoc, highly specialized semantic mappingrules but rather a set of general rules tied to a broad ontology of worldknowledge.andP;  In this respect, KBNL contrasts with several kinds of efforts,including conceptual dependency [51] and its descendants (e.g., [38]), Absity[28], and domain-dependent text retrieval (e.g., [49]).andM;The second key difference in KBNL is that we are enforcing a clean separationbetween linguistic knowledge and world knowledge.andP;  Our goal in doing this isboth to provide a basis for modularity in system design and to enable the useof the domain knowledge in other kinds of problem-solving systems.andP;  Thisapproach is in distinct contrast to the approach taken in systems from avariety of traditions, including semantic grammars [26] (and commercialproducts such as Qandamp;A), conceptual dependency [51], semantic nets [55],definite clause grammars [46], and transportable interfaces [29, 30] (andcommercial products such as Natural Language from NLI).andP;  In all of thesesystems, domain knowledge was used only to support English question answeringand therefore was not isolated at all from the knowledge of English and itsuse.andM;The Components of aandM;Natural LanguageandM;Processing SystemandM;KBNL is intended to be a complete language processing system for typewrittenEnglish (although, as we will describe in the section, Applications of KBNL,it makes sense to extend it to other languages as well).andP;  It is composed of aset of processing components and a set of knowledge bases that thosecomponents use.andP;  In this section, we will provide a brief overview of KBNL'scomponents, a more comprehensive description of the knowledge bases that theyuse, and a sketch of the control structure that the major KBNL componentsexploit.andP;  In succeeding sections we will then describe each of the componentsin more detail.andM;The ProcessingandM;Components of KBNLandM;KBNL is composed of the following three processing systems:andM;* Lucy is a knowledge-based English understanding system.andM;*Koko is a knowledge-based English generation system.andM;* Luke is a lexical acquisition tool that exploits regularities in the waylanguage relates to a knowledge base to assist in building a robust lexicon(dictionary) that can be used by Lucy and Koko.andM;Figure 3 shows the relationships among these three systems.andP;  Lucy and Kokoare the workhorses of the system, since they provide the basic services ofunderstanding and generating English.andP;  Luke provides a service to Lucy andKoko, since it assists in building the lexicon on which they rely.andP;  Thesesystems can be used in a wide variety of applications to be discussed later.andM;KBNL's Model ofandM;Language ProcessingandM;All the KBNL systems are based on a common model of the process of mappingbetween English expressions and knowledge base representations.andP;  This commonmodel is important because the various knowledge bases in the system arestructured around it.andP;  Sharing a single model of language processing enablesthe KBNL systems to share linguistic knowledge bases, including dictionaries,grammars, and semantic interpretation rules.andP;  This shared model of languageprocessing is shown in Figure 4.andP;  Understanding moves from top to bottom inthis figure; generation does the reverse.andM;Here we will describe each of the stages shown in the figure.andP;  In addition,Figure 5 shows examples of instances of the mapping rules that enable Lucyand Koko to move through the stages.andM;The English string can be any well-formed English expression.andP;  (2)andM;The word sequence is a sequence of units roughly corresponding to the words(or anything that occurs between word boundaries) of the English string.andO;These units have been assigned syntactic categories and may have internalstructure.andP;  For example, the surface string &quot;ate&quot; will be analyzed as a verbformed from the root verb EAT plus the past tense marker + ed.andP;  To performthe mapping between strings and analyzed units we need a set of morphologicalrules, such as the one shown in Figure 5 (a).andP;  These rules describe theprinciples of English word formation.andP;  We also need one or more lexiconswhose entries assign syntactic descriptions to the units.andM;The Syntactic Logical Form represents the syntactic structure of the Englishstring.andP;  It breaks the word string into constituents (e.g., noun phrases,relative clauses) and assigns a syntactic function (subject, object, etc.) toeach of them.andP;  (3)  A syntactic logical form records both surface syntacticfeatures and mapping onto &quot;logical arguments.&quot;andP;  For example, the surfacesubject of a passive sentence maps to the same role (the &quot;deep object&quot;) asthe direct object of an active sentence.andP;  Both levels of information arenecessary, since discourse phenomena are often sensitive to surfacestructure, while semantic interpretation works most simply at the level oflogical arguments.andP;  To map between morphological analyses and syntacticlogical form structures we need a grammar, which specifies both howconstituents can be combined and how to assign syntactic functions to theresults of the combination.andP;  For example, given the transitive verb &quot;eat,&quot;the noun &quot;peach&quot; and the determiner &quot;a&quot;, the grammar allows us to combine thedeterminer and the noun into the noun phrase &quot;a peach,&quot; and to combine thatin turn with the verb to form a verb phrase, &quot;eat a peach,&quot; in which the nounphrase fills the object role.andP;  Figure 5 (b) shows what this mapping lookslike.andM;The knowledge base logical form represents the full linguistic meaning of theexpression.andP;  In consists of a discourse representation structure [25, 36].andO;It contains CycL predicates rather than English words.andP;  At this level,quantifiers have been assigned scope; in &quot;every student ate a peach&quot; we havedecided whether they all necessarily ate the same peach.andP;  In addition, theantecedents of pronouns have been determined.andP;  For example, for the sentence,&quot;He ate a peach,&quot; uttered in some particular discourse context, we will havedetermined who &quot;he&quot; refers to.andM;This level is not, however, the ultimate interface to the knowledge base fortwo reasons: it retains linguistic information that the knowledge base cannotuse and fails to make distinctions that the KB needs.andP;  Tense and focus aregood examples of the first of these.andP;  Specifically in the case of tense,KBNL's theory is  based on [35] and employs concepts such as speech point andreference point, which are defined only in terms of a discourse and do notdenote properties that events have as objects in the world (or in the KB).andO;(See [59] for related work.)andP;  Another example of the need for thisintermediate level is the analysis of verbs.andP;  Some verbs (e.g., &quot;go&quot;)correspond to events, and their KB representation is in terms of specificinstances.andP;  Other verbs (e.g., &quot;live&quot;) are stative; their KB representationis normally a value of a slot (e.g., &quot;John lives in France.&quot;).andP;  But it isuseful at the linguistic level to have a uniform representation ofpredication, in which each verb has a discourse referent, and is a distinctentity, regardless of its eventual translation.andP;  Consider, for example, thesentence, &quot;John lives in France but his mother doesn't like it.&quot;andP;  We need areferent for John's living in France, since that is the antecedent of &quot;it.&quot;andM;Yet another example of the need for a separate layer of natural languagerepresentation is provided by generic sentences [12].andP;  Consider, for example,&quot;Dogs have tails,&quot; &quot;Dogs are mammals,&quot; and &quot;Small dogs get cold easily.&quot;andO;These sentences are similar linguistically.andP;  But they are represented in verydifferent ways in our KB.andP;  The first becomes a default attribute to beinherited by all instances of the class of dogs.andP;  The second is representedin the KB as a generalization link between the class of dogs and the class ofmammals.andP;  And the third is represented as a logical expression that describesonly those elments of the class of dogs that happen to be small.andP;  In order tobe able to look at each of these sentences and decide which KB representationis appropriate, we need a level at which we have converted from English wordsto predicates in the KB, but at which we have not built a complete KBrepresentation.andP;  Knowledge Base Logical Form provides that level.andM;To map between a syntactic logical form and its associated knowledge baselogical form we need: lexicons that assign KB interpretations to individualwords, rules that manipulate those assignments to handle noun compounding andother phenomena discussed earlier, an algorithm that combines theinterpretations of the individual words, rules for assigning quantifierscope, and finally, rules for discourse integration.andP;  For example, thelexicon will specify that &quot;peach&quot; maps onto the CycL expressionandM;This expression will be true of any: X in the KB that represents a peach.andO;The entry for &quot;eat&quot; will map onto (allInstanceOf :E EatingEvent)(objectConsumed :E :object) (performer :E :subject)andM;Here the first assertion describes the kind of event involved, while thesecond and third specify which roles the syntactic object and subject fill inthat event.andP;  These rules, when combined with the syntactic and morphologicalrules mentioned above and additional rules that assign quantifier scope,perform the mapping shown in Figure 5 (c).andP;  The interpretation of thediscourse representation structure that is shown there is roughly:  for every:Y that is a student, you can find an :X that is a peach and an :E that is aneating event so that :Y performs :E, :X is eaten during :E, and :E occurredprior to the time of the uttenrance.andM;The CycL description is a statement of the meaning of an expression inexactly the language that is used by the KB.andP;  When it is derived using Lucy,this type of expression can be used to update the KB or ask a question of it.andO;Alternatively, the KB can create such an expression when it wants to use Kokoto communicate with the user.andP;  An example of the mapping between the CycLdescription and an input sentence is shown in Figure 5 (d).andM;This organization of linguistic knowledge into a single framework thatsupports both understanding (via Lucy) and generation (via Koko) is one ofthe most significant characteristics of the KBNL system.andP;  This structurecontrasts with standard ones employed in understanding and in generatingsystems, both of which typically exploit knowledge bases with significantprocedural components that are not reversible.andP;  For example, manyunderstanding systems exploit the Augmented Transition Network framework [62,63] as a basis of their syntactic parsers.andP;  The framework supports aprocedural component, in which actions are associated with arc transitions,and network-based grammars typically exploit it.andP;  Going the other way,traditional language generation systems (e.g., [42] and [43]) have alsoexploited irreversible, procedural knowledge.andP;  Although some systems (e.g.,andO;[11] and [54]) exploit reversible knowledge, they are more limited in theknowledge they use, as we will see below.andM;Lucy and KokoandM;System OrganizationandM;The processing model described above suggests a way of organizing Lucy andKoko.andP;  They are both composed of a set of cooperating knowledge sources, eachof which performs one step of the mapping process that we have described.andO;These knowledge sources communicate with each other through a shared datastructure that represents the current state of all the processes.andP;  Thestructure of this shared representation has evolved, from one that initiallysupported only syntactic processing in Lucy to one that has been generalizedto support all steps of the process, in both Lucy and Koko.andP;  We will brieflysketch that evolution here.andM;Syntactic processing in Lucy is done using a conventional chart parser [17,37].andP;  The centra data structure of such a parser is the chart, which containsa set of edges, each of which corresponds to one syntactic interpretation ofone syntactic constituent.andP;  The chart is initialized with a set of edges thatcorrespond to the words of the input expression.andP;  New edges are added to thechart by the parser as it forms larger constituents.andP;  There may be more thanone edge corresponding to a single constituent if there is more than onesyntactic interpretation of it.andP;  Figure 6 shows an example of a chartproduced during Lucy's parse of the sentence, &quot;What did Fred use to makebreakfast?&quot;andP;  Each line in the figure corresponds to an edge on the chart.andP;  Itshould be noted that the chart is a two dimensional structure:  the x-axiscorresponds to the temporal order of words in the sentence; the y-axiscorresponds to the size of the constituent that is being represented.andO;Parsing in Lucy operates bottom to top (and usually, but not necessarily,left to right), forming larger constituents from smaller ones.andM;We can generalize this structure to support the other stages of Lucy'sprocessing by adding a third dimension to the chart.andP;  The resulting z-axiscorresponds to &quot;degree of elaboration.&quot;andP;  The chart can now be viewed as acube, as shown in Figure 7 for the simpler sentence, &quot;List the file.&quot;andP;  Theoriginal chart, whose edges contain only syntactic information, forms thefront face of the cube.andP;  As we move back along the z-axis, edges containadditional information, corresponding to the results of various stages ofsemantic processing.andP;  Just as there could be alternative edges for the sameconstituent on the chart if there were more than one syntacticinterpretation, there can now be several edges back along the z-axis, all ofwhich correspond to a single edge on the syntactic chart.andP;  This occurswhenever a single syntactic constituent has more than one semanticinterpretation.andP;  The overall sentence understanding process in Lucy operatesbottom to top, front to back in this structure.andP;  Processing begins with edgesthat correspond to individual words about which only syntactic informationhas been considered.andP;  It progresses upward, forming larger constituents andbackward, adding semantic information, until an edge that contains a completeinterpretation for the entire expression is constructed.andM;At this point, it is useful to think of this shared structure not as a chart,in the restricted sense in which the term is used in chart parsing, butrather as a more general structure.andP;  In chart parsing, the operators thatapply to edges are grammar rules.andP;  In our more general structure, theoperators are knowledge sources that correspond to the stages of languageprocessing.andP;  The blackboard model [18] provides a good framework fordescribing this more general structure.andP;  The specific blackboardimplementation that KBNL uses [14] is part of the HITS interface constructionsystem [31].andM;So far, we have described the use of the blackboard for sentenceunderstanding in Lucy.andP;  Because Koko shares Lucy's model of linguisticprocessing, it can use exactly the same blackboard structure.andP;  It simplyoperates in reverse.andP;  It begins in the upper rear corner of the blackboard,with an edge that contains a semantic description that needs to be mappedinto a linguistic expression.andP;  It operates forward and downward, until it hasproduced a syntactically valid string of words that correspond to the initialmeaning.andM;One advantage of the fact that Lucy and Koko share a top-level structureorganized around the blackboard is that they can be interleaved with eachother and with other programs that use the blackboard (e.g., graphicalinterface systems).andP;  Within the KBNL system, the most significant use forthis ability is the generation of paraphrases.andP;  This is particularly true ifLucy finds more than one acceptable meaning for an input expression (orsubexpression); it may be useful to ask a user which of the meanings wasactually intended.andP;  This can be done by invoking Koko on Lucy's output edgesand asking it to generate a new string corresponding to each edge.andP;  Thesegenerated strings can then be presented to the user, who can choose the onethat corresponds to his or her original intent.andM;One other important aspect of this blackboard structure is that itcorresponds to a completely monotonic process.andP;  During both understanding andgeneration, each successive process may only add information to the edge thatserves as its input; it may never change what is there.andP;  If a process is sureof its output, it may add its result to its input edge.andP;  If not, it mustcreate a branch along the appropriate axis of the blackboard and generate anew edge for each alternative interpretation.andP;  The major advantage of thiscommitment to monotonicity is that is is never necessary for the system tobacktrack.andM;Lucy--A System forandM;English UnderstandingandM;Lucy is an English understanding system.andP;  In some ways, it is a conventionalone; it uses standard techniques for morphological analysis and parsing.andP;  Butin some very important ways it is unconventional.andP;  Its model of semanticinterpretation is rich enough to support many kinds of metonymy.andP;  It canexploit a broad base of world knowledge that is stated separately from itslinguistic knowledge bases.andP;  And it exploits linguistic knowledge bases thatit shares with Koko.andP;  In the remaining part of this section, we will describethe way Lucy works, emphasizing the ways in which it is unconventional.andM;To map from English expressions to CycL ones, Lucy must move through thestages shown in Figure 4.andP;  It must move through them in the order given,since the output of each step corresponds to the input of the next.andP;  However,Lucy can perform these steps on subconstituents of a sentence (e.G., nounphrases).andP;  This may be desirable when the branching factor at an early stepis very large, since combining two constituents, each of which has manyinterpretations, can result in a combinatorial explosion in the number ofinterpretations.andP;  In such cases, it may be useful to use later processingstages to reduce the search space by eliminating subconstituents on semanticgrounds.andM;Morphological AnalysisandM;This stage takes a string as input and outputs a (possibly ambiguous)sequence of analyzed words.andP;  To do so, it makes use of a lexicon, whichcontains roots, affixes, and, optionally, complete words, as well as a set offinite state transducers, which are used to normalize the input.andP;  Thesetransducers will break the input into morphemes, and deal with spellingvariations.andP;  For example, the plural of English words that end in &quot;-ch&quot; isformed by adding &quot;es,&quot; instead of simpel &quot;s,&quot; so the transducers must removethe &quot;e&quot; during understanding, and insert it during generation, so that thelexicon records only the simple forms &quot;church&quot; and &quot;s-plural.&quot;andP;  English has avery simple morphology, so that it is possible, and more efficient, simply tolist most inflected forms of words in the lexicon, avoiding the need toanalyze words at runtime.andP;  However, the transducers are still necessary todeal with &quot;non-words&quot; input, such as file names, numbers, dates, and timeS,which cannot simply be listed in the lexicon.andP;  Morphologically richerlanguages, such as Finnish or Arabic, would pose much more of a computationalchallenge for this component.andM;Syntactic AnalysisandM;Parsing in Lucy consists of two logically distinct steps: combiningconstituents into larger units, and producing the structural description (thesyntactic logical form) of the outcome.andP;  The former step is performed with abottom-up chart parser (as described earlier) and a categorial unificationgrammar [1, 45, 61].andP;  In categorial grammars, unlike phrase structuregrammars, most of the information is in the words themselves, and there arerelatively few rules.andP;  For example, a transitive verb is treated as asentence that is missing two noun phrases, and simple combination rulescombine it with its subject and object.andP;  The grammar also contains arelatively small number of category-changing rules to handle constructionslike the passive, where a transitive verb is converted into an intransitiveone (i.e., into a sentence that is missing only a single noun phrase.)andO;Parsing may proceed in any order (right-to-left, left-to-right, inside out)and rules and lexical entries have optional context tests to restrict theirdomain of applicability.andM;In categorial grammars, parse trees tend to reflect indiosyncrasies of theorder in which constituents are combined rather than the suyntactic structureof the sentence.andP;  That is why the final output of syntactic processing inLucy is the more abstract syntactic logical form structure that was describedearlierandM;This structure is created from the parse tree by solving functional equationsfound on each lexical entry and rule.andP;  A typical set of equations would bethose for a transitive verb, specifying that its first argument (the firstconstituent it was combined with) is its object, and its second argument isits subject.andP;  The resulting structure is a directed graph whose arcs arelabelled wit syntactic roles and whose nodes correspond to the constituentsthat fill those roles.andM;Semantic InterpretationandM;Although the early stages of processing are necessary to anu working naturallanguage system, they are not substantially affected by the use of acomprehensive domain model.andP;  As a result, Lucy looks a lot like many otherEnglish understanding systems in its treatment of morphology and syntax.andP;  Itis only when we get go semantic processing that we begin to see the impact ofour knowledge-based approach to natural language processing.andP;  Here we willdescribe the semantic processing component of Lucy and show how it isaffected by the use of a powerful domain model.andM;Semantic interpretation in Lucy does two things:andM;* It attempts to create a knowledge base logical form representation of themeaning of the syntactic logical form structure(s) it is given.andM;* It rejects meanings and their syntactic logical form structures if thosemeanings are incomsistent with what is known about language and the world.andO;The knowledge contained in the KB provides the basis for doing this.andM;The result of the combination of these processes is a set of discourserepresentation structures tat represent semantically plausibleintertretations.andP;  The presence of a rich domain model, along with acorresponding description of how language is used to relate to that model,affects both of these processes, as we will show.andP;  The overall approach tosemantic processing that we are about to describe in based on the theory ofsemantics first presented by Montague [58].andM;Lexical SemanticsandM;In its simplest form, the first stage of semantic interpretation is astraight-forward process that does a recursive tree walk through thesyntactic logical form structure.andP;  At the leaves, it applies the semanticinterpretations that it retrieves from the lexicon when it looks up thewords.andP;  At other nodes, it combines constituents and checks for consistency.andO;For words with more than one translation, it branches and considers thecrossproduct of the individual meanings.andP;  A simple example of this processwas given earlier.andM;To show how nodes are combined it is necessary to introduce the notion of adiscourse referent.andP;  A formal treatment of discourse referents is given in[25] and [36]; for our purposes they can be viewed as &quot;long-lived&quot; variablesthat are accessible where normal bound variables would not be.andP;  the simplestcase is the following: &quot;A man entered.andP;  Then he left.&quot;andP;  If &quot;a&quot; is translatedwith an existential quantifier, the first sentence can be represented as: ($x) (man(x) andamp; enter(x)).andP;  When we get to the second sentence, we would like toset &quot;he&quot; =x, but we cannot, since we are outside the scope of the existentialquantifier.andP;  We could try to merge the first and second sentences into onelarge quantified expression, but then a number of problems arise, includingcases where no choice of quantifiers and scopings captures the meaning wewant.andP;  (See the cited works for more discussion.)andP;  Instead, we let &quot;a man&quot;set up a kind of global variable, a discourse referent, to which anysubsequent sentence can refer.andP;  Returning to the problem of constructing aknowledge base logical form structure from a syntactic logical form, we nowsee that syntactic logical form nodes can be combined by substituting theappropriate discourse referents for the arc names in the lexical entries.andP;  Inthe peach example of Figure 5(c), the discourse referent for &quot;a peach,&quot; :X,has been substituted for the :object parameter in the lexical entry for&quot;eat,&quot; indicating that we are referring to an:X that is both a peach and theobjectconsumed of the EatingEvent in question.andM;At each step of knowledge base logical form formation we check forconsistency with the KB, rejecting those interpretations that describe&quot;illegal&quot; objects or events.andP;  There are two forms of consistency checking, aquick form that may miss some inconsistencies, and a slower, more thoroughform.andP;  The quick version gathers up all the classes to which each referenthas been asserted to belong, either directly or indirectly (through domainand range restrictions on slots), and rejects any interpretation where areferent is assigned to a part of incompatible classes (i.e., classes thatare mutually disjoint with each other.)andP;  The more thorough method builds KBunits for the interpretation.andP;  In the peach example, we would create a unitfor an instance of a peach, another unit for an EatingEvent, then put thefirst unit on the second one's objectConsumed slot, etc., rejecting anyinterpretation where the KB signals a violation.andP;  This form of checking is,of course, much more powerful than the first, since any of the KB's inferenceprocedures can be triggered at any step of the way.andM;Consistency checking would rule out one interpretation in the peach exampleif the word &quot;peach&quot; had two meanings, one for the fruit and the other for acolor.andP;  It would also come into play, for example, if the input containedambiguous function words, since in this case the grammar alone isinsufficient for determining the way the phrases of the sentence should beput together.andP;  To see this, consider the two different meanings of &quot;at&quot; inthe sentence, &quot;John ate a peach at (location) Bill's house,&quot; and &quot;John ate apeach at (time) 7 o'clock.&quot;andM;Semantic RulesandM;The process of merging translations becomes more complex when we have to domore than just merge lexical entries.andP;  Consider the noun compounds: &quot;cornoil,&quot; &quot;olive oil,&quot; &quot;linseed oil,&quot; safflower oil,&quot; and &quot;baby oil.&quot;andP;  We couldjust add special modifier meanings for &quot;corn,&quot; etc., to the lexicon, but, aswe argued earlier this approach is brittle.andP;  We need a rule to generatemeanings as needed.andP;  In fact, as the &quot;baby oil&quot; example shows, we need atleast two rules, each sensitive to the kinds of objects in question.andP;  Thefirst rule states that if X denotes some kind of organic matter, and Ydenotes some sort of product, then &quot;an XY&quot; denotes a Y that is madeFrom X.&quot;andO;The second rule states taht if X denotes a higher animal, and Y a product,&quot;an X Y&quot; is Y forTheUSeOf an X.&quot;andP;  We need a similar sort of rule to handlemetonymous expressions, and for the semantics of vague works like &quot;have&quot; and&quot;of&quot;, where the set of possible meanigs is so large that is simply is notpractical to list them all in the lexicon.andP;  The rules used in KBNL aresimilar to those used in some other systems, (e.g., [19, 15, 24, 30, 57]),with the key difference being that in KBNL the rules are reversible, so theycan be used in both Lucy and Koko.andM;All KBNL's rules can be thought of as having a leaf-hand side consisting ofone or more CycL expressions (e.g., the two nouns, in cases of nouncompounding), and a right-hand side consisting of a single expression thatcorresponds to the interpretation of the whole compound.andP;  Multi-wordcompounds such as &quot;aluminum siding salesman&quot; are handled by multiple ruleapplications.andP;  Lucy runs these rules in a forward-chaining direction, whileKoko does the reverse by backward-chaining them.andP;  To implement these rules weneed a unifier that will match potential inputs against the rule template,and a mechanism for computing a fully instantiated output template from a setof bindings for the input.andP;  This mechanism makes extensive use of the KB,since, for example, it mut block application of the Corn-Oil rule to thephrases &quot;baby oil&quot; and &quot;motor oil&quot; by knowing that oil cannot be produced byprocessing humans or mechanical devices.andP;  similarly, given Shakespeare, itmut know that he is a writer, and not a musician, therefore uses of his namewill refer to written material, rather tan music.andP;  Returning to thedistinction between linguistic and world knowledge, we see that it issemantic rules of this sort that capture such linguistic facts as the name ofan artist can be used for his creations.andP;  At the same time, the KB knows thatShakespeare is a Poet and that poets are artists; together the rule and theKB license the use of &quot;Shakespeare&quot; to refer to his poetry.andM;The use of such rules greatly increases the number of possibleinterpretations for many expressions, and some form of control is needed toavoid combination explosion.andP;  Lucy therefore assigns a level of effort toeach lexical entry and rule, and an item is invoked only when its level ofeffort is reached; this will occur only if there is no consistent translationat a lower level.andP;  This ensures that metonymy rules are considered only whenliteral translation fail, and that more specific compounding rules overridemore general ones.andP;  This form of control suffices for the moment, but moresophisticated techniques are likely to be necessary in the future.andM;Quantifier Scope andandM;Discourse IntegrationandM;To complete the construction of a full discourse representation structure, weneed to assign scopes to quantifiers and integrate the representation of theindividual sentence into its discourse context.andP;  Though good theories areavailable on how to represent quantifier scopes and anaphoric links, thereare no satisfactory theories on how to choose the correct scopes andantecedents.andP;  As a result, both these modules of KNBL are designed to allowexperimentation with various partial theories, rather than reflecting acommitment to any particular theory.andP;  Both modules are organized as sets ofindependent knowledge sources, each representing an  individual theory offactor affecting scope or anaphora, with a separate scoring mechanism totally up the votes and choose the final result (see [48] for more details.)andO;The separation of knowledge sources is all the more important because theindividual theories work at different levels of representation.andP;  For example,in &quot;John was looking for Bill.andP;  He had not seen him since lunch,&quot; syntaxtells us that &quot;he&quot; and &quot;him&quot; cannot refer to the same person, but discoursefactors (including a syntactic preference for parallel constructions and asemantic preference for an interpretation in which the person we areconcerned about seeing in the one who was being looked for) tell us that &quot;he&quot;probably refers to John and &quot;him&quot; to Bill.andM;The discourse context can also be used for disambiguation.andP;  In the discourse&quot;John went to the bank.andP;  After he took out a loan, ...&quot;,the word &quot;bank&quot; is nolonger ambiguous (between a financial institution and the edge of a river),since the next sentence makes it clear which kind of bank he went to.andP;  Lucytherefore prefers interpretations that can be linked to other interpretationsin the discourse by means of temporal relations in the KB.andP;  That is, given asequence of event descriptions &quot;A then B then C&quot;, Lucy prefers thoseinterpretations of A, B, and C that are known to occur commonly in thatorder.andP;  Ultimately, this process of discourse linking will be extended toconsider relations other than temporal order, so that we will consider, forexample, the possibility that one utterance is intended as an explanation,modification, or amplification of another.andM;Creating a CycL RepresentationandM;The final step in the interpretive process is translating the knowledge baselogical form that contains the discourse representation structure into a CycLexpression that can be evaluated or run as a query.andP;  As mentioned earlier, wechoose to keep the discourse representation structure as an intermediarybecause it is cutom-tailored for representing linguistic meanings, while CycLis designed to represent facts about the world.andP;  Having this extra levelleaves u s free to handle linguistic phenomena in a natural way, withoutforcing a &quot;natural language view of the world&quot; onto the KB.andP;  The process ofconverting from this intermediate level (knowledge base logical form) to aCycL expression is driven by a set of CycL-specific rules tat encodeknowledge about how the CYC KB is organized.andP;  if the KBNL system were to beused with a different knowledge base system, new translation rules would benecessary, although many of them would probably be straightforward variantsof the CycL rules.andM;KoKo--A System forandM;English GenerationandM;Koko generates English descriptions of KB objects using the same linguisticknowledge that Lucy uses, but reversing the stages of processing.andP;  In thissense, we can think of Lucy and Koko as inverses of each other.andP;  There is,however, one important way in which they are fundamentally different fromeach other.andP;  Lucy's job is to find a KB meaning that best describes what theoriginal speaker/writer intended to say.andP;  Therefore, if more than oneinterpretation can be derived using its knowledge, Lucy cannot choose freelyamong them; it must chose the one most likely to correspond to the originalintent of its input.andP;  As we have shown, it does this by favoring theinterpretation that is most likely correct, given what is already known to betrue about the world.andP;  Becau se the knowledge in the KBNL system's knowledgebase usually defines many-to-many mappings between English expressions and KBstructures, Koko also frequently has choices to make as it proceeds.andP;  Koko'sjob is to make those choices so that the generated expression bestaccomplishes the larger system's overall goal.andP;  But this is a much less wellfocused task than the one Lucy does.andP;  The constraints that can be appliedduring the process are weaker than those Lucy can exploit.andP;  There are usuallymany ways to describe the same event or object.andP;  Because so many choices mustbe made in knowledge-based generation, the process has traditionally beendivided into two phases: deciding what to say, and deciding how to say it.andO;The components responsible for these two phases are called the strategic andtactical components.andP;  We shall consider them in order.andM;The Strategic ComponentandM;Koko is the most recent addition to KBNL, and its strategic component is onlypartly developed.andP;  For the moment we are concentrating on generating singlesentences or phrases.andP;  Even when we restrict ourselves to these short spansof text, there are usually a multitude of possibilities to choose from.andP;  Forexample, when asked to describe the unit Egg-001, Koko could generate any ofthe following: &quot;an egg,&quot; &quot;it,&quot; &quot;Mary's breakfast,&quot; &quot;the food that Johnprepared,&quot; &quot;the thing that Mary ate,&quot; etc.andP;  Though Koko will ultimately needa model of the user's knowledge and interests to make these choices, atpresent it uses a system of default &quot;salient&quot; properties, which are recordedin the KB.andP;  For example, if the salient properties for the class PreparedFoodare the slots ingredients and methodOfPreparation, Koko will generate, &quot;ascrambled egg.&quot;andP;  If we add the slot objectConsumedIn (which links an instanceof food to the EatingEvent in which it was eaten), Koko will now generate,&quot;the scrambled egg that Mary ate.&quot;andP;  Koko can make a more sophisticated choicewhen it is explicitly comparing two objects.andP;  For example, if Egg-002 is alsoa scrambled egg, and we want to ask the user which egg he meant, themethodOfPreparation is no longer salient, and Koko will generate, &quot;The eggthat John ate or the egg that Mary ate?&quot;andP;  We are now working on making thechoice of properties sensitive to the more general discourse context.andP;  In thelong run, Koko will need more sophisticated rhetorical strategies to enableit to plan entire paragrphs and text ([32], [41], [43].)andM;The Tactical ComponentandM;Given a specification of what to say, Koko's tactical component generates oneor more strings that have the desired meaning.andP;  More precisely, the input tothis component consists of a discourse representation structure, a&quot;distinguished referent&quot; that the utterance is supposed to be about, and,optionally, a partially syntactic logical form, which is used if there isreason to prefer to generate a particular syntactic structure rather thananother.andP;  (For example, this is often the case in translation systems.)andO;While the discourse representation structure contains the meaning, thedistinguished referent specifies a point of view on the meaning.andP;  Forexample, given a set of assertions describing a cat wearing a hat, we can, byspecifying different referents, focus on the cat, saying, &quot;A cat that wore ahat,&quot; or on the hat, generating, &quot;A hat that a cat wore,&quot; or on the wearingevent, producing either, &quot;A cat wore a hat,&quot; or &quot;A wearing of a hat by acat.&quot;andP;  The strategic component can give more precise syntactic instructionsto the tactical component by specifying a partial syntactic logical form,which the tactical component will then attempt to match.andP;  By specifyingvarious choices for the subject noun phrase, the strategic component canforce the tactical component to generate an active or passive sentence.andM;The design of Koko's tactical component has been strongly driven by theadvantages (as described in [2, 34, 53]) of bidirectionality.andP;  Koko runs onthe same blackboard as Lucy; it uses the same linguistic knowledge bases Lucyuses [5], and it exploits a variant of the bidirectional generationalgorithms described in [11] and [54].andP;  The algorithm works by &quot;peeling off&quot;a lexical item with the desired syntactic category and with semantics thatpartially  match the input.andP;  It then recurses on the remaining assertions.andO;In Figure 8, Koko is in the process of generating the verb phrase &quot;eat apeach,&quot; and, having found a lexical match with &quot;eat,&quot; has established asubgoal to generate a noun phrase matching the remaining assertion.andP;  Kokoincrementally constructs a syntactic tree bottom up as the assertions arematched, and branches whenever there are multiplel lexical or syntacticchoices.andP;  The search fails if Koko either cannot find any suitable lexicalitems, or cannot match the input syntactic logical form.andM;To find lexical items, Koko uses an inverted lexicon to reverse the mappingsthat Lucy uses; where Lucy's lexicons map from strings to syntactic entriesto (possibly multiple) semantic entries, Koko's lexicon maps from KB units tosyntactic entries and then to strings.andP;  Thus, the understanding lexiconcontains pointers from &quot;cat&quot; to CAT-NOUN, and then to(allInstanceOf:XFeline), while the generation lexicon reverses the order,mapping from (allInstanceOf:X Feline) to CAT-NOUN to &quot;cat.&quot;andP;  Since the entrypoints into its lexicon are DB units, Koko can use the KB class hierarchy tofind more general and more specific words, as well as such things assynonyms.andP;  Thus, if Koko is trying to generate a nount matching(allInstanceOf:X Feline), and cannot find a word in the lexicon, it can moveup the class hierarchy to Mammal or Animal and generate &quot;mammal&quot; or &quot;animal.&quot;andM;To generate noun compounds and metonymous expressions, Koko uses the samesemantic rules the Lucy does.andP;  It just reverses the order of application.andO;Suppose that Koko's immediate subgoal is to generate a noun referring to Oilthat is madeFrom Corn.andP;  Since there is neither a noun in the lelxicon thatmatches all of these assertions, nor an adjective that matches some of them,Koko's only choice is to try to revise its goal using a semantic rule.andP;  I nthis case, backward chaining the previously mentioned Corn-Oil rule allowsKoko to satisfy the original goal by satisfying two new subgoals: generatinga noun denoting Corn and another noun denoting Oil.andP;  Both of thes nouns canbe found in the lexicon, and Koko generates the compound &quot;corn oil.&quot;andP;  Sincethe input and output of compounding rules are both noun goals, rules canultimately apply to the output of other rules, generating multiword compoundssuch as &quot;corn oil factory foreman.&quot;andM;The generation of metonymous expressions is more complicated since there isalways another way of expressing the same content.andP;  Suppose Koko's goal is togenerate a noun phrase denoting an instance of Poem whose author isShakespeare.andP;  If the goal syntax and semantics that are specified by thestrategic component indicate brevity of informality, Koko tries to revise theoriginal goal by applying metonymy rules.andP;  In this case, the Creator rulementioned earlier will succeed, setting a revised goal of generating a nounphrase referring to Shakespeare.andP;  This goal succeeds, and Koko's final outputis &quot;Shakespeare.&quot;andP;  Of course, using a metonymous expressin is more than justa matter of brevity  and informality; there are many stylistic factors andassumptions about the hearer involved.andP;  Though everything Koko says iscorrect, it will not really sound fluent until its strategic component ismore sophisticated.andM;Luke--A LexicalandM;Acquisition SystemandM;A major part of any knowledge-based natural language system is the lexicon, arepository of information about words and how those words correspond toknowledge base entities.andP;  Luke [64] is a system for semiautomaticallyconstructing very large lexicons.andM;A standard approach to building natural language systems is to create afront-end for an already existing knowledge base, as shown in Figure 9.andP;  Thisapproach suffers from the problem that the natural language expert mustinvest agreat dea of time in understanding the structures in the knowledgebase in order to be able to make the necessary connections betwen KB objectsand the words that are to be entered into the lexicon.andP;  The design of the KBmay not support language processing very well.andP;  Finally, if the knowledgebase changes, it is difficult to maintain synchrony betwen it and thelexicon.andM;Our approach is to integrate the processes of lexicon construction andknowledge base construction, as shown in Figure 10.andP;  We build up the twosimultaneously.andP;  This integration has a number of implications:andM;* The natural language expert has been taken out of the loop.andP;  We will buildhis or her expertise into the integrated acquisition tool.andM;* The knowledge engineer is now responsible for building the lexicon.andP;  Thisis good, because the knowledge engineer is the person who is in the bestposition to know how words and phrases map onto knowledge base entities--hehas created those entities himself, and he speaks a natural languagefluently.andP;  The problems lie in tapping his linguistic intuitions withoutassuming any specific knowledge of natural language technology.andM;* The power of the natural language system is available to the knowledgeengineer via Show, which, as we will describe, provides natura languageaccess to units in the KB.andM;Language acquisition tools for interfaces to databases, (e.g., TEAM [24] andTELI [4]), have pioneered the idea of gathering lexical information fromdatabase experts who have no knowledge of natura language processingtechnology.andP;  Suppose the word &quot;water&quot; is associated with some databaseobject.andP;  Rather than ask something like,andM;Is &quot;water&quot; a count noun or a mass noun? systems like TEAM present the userwith sample sentences:andM;In this way, lexica entries are constructed from information taken from adatabase expert, not a language expert.andP;  When we move from databses toknowledge bases, we suddenly have a great deal more structure to exploit.andP;  Wecan use the knowledge base to guess lexical structures.andP;  Tell's guesses arewhat are presented to the user:andM;How can Luke make the correct guess?andP;  It guesses the property of the word&quot;water&quot; by examining the concetp Water, and noting that if we were to cut upWater into smaller and smaller pieces, each piece would still be Water.andP;  TheCYC knowledge base makes a distinction between types of substances (such asWater) that can be cut up like this, and types of entities (such as Dog) thatcannot.andP;  Language in turn reflects that distinction in the lexical classesfor count and mass nouns.andM;When we tease apart knowledge about the world and knowledge about language,we find there is a great deal of redundancy between the two.andP;  We can use thatredundancy to make guesses about one based on what we know about the other.andO;For example, if we know a lot about the concept EatingEvent, we can make goodguesses about properties of the verb &quot;eat.&quot;andP;  Similarly, if we know that thename &quot;Nancy&quot; has feminine gender, we can make specific guesses about thephysical features of the Person with which that name is associated.andM;Although the goal of Luke is to build a large, computationally usefullexicon, its emphasis is very different from most work in computationallexicography (e.g., [8, 10]).andP;  That work concentrates on finding andrepresenting relationships among words.andP;  In contrast, Luke is aimed atfinding relationships between words and knowledge-base entities.andP;  Thus wordrelationships in a Luke lexicon are secondary; they can be derived from theknown mappings from words to KB entities and from KB entities to other KBentities.andM;Luke currently contains approximately 30 heuristics about morphological,syntactic, and semantic properties of single words.andP;  For example, it knowshow to form plurals, comparatives, and superlatives;  how to create semanticmapping rules for nouns (using the mass/count distinction described above);and how to create semantic mapping rules for adjectives (including the factthat adjectives can be used either as modifiers or as predicate adjectives).andO;These heuristics are represented as rules in CycL.andP;  In addition, Lukecontains other heuristics that involve multiple words and concepts.andP;  Forexample, during the acquisition of a noun, Luke will ask for other nouns andprepositions that refer to slots of the KB concept to which that noun refers.andO;Luke also asks for synonyms.andP;  If a noun is associated with a scalar-valuedslot, Luke will prompt for adjectives that refer to high (and low) values ofthat slot, as well as verbs that denote value changes.andP;  Furthermore, for manynouns (e.g., &quot;lung&quot;), there are adjectives (e.g., &quot;pulmonary&quot;) that can referto any aspect of the entities denoted.andP;  Luke prompts for such objectives.andP;  ASthese related words are acquired, they are stored in the knowledge base andcompiled into the runtime natural language system, giving the system broadercoverage and more robustness.andM;What follows is an example of lexical acquisition in Tell.andP;  The entry pointis the user-level command:andM;Associated Noun (a word) &quot;city&quot; (a unit) UrbAreaandM;In Figure 11, Luke has guessed correctly the plural form of &quot;city,&quot; as wellas its syntatic subcategory (count noun) and semantics.andP;  It presents itsguesses in such a way that a knowledge engineer can verify them.andP;  After doingso, the user clicks on &quot;Done.&quot;andM;Luke now prompts for related words and phrases of &quot;city:&quot;andM;At the bottom of the window in Figure 12, the user has indicated theexistence of the word &quot;metropolitan.&quot;andP;  The user can also tell the systemabout any nouns and prepositions that can be used to refer to slots ofUrbArea.andP;  Luke makes this easy by providing sample sentences in which thesolicited words might occur.andP;  It is able to construct the example sentencesince it knows, from looking at the KB itself, what kinds of objects mustfill the slots.andP;  It can thus pick representative objects and use the wordsthat are associated with them.andP;  In this example, as shown near the middle ofthe window, the user has entered &quot;population&quot; as a noun that refers to thenuminhabitants slot that an instance of UrbArea might have.andP;  Lukeautomatically associates the noun &quot;population&quot; with this slot; it then popsup an acquisition window for this new association, as shown in Figure 13.andM;Luke guessed that &quot;population&quot; is a count noun.andP;  Because it can also be usedas a mass noun, the user has indicated that.andP;  Luke has guessed that&quot;population&quot; is a relational noun because it has been associated with a slot(and a slot is actualy a relation).andP;  Luke has also provided appropriatesemantics, as shown in the figure.andP;  You can read that semantic description assaying that anything that is a population must appear as the value of thenumInhabitants slot of some GeopoliticalEntity.andP;  Semantic routines are nowable to analyze phrases like &quot;Glasgow's population.&quot;andP;  Again, the user justclicks on &quot;Done.&quot;andM;Since the numInhabitants slot takes on a scalar range of values, Luke promptsfor related words that can be used when the value of the slot falls intovarious subranges, shown in Figure 14.andM;The use has entered the adjective &quot;populous&quot; to describe instances of UrbAreawith high numInhabitants values.andP;  This brings up the window shown in Figure15.andM;Lukes guesses that the comparative form of &quot;populous&quot; is &quot;populouser,&quot;requiring the user the correct this guess by clicking the mouse on &quot;None.&quot;andO;This ends the chain of acquisitions.andP;  Lexical entries for &quot;city,&quot;&quot;metropolitan,&quot; &quot;population,&quot; and &quot;populous&quot; are now stored permanently inthe knowledge base.andP;  They have also been compiled incrementally into theruntime lexicon, so that phrases like &quot;metropolitan government&quot; and&quot;Glasgow's population&quot; can be parsed and generated immediately.andM;Our intention is to avoid long acquisition sequences by having usersassociate words with concepts at the same time the concepts are created, butif a body of language already exists, Luke can also help ensure that a robustlexicon is produced to match it.andM;Now we can look briefly at what happened behind the scenes.andP;  In Figure 11,the user associated the word &quot;city&quot; with the unit UrbArea.andP;  Immediately afterthis, Luke created a simple instance of AiNoun in the KB.andP;  This instancelooked like:andM;The only interesting observation about Figure 16 is that, by virtue of beinga noun instance, the lexical unit has inherited several constraints on itsown slots.andP;  For example, the NounMassNoun-Constraint says that any noun whoseassociated concept is an instance of TangibleStuffType will be a mass noun.andO;Next Luke inserts the value UrbArea onto the unit's aiAssociatedConcept slot,resulting in:andM;We should note that two other slot/value pairs in Figure 17 have beenautomatically created by applying inference rules stored in the KB.andP;  When aslot value appears in italics, it means that the value is the result of someinference mechanism, in this case the forward-chaining rules located on theconstraints slot.andP;  (These rules are implemented via CYC's slotValueSubsumesinference scheme, which permits logical expressions involving set membership,negation, disjunction, etc.)andP;  These rules guess that the subcategory of thenoun is count noun because UrbArea is a class that does not fall witin theTangible-StuffType hierarchy.andM;Finally, Luke inserts the string &quot;city&quot; onto the unit's aiSpelling slot,resulting in Figure 18:andM;Luke's inference rules use the word's spelling (&quot;city&quot;) to deduce theprobable plural form (&quot;cities&quot;).andP;  The unit is now a full-fledged lexicalentry, ready to be edited and compiled.andM;Applications of KBNLandM;Because the total problem of natural language processing is so hard (in fact,some people say it is impossible), the ideal application of KBNL are &quot;failsoft,&quot; by which we mean that, to be useful, it is not necessary to get everything right.andP;  Instead, in fail-soft applications, partial answers are useful.andO;And further, these applications have the property that, as the KBNL systemitself and the KB on which it relies get better, the performance of theapplication programs also improves [47].andP;  some applications, such as airtraffic control, are not fail soft.andP;  But some are.andP;  Three that we arepursuing arer navigation and querying in a large KB, text retrieval, andtranslation.andM;Navigation and QueryingandM;in a Large KBandM;The most obvious application of KBNL is to provide a solution to the problemof navigation through the CYC KB itself.andP;  Figure 19 shows an example of whatcan happen when a user tries to display just a part of a complex knowledgebase.andP;  Figuring out how to move through such a space is nearly impossible.andO;To find a desired object in such a system, it is necessary to know enoughabout the structure of the knowledge base to be able to define a path to thedesired object.andP;  It is difficult enough, however, for people to remember eventhe structures they themselves have defined, without attempting to know whatother knowledge-base builders have done.andP;  Even simple things, like whether aclass is named Cars or Autos can get in the way of a successful retrieval.andM;In contrast, KBNL supports a function called Show, which allows users todescribe what they want in English.andP;  Lucy can be used to construct a CycLinterpretation of the description, which is then used to retrieve theconcept.andP;  If we choose to, we can close the loop by hooking up Koko to builda question-answering system: the user asks CYC a question in English, andgets an answer in English.andP;  In essence, KBNL provides a declarative solutionto the navigation problem (in contrast to the procedural one mentionedabove).andP;  This application is fail soft because, to be useful, the system doesnot need to construct a totally accurate and complete interpretation.andP;  If issucceeds in getting the user into the right &quot; neighborhood&quot; in the knowledgebase, then current navigational techniques can be used effectively to findexactly the concept that is desired.andM;The same techniques we use for navigation allow us to use English to askexplicit question of the KB and to get back responses.andP;  We are also beginningto investigate a less fail soft application, namely the use of naturallanguage to update the KB.andP;  This is a more dangerous undertaking because ofthe possibility of self-perpetuating errors; a misinterpretation couldcorrupt the KB, which would then support only incorrect interpretations.andP;  Weexpect natural language to be useful only in well-developed areas of the KBwhere our informatio is sufficient to make it likely that the interpretationis correct and &quot;safe.&quot;andM;Text RetrievalandM;Another application under active development is Scan, a full-text retrievalsystem.andP;  Current commercial text retrieval systems use Boolean combinationsof words and phrases to search for and display documents.andP;  This is limitingon various counts.andP;  Users are forced to understand the usually complex queryformalism; very possibly there are retrieval requests we would like to makethat are difficult within the confines of a primarily Boolean query language;and there is no way to ask for information based on related concepts withoutspecifying them exactly.andP;  More problematic is that spelling out theseconcepts exactly in queries can result in the retrieval of completelyirrelevant information, precisely because these systemss use no understandingof the query in their search.andP;  These problems severely constrain theeffectiveness of conventional retrieval systems [7].andP;  Neither newer, AI-basedapproaches (e.g., [20] and [13] nor those based on sophisticated statisticaltechniques (E.G., [50] and [22] offer solutions to all of these problems.andM;The use of state-of-the-art natural language processing techniques, coupledwith traditional retrieval mechanisms, promises improvements in theeffectiveness of information retrieval systems, without requiring solutionsto all the outstanding problems of artificial intelligence.andP;  Scan useLucy forsentence-level understanding, supplementeing, but not completely replacing,standard key-word techniques.andP;  Lucy can perform two different operations thatare important to Scan.andP;  The first, which has already been implemented,extends the idea of query expansion as described in [21] and [56].andP;  Lucyunderstands the user's query and maps it into a CycL expression that can betransformed and expanded to include related concepts that have not beenexplicitly asked for.andP;  The full reasoning power of the KB is now available tobe used for this purpose.andP;  The resulting augmented query (still in CycL) isthen given to Koko, which generates a new set of Boolean queries, which itpasses off to standard retrieval engine.andP;  The second operation, which we willpursue later, is to understand at least fragments of the stored textsthemselves, thus enabling Scan to use genuine conceptual matching of queriesto texts.andP;  Like the navigation application mentioned above, text retrieval isa fail soft application that offers an opportunity for incrementalimprovements; increases in the amount of knowledge in the KB or in thesophistication of Lucy's analyses will lead to proportional improvements inScan.andM;Machine TranslationandM;Although we have spoken only of English so far, there is no impediment toporting KBNL to another language.andP;  To do this, we need to a) write asyntactic grammar of the language in our framework, b) create a version ofLuke to acquire words in that language, c) use the new version of Luke tobuild up a lexicon, and d) create an appropriate set of semantic anddiscourse rules for the language.andP;  Varying amounts of the knowledge in theEnglish system could be reused for the new language, as to En glish.andP;  Forexample, Japanese has nouns, adjectives, and verbs, all denoting the samesorts of objects that they do in English.andP;  We could therefore retain the coreof Luke, but we would have to remove the heuristics that try to distinguishcount from mass nouns, since Japanese does not make that distinction.andM;Once we have ported KBNL to another language, we can run Lucy in one languageand Koko in another, thus providing the core of a knowlege-based machinetranslation system (in the tradition of [16, 44].andP;  This type of a system willbe more powerful than conventional, syntactically-based systems (such as [3,6, 33, 52]) because it has an explicit representation of the meaning of theutterance available to help it disambiguate and choose the most appropriateexpression in the target language.andP;  Syntactic translation systems can beefficient, precisely because they do not do any deep processing, but hey hita wall when they encounter a sentence that has multiple translations into thetarget language, depending on intended meaning.andP;  High quality translationwill  not be possible, particularly among dissimilar languages, without thesort of explicit model of the meaning.andP;  High quality translation a systemlike KBNL can provide.andP;  As a result, we are just beginning the development ofa prototype Spanish/English translation system based on KBNL.andM;Summury andandM;ConclusionandM;In this article, we have described the philosophy on which the KBNL system isbased, and we have summarized the current implelentation of that philosophy.andO;The key tenets of this philosophy are:andM;* Natural language processing systems can be effective components of avariety of kinds of application systems, including full text retrieval,machine translation, and many kinds of knowledge-based systems.andM;* There are natural relationships between facts about the world and thelinguistic structures that are used to describe those facts.andP;  (For example,the flesh of animals can be eaten, and you can use the same word for theliving animal as for the flesh.)andP;  Further, these relationships can be encodedin a computationally effective way.andM;* There are also exceptions to these relationships (e.g., cow/beef).andP;  Infact, there are rules that apply at all levels of specificity.andP;  To build areal system requires capturing rules at all levels.andM;* There relationships are separate from facts about the physical world perse, although they may refer to domain knowledge in various ways.andP;  (Forexamples, the &quot;oil&quot; example referred to facts about plants as opposed topeople.)andP;  Taking advantage of this separation leads to knowledge bases thatcan support NL systems but, at the same time,  can support both multiplelanguage in NL systems as well as other reasoning tasks.andM;* By articulating these relationships and by separating linguistic knowledge,we can build domain knowledge, we can build a m ore robust and portablenatural language processing system.andP;  (For example, we can guess what&quot;hazelnut oil&quot; means without ever being told.)andM;* The same linguistic knowledge can be used for both language understandingand language generations.andM;(1) &quot;Metonymy&quot; is a term from classical rhetoric.andP;  It describes thenonliteral use of a word to refer to a related concept.andM;(2) Although &quot;well-formed&quot; usually means syntactically correct, here it means&quot;accounted for by the grammar.&quot;andP;  We can write grammar rules to account bothfor syntactically correct expressions and for any syntactically &quot;incorrect&quot;ones that occur often in the corpora being considered.andM;(3) A Syntactic Logical Form is similar in spirit to an' f-structure inLexical Functional Grammar [9], but the actual analyses often differ.andM;(4) Actually, &quot;four waters&quot; is correct in some circumstances.andP;  It means fourstandard units (in this case glasses) or four kinds of water.andP;  Luke createssuch count readings from mass nouns.andM;ReferencesandM;[1] Ades, A. and Steedman, M. On the Order of Words.andP;  Linquistics andPhilosophy 4 (1982), 517-558.andM;[2] Appelt, D.E.andP;  Bidirectional Grammars and the Design of Natural LanguageGeneration Systems.andP;  TINLAP-3 Position Papers, 1987.andM;[3] Arnold, D.J., Krauwer, S., Rosner, M., des Tombe, L. and Varile, G.B.andO;The [is greater than]C,A[is less than], T Framework in Eurotra: ATheoretically Committed Notation for MT.andP;  In Proceedings of the 11thInternational Conference on Computational Linguistics (COLING), 1986.andM;[4] Ballard, B. and Stumberger, D.E.andP;  Semantic Acquisition in TELI: ATransportable, User-Customized Natural Language Processor.andP;  In Proceedings ofthe 24th Annual Meeting of the Association for Computational Linguistics.andO;(1986).andM;[5] Barnett, J. and Mani, I.andP;  Using Bidirectional Semantic Rules forGeneration.andP;  In Proceedings of the fifth International Workshop on  NaturalLanguage Generation (1990).andM;[6] Bennett, W.S.andP;  and slocum, J. The LRC Machine Translation system.andO;Comput.andP;  Linguist.andP;  11, 1985, 2-3andM;[7] Blair, D. and Maron, M. An Evaluation of Retrieval Effectiveness for aFull-Text Document-Retrieval system.andP;  Commun.andP;  ACM 28, 3 (1985), 289-299.andM;[8] Boguraev, B. and Briscoe, T. Large Lexicons for Natural LanguageProcessing: Utilising the Grammar Coding system of LDOCE.andP;  Comput.andP;  Linguist.andO;13 (1987), 3-4.andM;[9] Bresnan, J. ED.andP;  The Mental Representation of Grammatical Relations.andP;  MITPress, Cambridge, Mass., 1982.andM;[10] Byrd, R., Calzolar, N., Chdorow, M., Klavans, J., Neff M. and Rizk, O.andO;Tools and Methods for Computational Lexicology.andP;  Comput.andP;  Linguist.andP;  13(1987), 3-4.andM;[11] Calder, J., Reape, M. and Zeevat, H. An Algorithm for Generation inUnification Categorial Grammar.andP;  In Proceedings of the 4th Conference of theEuropean Chapter of the ACL, (1989), pp.andP;  233-240.andM;[12] Carlson, G. Generic Terms and Generic Sentences.andP;  J. of Philos.andP;  Logic11, 2 (1982), 145-181.andM;[13] Cohen, P. and Kjeldsen, R. Information Retrieval by ConstrainedSpreading Activation in Semantic Networks.andP;  Inf.andP;  Proces.andP;  Manage.andP;  23(1987), 225-268.andM;[14] Cohen, R.M., McCandless, T.P.andP;  and rich, E.A.andP;  A Problem solvingApproach to Human-Computer Interface Managment.andP;  In Proceeding of Workshop onblackboard Systems, IJCAI-89, (1987).andM;[15] Dahl, D., Palmer, M. and Passonneau, R. Nominalizations in Pundit.andP;  InProceedings of the 25th Annual Meeting of the ACL, (1987).andM;[16] Dorr, bonnie J. UNITRAN: An Interlingual Approach to MachineTranslation.andP;  In Proceedings of the Sixth National conference of AAAI,(1987).andM;[17]  Earley, J. An Efficient Context-Free Parsing Algorithm.andP;  Commun.andP;  ACM13 (1970), 94-102.andM;[18] Englemore, R. and Morgan, T. Ed.andP;  Blackboard Systems.andP;  Addision-Wesley,Reading, Mass., 1988.andM;[19] Finnin, T. The Semantic Interpretation of Compound Nominals.andP;  Ph.D.andO;Univ.andP;  of Illinois, 1980.andM;[20] Fox, E.A.andP;  Development of the CODER System: a Testbed for ArtificialIntelligence Methods in Information Retrieval.andP;  Inf.andP;  Process.andP;  Manage.andP;  23,4 (1987), 341-366.andM;[21] Fox, E.A.andP;  Improved Retrieval Using a Relational Thesaurus forAuthomatic Expansion of Boolean Logic Queries.andP;  In Relational Models of theLexicon, M. Evens, Ed., Cambridge University Press, 1988, pp.andP;  199-210.andM;[22] Furnas, G., Deerwester, S., Dumais, S., Landauer, T. Harshman, R.,andO;Streeter, L. and Lochbaum, K. Information Retrieval using a Singular ValueDecomposition Model of Latent Semantic Structure.andP;  In Proceedings ofSIGIR-88, (1988), pp.andP;  465-480.andM;[23] Grice, H.P.andP;  Logic and Conversation.andP;  In Studies in Syntax, Vol.andP;  (III),P. cole and J.L.andP;  Morgan, Ed., Seminar Press, New York, 1975.andM;[24] Grosz, B.J., Appelt, D.E., Martin P.A.andP;  and Pereira, F.C.N.andP;  TEAM: AnExperiment in the Design of Transportable Natural-Language Interfaces.andP;  ArtifIntell 32, 2, (May 1987), 173-243.andM;[25] Heim, I. The Semantics of Definite and Indefinite Noun Phrases.andP;  Ph.D.andO;thesis, Univ.andP;  of Massachusetts, 1982.andM;[26] Hendrix, G.G., Sacerdoti, E.D., Sagalowicz, D., and Slocum, J.andO;Developing a Natural Language Interface to Complex Data.andP;  ACM Trans.andO;Database Syst.andP;  3 (1978), 105-147.andM;[27] Hirschberg, J. Toward a Redefinition of Yes/No Questions.andP;  In Proceedingof the 10th International Conference on Computational Linguistics (COLING)(1984).andM;[28] Hirst, G. Semantic Interpretation and the Resolution of Ambiguity.andO;Cambridge University Press, 1987.andM;[29] Hobbs, J. Overview of the TACITUS Project.andP;  Comput.andP;  Linguist.andP;  12, 3(1986).andM;[30] Hobbs, J. and Stickel, M. Interpretation as Abduction.andP;  In Proceeding ofthe 26th Annual Meeting of the Association for Computational Linguistics,(1988), pp.andP;  95-103.andM;[31] Hollan, J.D., Miller, J., Rich, E. and wilner, W. Knowledge Bases andTools for Buidling Integrated Multimedia Intelligent Interfaces.andP;  InProceedings of the Workshop on Architectures for Intelligent Interfaces:Elements and Prototypes, (1988).andM;[32] Hovy, E.H.andP;  Generating Language with a Phrasal Lexicon.andP;  In NaturalLanguage Generation Systems, D.D.andP;  McDonald andamp; L. Bolc, Eds., Springer-Verlag,New York, 1988.andM;[33] IBM Japan.andP;  The Art of Machine Translation: English to Japanese.andP;  IMBRes.andP;  Mag.andP;  27, 4 (1989).andM;[34] Jacobs, P.S.andP;  Achieving Bidirectionality.andP;  In Proceedings of the 12thInternationa Conference on Comput.andP;  Linguist.andP;  (COLING) (1988), pp.andP;  267-269.andM;[35] Kamp, H. and Rohrer, C. Tense in Texts.andP;  In Meaning, Use and theInterpretation of Language, Bauerle, Schwartz and von Stechow, Eds., deGruyter, Berlin, 1983.andM;[36] Kamp, H. A. Theory of Truth and Semantic Representation.andP;  In Truth,Interpretation, and Information, Groenendijk, J., Janssen, T. and Stokhof,M., Eds.andP;  Foris, dordrecht, Germany, 1984.andM;[37] Kaplan, R. A. General Syntactic Processor.andP;  In Natural LanguageProcessing.andP;  R. Rustin, Ed., New York: Algorithmics Press, 1973.andM;[38] Kolodner, J.L.andP;  Retrieval and Organizational Strategies in ConceptualMemory: A Computer Model.andP;  Lawrence Erlbaum Associates, Hillsdale, N.J.,andO;1984.andM;[39] Lenat, D.B.andP;  and Guha, R.V.andP;  Building Large Knowledge Based Systems.andO;Addison Wesley, Reading, Mass., 1990.andM;[40] Lenat, D.B., Guha R.V., Pittman, K., Pratt, D., and shepherd, M. Cyc:Toward Programs with Common Sense.andP;  Commun.andP;  ACM (August 1990).andM;[41] Mann, W.C.andP;  and Matthiessen, C. Nigel: A Systemic Grammar For TextGeneration.andP;  USC Information Sciences Institute Tech.andP;  Rep.andP;  RR-83-105, 1983.andM;[42] McDonald, D.D.andP;  and Meteer, M.W.andP;  From Water to Wine: Generating NaturalLanguage Text from Today's Application Programs.andP;  In Proceedings of theSecond ACL Conference on Applied Natural Language Processing, (1988).andM;[43] McKeown, K.R.andP;  Generating Natural Language Text in Response to QuestionsAbout Database Structure.andP;  PH.S.andP;  Thesis Univ.andP;  of Pennsylvania, 1982.andM;[44] Nirenburg, S. Knowledge-Based Machine Translation.andP;  Mach.andP;  Trans.andP;  4, 1(1989), 5-20.andM;[45] Oehrle, R., Bach, E. and Wheeler, D. (Ed.) Categorical Grammars andNatural Language Structures.andP;  Reidel, Dordrecht, holland, 1987.andM;[46] Pereira, F. and Warren D. Parsing as Deduction.andP;  In Proceeding of the21st Annual Meeting of the Association for Computational Linguistics, (1983),pp.andP;  95-103.andM;[47] Rich, E.A.andP;  The Gradual Expansion of Artificial Intelligence.andP;  IEEEcomput.andP;  (May 1984), 4-12.andM;[48] Rich, E.A.andP;  and Luper-Foy, S. An Architecture for Anaphora Resolution.andO;In Proceedings of the Second ACL conference on applied Natural LanguageProcessing, (1988).andM;[49] Sager, N., Friedman, C. and Lyman, M.S.andP;  Medical Language Processing.andO;Addison-Wesley, Reading, Mass., 1987.andM;[50] Salton, G., Fox, E.A.andP;  and wu, H. Extended Boolean InformationRetrieval.andP;  Commun.andP;  ACM 26, 12 (1983), 1022-0136.andM;[51] Schank, R.C.andP;  and Riesbeck, C.K.andP;  Ed.andP;  Inside Computer Understanding:Five Programs Plus Miniatures.andP;  Erlbaum, Hillsdale, N.J., 1981.andM;[52] Scott, B.E.andP;  the Logos system.andP;  MT SUMMIT II Conference--Munich, 1989.andM;[53] Shieber, S.M.andP;  A Uniform Architecture for Parsing and generation.andP;  InProceedings of the 12th International conference on Computational Linguistics(COLING), (1988), pp.andP;  614-619.andM;[54] Shieber, S.M., van Noord, G., Moore, R.C.andP;  and Pereira, F.C.N.andP;  ASemantic-Head-Driven Generation Algorithm for Unification-Based Formalisms.andO;In Proceeding of the 27th Annual Meeting of the Association for ComputationalLinguistics, (1989).andM;[55] Simmons, R.F.andP;  Semantic Networks: Their Computation and Use forUnderstanding English Sentences.andP;  In Computer Models of Thought and Language,R.C.andP;  Schank and K.M.andP;  Colby, Eds., Freeman, San Francisco, 1973.andM;[56] Spark-Jones, K. and Tait, J.I.andP;  Automatic Search Term VariantGeneration.andP;  J. Docu.andP;  40, 1 (1984), 50-66.andM;[57] Stallard, D. The Logical Analysis of Lexical Ambiguity.andP;  In Proceeddingsof the 25th Annual Meeting of the Association for Computational Linguistics,(1987).andM;[58] Thomason, R. Ed.andP;  Formal Philosophy: Selected Papers of RichardMontague.andP;  Yale University Press, 1974.andM;[59] Webber, B.L.andP;  (Ed.) Compu.andP;  Linguis.: Special Issue on Tense and Aspect,1988.andM;[60] Winograd, T. and Flores, F. Understanding Computers and Cognition: A newFoundation for Design.andP;  Ablex, Norwood, N.J., 1986.andM;[61] Wittenburg, K. A Parser for Portable NL Interfaces UsingGraph-Unification-Based Grammars.andP;  In Proceedings of the Fifth NationalConference of AAAI 86, (1986).andM;[62] Woods, W.A.andP;  Transition Network Grammars for Natural Language Analysis.andO;Comm of ACM 13 (1970), 591-606.andM;[63] Woods, W.A.andP;  Cascaded ATN Grammars.andP;  Am.andP;  J. Compu.andP;  Linguis 6, 1(1980),1-12.andM;[64] Wroblewski, D.A.andP;  and Rich, E.A.andP;  Luke: An Experiment in the EarlyIntegration of Natural Language Processing.andP;  In Proceedings of the Second ACLconference on Applied Natural Language-Processing (1988).andM;JIM BARNETT leads the natural language processing project at MCC.andP;  hiscurrent research interest include the semantics and pragmatics of naturallanguage.andP;  Author's Present Address: MCC, 3500 W. Balcones Center DR.,andO;Austin, TX 78759.andM;KEVIN KNIGHT is a Ph.D.andP;  candidate in computer science at Carnegie MellonUniversity.andP;  He is also a regular consultant to the Artificial IntelligenceLaboratory at Carnegie Mellon University.andP;  Author's Present Address: Schoolof Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.andM;INDERJEET MANI is a member of the technical staff at MCC.andP;  His currentresearch interests and in natural generation.andP;  Author's present address: MCC,3500 W. Balcones Center Dr., Austin, TX 78759.andM;ELAINE RICH is director of the Artificial Intelligence Laboratory at MCC.andO;Her current research interest include natural language processing and hybridreasoning systems.andP;  Author's present address: MCC, 3500 W. Balcones CenterDr., Austin, TX 78759.andO;</TEXT></DOC>