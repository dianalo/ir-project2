<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-175-652  </DOCNO><DOCID>07 175 652.andM;</DOCID><JOURNAL>Communications of the ACM  April 1989 v32 n4 p444(15)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Linda in context. (artificial intelligence and languageprocessing) (technical)</TITLE><AUTHOR>carriero, Nicholas; Gelernter, David.andM;</AUTHOR><SUMMARY>Linda is an artificial intelligence language processing programthat is simpler, more powerful, and more elegant than any of thealternatives.andP;  The program is compared to three leading models ofparallel programming.andP;  Linda is a more practical and elegantalternative to logic programming: the tools in concurrent logiclanguages are inflexible and policy-laden.andP;  Pure functionallanguages fail to provide the needed expressivity.andP;  Linda isprepared to absorb computational wisdom from any source: it willco-exist with almost any model of computing and should flourishwith a variety of machines and language models.andM;</SUMMARY><DESCRIPT>Topic:     Artificial intelligenceLanguage Processing.andO;Feature:   illustrationprogram.andO;Caption:   Server-clients in Parlog86 and in C-Linda. (program)DNA sequence comparison using recursion equations and C-Linda.andO;(program)Parallel sequence database search in C-Linda. (program)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>LINDA IN CONTEXT Linda consists of a few simple operations that embody thetuple space model of parallel programming.andP;  Adding these tuple-spaceoperations to a base language yields a parallel programming dialect.andP;  Use ofLinda is growing slowly but steadily as we distribute versions of the systemfor beta test, and as several manufacturers work on their ownimplementations.andP;  But Linda remains stubbornly outside the researchmainstream.andP;  In the research community, discussion of parallel programmingmodels focuses mainly on message-passing, concurrent object orientedprogramming, concurrent logic languages, and functional programming systems.andO;Linda bears little resemblance to any of these.andP;  How can a system thatdiffers sharply from all currently fashionable approaches score any kind ofsuccess, even a tentative and preliminary one (which is all we claim forLinda)?andP;  We'll argue that, on balance, Linda is a simpler, more powerful andmore elegant model than any of these four alternatives.andP;  The claim doesn'thold for all problems and includes, obviously, a subjective element; but wehope to convince readers that any consideration of how to write parallelprograms is incomplete without considering this heterodox model also.andP;  Ourmethod is a series of head-to-head comparisons.andP;  First we introduce the Lindamodel and compare it to the concurrent object model (or actually theconcurrent object non-model, as we explain).andP;  Then we compare sampleprograms, contrasting Linda with examples coded in Parlog86, a concurrentlogic language prominently featured in a recent issue of Communications ofthe ACM, and with a pure functional language.andM;THE MODEL AND THE SYSTEM'S STATUSandM;To write parallel programs, programmers must be able to create and coordinatemultiple execution threads.andP;  Linda is a model of process creation andcoordination that is orthogonal to the base language in which it's embedded.andO;The Linda model doesn't care how the multiple execution threads in a Lindaprogram compute what they compute; it deals only with how these executionthreads (which it sees as so many black boxes) are created, and how they canbe organized into a coherent program.andM;The model is based on generative communication.andP;  If two processes need tocommunicate, they don't exchange messages or share a variable; instead, thedata producing process generates a new data object (called a tuple) and setsit adrift in a region called tuple space.andP;  The receiver process may nowaccess the tuple.andP;  Creating new processes is handled in the same way: aprocess that needs to create a second, concurrently executing processgenerates a &quot;live tuple&quot; and sets it adrift in tuple space.andP;  The live tuplecarries out some specified computation on its own, independent of the processthat generated it, and then turns into an ordinary, data object tuple.andM;This simple scheme has a series of important implications.andP;  First,communication and process creation are two facets of the same operation.andP;  Tocreate processes, we generate live tuples, which turn into data objecttuples; to communicate, we generate data object tuples directly.andP;  The resultin both cases is the same: a new object is added to tuple space, where anyinterested party may access it.andP;  Second, data is exchanged in the form ofpersistent objects, not transient messages.andP;  The receiver may remove thetuple generated by the data-producing process, but may also leave it in tuplespace, where many other processes may read it too.andP;  We can organizecollections of tuples into distributed data structures, structures that areaccessible to many processes simultaneously.andP;  The unification of process anddata creation means that we can organize collections of processes in the sameway, into &quot;live data structures&quot;: each process in the live data structurecomputes and then turns into one element of the passive data structure thatis returned as the result.andP;  Roughly speaking, the fact that we can writeprograms that use messages, distributed data structures or live datastructures means that this simple model encompasses coarse, medium andfine-grained approaches to parallelism.andM;In fact, the implications of generative communication extend beyond parallelprogramming.andP;  If we think of communication as a transaction between separateprograms or processes--a transaction that can't rely on standardintra-program mechanisms like shared variables and procedure calls--thencommunication is a fundamental problem for which few unified models exist.andO;Two processes in a parallel program may communicate; a program in onelanguage may use communication mechanisms to deal with a program in anotherlanguage; a user program may communicate with the operating system; or aprogram may need to communicate with some future version of itself, bywriting a file.andP;  Most systems classify these events under separate andunrelated mechanisms, but the tuple space model covers them all.andP;  Tuple spaceexists outside of (encompasses) the black-box programs that do the computing.andO;Accordingly it can be used to pass information between black boxes indifferent languages (its own simple semantics is independent of any onelanguage's), between user and system black boxes and between past black boxesand future ones (its lifetime isn't bounded by the programs it encompasses).andO;Although implemented Linda systems focus only on communication withinparallel programs, the generative communication model encompasses all ofthese forms of communication, as will future Linda systems.andM;Linda, to summarize, deals only with process creation and coordination.andP;  If amodular language is embedded in the Linda model, Linda becomes part of a&quot;modular&quot; approach to parallelism; likewise with an object oriented language,or a logic language, or an interpreted language or anything else.andP;  By notmeddling in computation issues, Linda wins the freedom to coexist peacefullywith any number of base languages and computing models, and to support clean,simple, and powerful operations within its own domain.andM;Current StatusandM;Linda has been implemented and tested in a broad range of environments.andP;  Inour group we have added the Linda operations to C and Fortran, and othergroups are working on C++, PostScript and Scheme as base languages; describesa Modula-2 Linda, an object-oriented derivative.andP;  Linda runs on a wide rangeof parallel machines: shared-memory multi-computers like the Encore Multimax,Sequent Balance and Symmetry and Alliant FX/8; distributed-memorymulticomputers like the Intel iPSC/2 and the S/Net; and Vax/VMS-based localarea nets.andP;  Linda applications have shown good speedup through 64 nodes onour iPSC/2, which is the biggest machine we have used to date; given thenature of the Linda implementation, these applications are very likely tocontinue to speed up on larger machines as well.andP;  (Linda implementations formachines like the Intel hypercube are based on distributed hash tables thatscale with machine size.)andP;  Ports to other machines are in progress.andP;  A Lindasimulator runs on Sun workstations, and a &quot;Linda Machine&quot; that supports tuplespace operations in hardware is under construction.andP;  The system has been usedfor a variety of parallel programming experiments, including matrixmultiplication and LU decomposition, DNA sequence comparison and paralleldatabase search, traveling salesman, expert systems, charged particletransport, finite element equation solvers, linear programming and others.andO;Particularly interesting from a numerical algorithms point of view is newwork on a sparse system solver in Linda; ongoing work includes a neural netsimulator.andM;Several independent commercial implementations now underway will expand therange of supported architectures.andP;  In some contexts the system is now seeingwhat is essentially production as opposed to experimental use; Linda is usedat Yale in the generation of ray-tracing displays of fractal images (by KenMusgrave of Benoit Mandelbrot's group), and at Sandia National Labs inexecuting a parameter sensitivity analysis for rocket plume simulations overa local area network.andM;State of the Art?andM;Turn now to current research in computer science: how should parallelprograms be written?andP;  At present, three approaches make up the most widelydiscussed group: concurrent object oriented programming, concurrent logicprogramming, and functional programming.andP;  Linda falls into none of thesecategories.andP;  The tuple space model is strictly sui generis.andP;  Where does itfit, though, and how does it relate to the Big Three?andM;In the following, we will discuss each of the three named approaches in turnand compare each to Linda.andP;  In their enthusiasm for object oriented, logicbased, or functional programming, proponents have argued that these modelsare ideal not only for computing but for parallel computing.andP;  We will arguethat in fact, the strengths of all three models are irrelevant toparallelism, and generally unhelpful in dealing with process creation andcoordination in parallel programs.andM;The following three sections vary in structure.andP;  In the first, we describeLinda's tuple space model; we contrast it with concurrent object orientedprogramming, and deal briefly with message passing and the Actors model aswell.andP;  In the next section, we discuss parallel logic programming.andP;  We focushere on the programming examples presented in a recent article on Parlog86,contrasting them with Linda's solutions to the same problems.andP;  In the lastsection we discuss pure functional programming, again comparing a Linda and apurely functional solution to the same problem, and discussing theimplications of the comparison.andM;A Note on Parallelizing CompilersandM;It used to be widely argued that, with sufficiently good parallelizingcompilers, programmers could code in standard sequential languages and leavethe compiler to worry about parallelism.andP;  Smart compilers, it was thought,could produce good parallel programs automatically, and this was importantfor two reasons: not only would it allow old programs to be recycled forparallelism without any programmer intervention, but it would spareprogrammers the unknown and presumably gruesome horrors of writing anddebugging explicitly parallel programs.andM;Much progress has indeed been made on parallelizing compilers.andP;  But twothings have become clear as well.andP;  First, compilers can't find parallelismthat isn't there; the algorithms that work best on parallel machines areoften new algorithms, different from the ones relied on in sequentialprograms.andP;  Second, evidence to date (some of which we will discuss here)suggests that writing explicitly parallel programs isn't so terriblydifficult after all.andP;  A growing (if admittedly still small) number ofapplications programmers write parallel programs on a regular basis.andO;Granted, parallelizing compilers are the only way to parallelize existingprograms without rewriting them.andP;  Nonetheless, few researchers today seem todisagree with the contention that programmers confonting a new,compute-intensive application should have a well-designed and efficientparallel language at hand.andP;  The question is, which parallel language?andM;TUPLE SPACES AND CONCURRENT OBJECTSandM;We concentrate here on two topics: explaining the tuple space model, andcontrasting it with concurrent object oriented systems.andP;  We briefly discussActor systems as well.andM;As a point of departure for explaining tuple space, consider the best-knownof parallel programming techniques, message passing.andP;  To build a parallelprogram using message passing, we create many processes, all executingconcurrently and asynchronously; to communicate--to disperse input data,collect final results and exchange intermediate results--processes sendmessages to each other.andP;  This model or something similar underlies parallelprogramming in systems as diverse as the native communication system suppliedby Intel with the iPSC hypercube, the CSP language fragment and the Occamlanguage that is based on it, and the Mach distributed operating system.andM;Message passing systems rely on three basic operations: create-process,send-message, and receive-message.andP;  If a sending process S has a message fora receiver R, S uses the send-message operation and R uses receive-message.andO;In the simplest case, a single process was created automatically when westarted the program; this initial process used create-process to create S andR.andM;Linda on the other hand provides four basic operations, eval and out tocreate new data objects, in and rd to remove and to read them respectively.andO;If a Linda sending process S has data for a receiver R, it uses out togenerate a new tuple.andP;  Then R uses in to remove the tuple.andP;  A tuple, unlike amessage, is a data object in its own right.andP;  In message-sending systems, amessage must be directed to some receiver explicitly, and only that receivercan read it (unless the programmer uses some special broadcast messageoperation, which some message systems supply and some don't).andP;  Using Linda,any number of processes can read a message (i.e., a tuple); the senderneedn't know or care how many processes or which ones will read the message.andO;Processes use the rd operation to read a tuple without removing it.andM;To create processes S and R, the initial process used eval.andP;  Processes canalso communicate by using eval if they choose; when a sending process S hasdata for a receiver R, it can use eval to generate a new process M.andP;  Mexecutes some assigned piece of code; when it terminates, it turns into atuple.andP;  Then R uses in to remove the tuple.andM;The fact that senders in Linda needn't know anything about receivers and viceversa is central to the language.andP;  It promotes what we call an uncoupledprogramming style.andP;  When a Linda process generates a new result that otherprocesses will need, it simply dumps the new data into tuple space.andP;  A Lindaprocess that needs data looks for it in tuple space.andP;  In message passingsystems, on the other hand, a process can't disseminate new results withoutknowing precisely where to send them.andP;  While designing the data generatingprocess, the programmer must think simultaneously about the data consumingprocess or processes.andP;  In our experience, parallel programming needn't beterribly difficult, but this kind of &quot;thinking in simultaneities&quot; seemscalculated to make it difficult.andM;A tuple exists independently of the process that created it, and in fact manytuples may exist independently of many creators, and may collectively form adata structure in tuple space.andP;  It's convenient to build data structures outof tuples because tuples are referenced associatively, in many ways like thetuples in a relational database.andP;  A tuple is a series of typed fields, forexample (&quot;a string&quot;, 15.01, 17, &quot;another string&quot;), or (0, 1).andP;  Executing theout statements out(&quot;a string&quot;, 15.01, 17, &quot;another string&quot;) and out(0, 1)causes these tuples to be generated and added to tuple space.andP;  (outstatements don't block: the process executing out continues immediately.)andP;  Anin or rd statement specifies a template for matching: any values included inthe in or rd must be matched identically; formal parameters must be matchedby values of the same type.andP;  (It's also possible for formals to appear intuples, in which case a matching in or rd must have a type consonant value inthe corresponding position.andP;  Values are not communicated from the instatement &quot;backward&quot; to the tuple, however.andP;  Formals in tuples serve only aswildcards, expanding the range of possible matches.) Consider the statementin(&quot;a string&quot;, ? f, ? i, &quot;another string&quot;)andM;Executing this statement causes a search of tuple space for tuples of fourelements, first element &quot;a string&quot; and last element &quot;another string&quot;, middletwo elements of the same types as variables f and i, respectively.andP;  When amatching tuple is found it is removed, the value of its second field isassigned to f and its third field to i.andP;  If there are no matching tuples whenin executes, the in statement blocks until a matching tuple appears.andP;  Ifthere are many, one is chosen nondeterministically.andP;  The read statement, forexample rd(&quot;a string&quot;, ? f, ? i, &quot;another string&quot;) works in the same way,except that the matched tuple is not removed.andP;  The values of its middle twofields are assigned to f and i as before, but the tuple remains in tuplespace.andM;It's now easy to see how to build data structures in tuple space.andP;  Considerone simple but important case: we can store an n-element vector V as n tuplesof the form (&quot;V&quot;, 1, FirstElt) (&quot;V&quot;, 2, SecondElt) ...andP;  (&quot;V&quot;, n, NthElt)andM;To read the jth element of the vector and assign it to x, processes userd(&quot;V&quot;, j, ? x); to change the ith element, in(&quot;V&quot;, i, ? OldVal); out(&quot;V&quot;, i,NewVal)andM;We discuss some more elaborate cases in the next section.andM;We can also use Linda to build fine grained live data structure programs.andP;  Alive data structure program takes its shape from the result it is intended toyield.andP;  If the result is a vector, the program is a vector of processes, andso on.andP;  Each process in the live data structure computes and then turns intoone element of the passive data structure yielded as result.andP;  Consider, forexample, a program that yields an n X n matrix whose jth counter-diagonaldepends (only) on the preceding counter-diagonal.andP;  The computation canproceed wavefront-wise: as soon as we know a counter-diagonal, we can computein parallel all elements of the next counter-diagonal.andP;  (We describe a realand slightly more complicated example later.)andP;  It is easy to express such aprogram in Linda: we use eval statements of the form eval(&quot;M&quot;, i, j,compute(i, j)) to create one process for each element of the result.andP;  Thefunction compute(i, j) uses rd to examine the values of the precedingcounter-diagonal--for example, rd(&quot;M&quot;, i - 1, j, ? value).andM;As soon as the processes along the kth counter-diagonal have completedcomputing, they turn into passive tuples and become visible to the processesalong the (k + 1)st counter-diagonal.andP;  Thus the computation proceeds instages, as active processes turn into passive tuples along a wave-front fromupper-left to lower-right.andM;Such fine-grained programs are generally impractical given our currentimplementations.andP;  The point is, however, that Linda can express them cleanly,and implementations on future generation machines can be expected to supportthem efficiently.andM;Linda versus Concurrent Objects and ActorsandM;Although there is no single canonical concurrent object model, the Emeraldsystem appears to be a state-of-the-art example.andP;  A concurrent Emeraldprogram consists of a collection of objects, some passive and some withembedded active processes.andP;  New objects are generated by executing an objectconstructor.andP;  An object defines methods which may be invoked procedure-styleby other objects.andP;  An object that is accessible to many others must protectits internal data structures against unsafe concurrent access.andP;  To do so, itembeds its methods in a monitor (the structure proposed by Hoare in [25]).andO;By definition, only one process may be active inside a given monitor'sprotected routines at any one time.andP;  A second process that attempts to enterthe monitor is automatically blocked on a monitor queue until the monitorbecomes free.andP;  When processes enter a monitor prematurely--they needed accessto some resource, an empty buffer, say, that is at present unavailable--theyblock themselves explicitly on a condition queue.andP;  A process that freesresources associated with condition queue j signals j, allowing some processblocked on j to continue.andP;  (In addition to these mechanisms, Emerald includessome sophisticated operations designed to relocate objects in a network.andO;They are not germane here, though; we are interested in how Emerald can beused to express parallel programs.)andM;Monitors were designed originally for concurrent processes within a singleaddress space.andP;  In Emerald, a process may access a monitor via a remoteprocedure call across address spaces and hence, potentially, across machineboundaries.andP;  The effect is not to alter in any way the expressivity ofmonitors, but rather to make them available in a new environment.andM;The strengths and weaknesses of monitors were explored extensively inlanguages like Concurrent Pascal, Modula and Mesa.andP;  We prefer Linda tomonitors for several reasons.andP;  We can divide them roughly into matters ofsimplicity and of flexibility.andM;In the monitor approach, process creation, inter-process communication andsynchronization fall into three separate categories: process forking orobject creation, the monitor's shared state variables, and the conditionqueue and signal mechanism, respectively.andP;  Linda unifies the three within thecontext of tuple-space operations.andP;  It follows that support for monitor-basedconcurrency requires significant changes to a base language.andP;  Linda requiresthat we add new operations, and so do monitors (monitors require wait andsignal operations on condition queues).andP;  But monitors also require a newenvelope structure within the language, namely the monitor construct itself.andO;Linda is simpler, and its greater flexibility is related to its simplicity.andM;Communication in monitor-based systems is based on procedure call (or methodinvocation, in the terminology of object oriented monitor systems).andP;  The sameholds for distributed operating systems that are based on remote procedurecall [e.g., 9].andP;  Procedure call is inherently a synchronous operation: aprocedure call blocks until a result is returned.andP;  This characteristic isproblematic in the programming environment for which Linda is designed.andP;  Inour experience, processes in a parallel program usually don't care whathappens to their data, and when they don't, it is more efficient andconceptually more apt to use an asynchronous operation like Linda's out thana synchronous procedure call.andP;  Using out, a process can dump a result intotuple space and continue immediately; invoking a monitor procedure requiresblocking until the monitor is free, and then until the requested transactionhas been performed and a result is available.andP;  It's always possible to patchup a procedure-call system so that that it supports some form of asynchronouscommunication also.andP;  But in our experience, asynchronous communication is farmore common than synchronous.andP;  This is hardly surprising; in parallelprogramming, the goal is to compute a result fast.andP;  In order to workefficiently, the processes that make up such a program will avoid blocking asfar as possible.andP;  It's trivial, in Linda, to implement a synchronousremote-procedure-call-like operation in terms of out and in.andP;  There is noreason we know of, however, to base an entire parallel language on this oneeasily programmed but not crucially important special case.andM;Monitors are not as flexible as tuples for building distributed datastructures.andP;  All elements of the distributed structure must be stored withina single monitor, which restricts access to one process at a time, or theymust be stored in separate monitors, which requires that a new monitor andmonitor queue be created for each new element or group of elements.andP;  Monitorsdon't support the transparent transition from process to data object thatlive data structures require.andM;Monitors do have points in their favor: most important, they allow alloperations on a particular shared structure to be encapsulated in a simpleand language-enforced way.andP;  But Linda strikes us as more powerful, simpler,and easier to integrate unobtrusively into a base language.andM;Where Are the &quot;Objects&quot;?andM;What does the preceding discussion have to do with object orientedprogramming?andP;  Curiously enough, nothing.andP;  Object oriented programming, whichoriginated with Simula 67 and reached a cathartic climax in Smalltalk, is amodel in which objects are instantiated from templates or classes.andP;  Classesmay be created according to an interesting scheme that allows a new class toinherit and augment (or override) the properties of preexisting classes.andP;  Themodel is powerful and attractive, but irrelevant to parallelism.andP;  In theobject model, each object is accessible only by way of the methods itdefines; but what if two methods are invoked by separate processessimultaneously?andP;  We can insist that each object be an active process, that itaccept messages (of the form &quot;invoke method M&quot;) one at a time, and returnreply messages to each invoker in turn.andP;  So far as parallelism goes, this ismerely a message passing model.andP;  We can implement monitors, as described, butthen we're left with a monitor-based model of parallelism.andP;  Alternatively, wemight combine an object oriented language with Linda.andP;  Objects are generatedusing out (for passive objects) or eval (active ones).andP;  Passive objects areimmutable; processes get their own copies by using rd, then invoke themethods directly.andP;  All communication with active objects goes through tuplespace.andP;  Again, objects per se don't figure in this model's approach toparallelism.andP;  We believe in sum that the current widespread enthusiasm for&quot;concurrent object oriented programming&quot; is to some extent misinformed.andP;  Ifyou are designing an object oriented parallel language, you face exactly thesame design choices you would face if you were designing any other kind ofparallel language.andP;  &quot;Objects&quot; in and of themselves do not help.andM;&quot;Actors&quot; is related in some ways to concurrent object systems.andP;  The proposalis fairly old, but it continues to attract interest (see particularly Agha).andO;In an Actors system, processes (called actors) communicate by sending eachother messages (called tasks).andP;  A process may respond to a message bygenerating one or more new processes.andP;  This kind of system is readilyexpressed in Linda: processes can exchange messages in the form of tuples,and generate new processes by using eval.andP;  Linda, of course, can also be usedfor distributed and active data structure programming; Actors, which isbasically a message passing model, seems less suitable for these purposes.andO;It's interesting that the Actors model seems to cover fewer programmingpatterns than Linda does, but in a more complicated way.andP;  Thus a process anda message in the Actors model are separate structures; in Linda, a processbecomes a tuple.andP;  An Actors message contains three separate parts, a tag, atarget and a communication; a tuple is merely a set of fields, any or all ofwhich may be tag (i.e., message id) or target (address) or communication(data).andP;  Actors doesn't support the associative addressing of tasks, nor doesit allow messages to be sent to not-yet-created processes.andP;  (In Linda, atuple may be read or removed by a process created long after the tuple itselfwas outed.)andP;  The Actors model is, on the other hand, supported by a fairlyelaborate formal framework.andP;  Linda is not.andP;  But Linda exists in practicalimplementations on a range of commercial systems, and we don't believe thisholds (at least at present) for the Actors model.andM;CONCURRENT LOGIC PROGRAMMINGandM;Concurrent logic programming is a booming field.andP;  The Japanese Institute forNew Generation Computer Technology (ICOT) continues work on their ParallelInference Machine, which is intended for concurrent logic programming.andP;  Arecent Communications of the ACM article discussed the new concurrent logiclanguage Parlog86, and collected papers on one of the first of theselanguages, Concurrent Prolog, have just been these languages, ConcurrentProlog, have just been published (complete in 1,178 pages) by MIT Press.andO;Several years ago we published a brief discussion contrasting ConcurrentProlog with Linda, but new developments make a comparison between Linda andsome of the Parlog86 solutions recently featured in Communications seemdesirable.andP;  The bulk of Ringwood's presentation of Parlog86-style concurrencydeals with two examples, the client-server paradigm and the diningphilosophers problem.andP;  We will discuss each in turn.andM;Concurrent logic programming takes several forms, but in most cases the basicidea is as follows: we can specify many parallel activities by use of a&quot;parallel conjunction,&quot; which states that some result depends on a series ofsub-results, all of which may be pursued simultaneously.andP;  A collection of&quot;guarded clauses&quot; allows us to specify that only one of a series of parallelconjunctions be performed, but the selection criteria (the &quot;guards&quot;) can beevaluated simultaneously.andP;  Parallel execution threads communicate by means ofshared logical variables, which are initially unbound (&quot;uninstantiated&quot;) butcan't be referenced until one party to the communication binds them to avalue.andP;  Data streams are implemented by &quot;partially instantiated&quot; shared logicvariables.andP;  The data producer successively appends (in effect) new elementsto a list or stream of elements; the data consumer reads the stream.andP;  It alladds up to a parallel version of logic programming--to a parallel version (inother words) of what Ringwood and many others have called a very high levelapproach to programming.andP;  Very high level programs have mathematicalproperties that their lowlier brethren lack, but we focus here on a moreconcrete issue: presumably, very high level programs are more elegant andconcise than other kinds.andM;In the client-server paradigm, many client processes must communicate with asingle server process.andP;  The problem is usually associated with distributedoperating systems (the server might provide a file service, mail,name-location or other resource management function); in our experience, itis equally or more important in parallel programming.andM;Consider the Parlog86 solution (Figure 1).andP;  It creates, in effect, threeprocesses, one corresponding to each paragraph of code in the figure.andP;  The&quot;server&quot; process repeatedly reads a stream of requests.andP;  It accomplishes thisby binding the name Transaction to the first element and the nameMore-transactions to the rest of some input stream.andP;  It proceeds to respondto Transaction (that is, it services the stream's head request), and then itapplies the same procedure recursively to the remainder of the stream.andP;  Whenthe client process needs to communicate with the server, it adds to thestream a message of the form transaction (Reply).andP;  Reply is an&quot;uninstantiated variable&quot;: it is forwarded to the server as (in effect) aname without a binding.andP;  When service to the client is complete, the serverbinds the client's Reply variable to the string &quot;Roger Roger&quot; (the code forthis step isn't shown).andP;  By executing wait (&quot;Roger Roger&quot;), the client blocksuntil this binding is accomplished.andM;The complete system entails more than a server and client processes, though.andO;A Parlog86 stream may be appended to by a single process only, but ordinarilythere are many clients.andP;  The solution is for each client to append messagesto its own private stream, and for these multiple client streams to be mergedto form the single request stream that the server expects.andP;  Merging is thejob of the third process: it examines each client stream (the example shownassumes that there are exactly two, Stream1 and Stream2), and merges theminto a single Outstream, which the server scans in turn.andP;  (Invocations thatcause names to be bound correctly--e.g., that cause the merge process'sOutstream to be identified with the More-transactions stream read by theserver--are required as well, but the code is omitted.)andM;A C-Linda version is also shown in Figure 1.andP;  It assumes again that theserver will scan down a stream of requests.andP;  In Linda, the stream exists as aseries of numbered tuples, of the form (&quot;request&quot;, 1, FirstRequest)(&quot;request&quot;, 2, SecondRequest) ...andM;By successively incrementing the index field that forms part of theidentification template in the in statement, the server can in each of thesetuples in sequence.andP;  To respond to the jth request, it places its response ina tuple labeled j.andP;  Whichever process made the jth request will be waiting(again using in) for a tuple so labeled.andM;Client processes must first establish where the end of the stream is.andP;  To doso, they read and update a tuple labelled &quot;server index&quot;.andP;  If many clientsattempt to update the server index simultaneously, they succeed one-at-a-timeand the index is updated safely.andP;  Once a server has determined the currentend-of-stream index, it adds its request-tuple to the stream.andP;  It then blocksusing in until a response arrives.andM;How do the solutions compare?andP;  They are comparable in length (in both casesinitialization code is omitted, amounting to several lines in C-Linda andprobably about the same for Parlog86).andP;  We would argue that the C-Linda codeis easier to understand.andP;  This contention is subjective, but surely the Lindaversion is no harder to understand.andP;  The real difference involves theflexibility of the two versions.andP;  The Parlog86 version requires an extraprocess to perform merging; the Linda version does not.andP;  More important, thecode of Parlog86's merge process depends on how many and which streams arebeing merged.andP;  The merge process examines the head of each client streamexplicitly.andP;  Consider how the Parlog86 code would look if there were 20 or100 streams to be merged instead of two.andP;  The Linda code is insensitive tonumber of clients.andP;  Nothing in the code reflects this number, or needschanging when the number or identity of client processes change.andM;There is nothing unrealistic in imagining 20 or 100 input streams to someserver.andP;  In our experience, this kind of pattern occurs often inmaster-worker-style parallel applications.andP;  Tasks (work assignments) areperformed by some arbitrary number of identical worker processes in parallel.andO;A master process sets up the computation, then reels in and coordinates theresults.andP;  The workers are clients and the master is the server.andM;This master-worker version of the server-client paradigm raises aninteresting and more subtle problem.andP;  The order in which task results arereturned to the master may be irrelevant--so long as the master getseverything eventually, there's no need to deal with the results in FIFOorder.andP;  If ordering isn't needed, much better to leave it out: it complicatesthe code and may require nontrivial processing at runtime.andP;  But in Parlog86,client messages must be ordered: there is no way to build an unorderedstream.andP;  In Linda, clients can simply drop messages into tuple space usingout, omitting the index field.andP;  The server withdraws tuples in arbitraryorder using in.andM;The merging problem in Prolog-derived concurrent languages is well-known.andO;Some researchers argue that the solution is simply to add a new mergeprimitive.andP;  This solution points to a deeper problem, however.andP;  Theclient-server example is important because it brings into play one of Linda'smost important advantages, flexibility.andP;  One particular species of stream iscanonical in Parlog86.andP;  A Parlog86 stream has one writer, and perhaps aseries of readers.andP;  Linda has no canonical stream, but it's easy to build avariety of important types.andP;  The client-server example requires a one-readermany-writer stream.andP;  Other examples require one-writer, many-remover streams.andO;This type of stream's first element is removed repeatedly by any one of agroup of interested processes--a situation that arises where (for example) amaster process generates an ordered list of tasks, and each worker repeatedlyremoves and carries out the head task.andP;  This kind of stream is again simpleto build in Linda, but unidiomatic in Parlog86.andP;  (Note that this example'scrucial characteristic is that many processes jointly disassemble, not merelyread, the stream.)andM;We turn briefly to the main focus of the Parlog86 article, the diningphilosophers problem.andP;  A round table is set with some number of plates(traditionally five); there is a single chopstick between each two plates,and a bowl of rice in the center of the table.andP;  Philosophers think, thenenter the room, eat, leave the room and repeat the cycle.andP;  A philosophercan't eat without two chopsticks in hand; the two he needs are the ones tothe left and the right of the plate at which he is seated.andP;  If the table isfull and all philosophers simultaneously grab their left chopsticks, no rightchopsticks are available and deadlock ensues.andP;  To prevent deadlock, we allowonly four philosophers (or one less than the total number of plates) into theroom at any one time.andM;What is the point of solving such a silly problem?andP;  According to Ringwood,dining philosophers &quot;is a benchmark of the expressive power of new primitivesof concurrent programming and stands as a challenge to proposers of theselanguages&quot; [30, p. 11].andP;  While we can't agree that the problem is as centralas Ringwood makes it out to be, it has indeed been used as a benchmark ofexpressivity since Dijkstra first suggested it.andM;The Parlog86 solution (Figure 2) is too complicated to explain here, butgenerally speaking it uses a series of processes to implement each state thata philosopher can occupy--thinking, preparing to eat, and eating.andO;Philosophers circulate along data streams among these process-states, passingalong the way through intermediate processes that merge streams, matchavailable chopsticks to hungry philosophers and so on.andP;  In the Lindasolution, assume for concreteness that the number of place-settings and ofphilosophers is five.andP;  There are four &quot;room tickets,&quot; represented by fourtuples.andP;  A philosopher who is ready to enter the dining room uses in to graba ticket.andP;  (If there are no free tickets, he will block until some otherphilosopher leaves and releases his ticket.) Once inside, he uses in to grabhis left chopstick and then his right chopstick.andP;  Each chopstick is againrepresented by a separate tuple.andP;  When he's done eating, he replaces bothchopsticks and his room ticket.andM;(Careful readers will notice that, if the Linda kernel is &quot;unfair&quot;--if it canrepeatedly bypass one process blocked on in in favor of others--the Lindasolution allows indefinite overtaking or livelock.andP;  A slow philosopher couldremain blocked on an in (&quot;room ticket&quot;) statement while a speedy onerepeatedly outs a room ticket and then grabs it again, leaving the slowphilosopher still blocked.andP;  The Parlog86 solution suffers from a similarproblem.andP;  Both solutions should be read, then, under a fair implementationassumption.)andM;The Linda solution is, in a sense, nothing special.andP;  It uses in and out ascounting semaphore operations, and essentially the same solution is possiblein any system that supports distributed semaphores.andP;  But here again, webelieve that the contrast between Linda and Parlog86 makes an importantpoint.andP;  When a problem has a simple solution, a useful system will giveprogrammers access to the simple solution.andP;  Forcing complex solutions tosimple problems makes us suspect that a language has chosen the wrong&quot;abstraction level&quot; for its primitives, chosen operations with too manypolicy decisions built-in and too few left to the programmer.andP;  Such languagesare ostensibly &quot;higher level&quot; than ones with more flexible operations, butthis kind of high-levelness dissipates rapidly when programmers step outsidethe (often rather narrow) problem spectrum that the language designer had inmind.andP;  A Parlog86 proponent would almost certainly call Parlog86 a&quot;higher-level&quot; language than C-Linda; but we've shown that it is somewhateasier to solve the client-server problem in C-Linda than in Parlog86, andmuch easier to solve the dining philosophers problem.andP;  Each of the problemsinvolves significant issues in concurrent programming.andP;  Both were chosen asshowpieces for Parlog86 by a proponent of concurrent logic programming.andM;FUNCTIONAL PROGRAMMINGandM;We have discussed problems for which Linda has a shorter and neater solutionthan some alternative, but we turn now to a comparison that cuts the otherway.andP;  Figure 3 shows two versions of a program to compute the &quot;similarityvalue&quot; of two DNA sequences.andP;  One version is written using a pure functionallanguage (the language shown is Crystal); the other version uses C-Linda.andO;Although the two programs do not differ by much, the C-Linda version isslightly longer.andM;Unsurprisingly, there's more to this comparison than meets the eye.andO;Advocates of pure functional languages for parallel programming argue asfollows: Programmers shouldn't decide how to parallelize their algorithms.andO;Armed with a pure functional language, they should express their algorithmsas a set of recursion equations.andP;  Pure functional languages impose a decisivelimitation: they don't allow assignment statements, and hence no variable'sbinding can ever change.andP;  This restriction simplifies the task of finding analgorithm's implicit parallelism; programs expressed in functional languagesare therefore good candidates for automatic parallelization by smartcompilers or run-time systems.andP;  These arguments have been made for some timeby proponents of dataflow functional languages, and also (for example) byproponents of Crystal.andP;  The promised advantages of the approach aresubstantial.andP;  Source programs are portable and completely machineindependent.andP;  Programmers don't bother with parallelism; they producemathematical expressions instead.andP;  Mathematical expressions are moreconducive to formal verification than ordinary programming languages, theyare easier to debug, and the advantages of mathematical expressions in termsof elegance and conciseness are well known.andP;  Thus Turner writes that &quot;a basicdifficulty&quot; of programming languages that are not &quot;functional&quot; is that &quot;theyare very long winded, in terms of the amount one has to write to achieve agiven effect.andP;  .  .&quot;andM;We can put these arguments in perspective by examining a concrete case.andP;  InThe DNA-sequence comparison problem (Figure 3), we need to compute asimilarity value for two sequences; this value is designed to capture ageneticist's qualitative judgement.andP;  In the algorithm shown, we compute asimilarity matrix whose i, jth entry is the similarity between the first ielements of one sequence and the first j of the other.andP;  The matrix can becomputed in wavefront fashion: first the upper-left element, then the secondcounter-diagonal, then the third and so on.andM;This is an ideal problem for any functional language.andP;  It's simple andconvenient to express this algorithm as a series of equations giving eachmatrix element's dependence on previous elements.andP;  A smart compiler couldestablish that all elements of the similarity matrix can be computed inparallel, with the elements along the jth counter-diagonal blockingdataflow-style until all previous counter-diagonals have been computed.andM;The C-Linda version isn't a set of equations; it is a fine grained explicitlyparallel program.andP;  It builds a live data structure called H.andP;  The i, jthelement of H is a process that computes, then turns into, the correspondingelement of the similarity matrix.andM;Having the introduced the two versions, we can contrast them.andP;  The C-Lindaversion is longer, but the difference in compactness seems minor.andP;  One majorunderlying assumption of the functional-languages advocates--thatexplicitly-parallel programs are hard to design and to understand--gets nosupport from this comparison.andP;  The C-Linda version was easy to design.andP;  Somereaders will find the Crystal version easier to understand, but we doubtwhether too many will find a dramatic difference in comprehensibility.andP;  Thereal difference between the two lies elsewhere.andP;  The C-Linda version is aparallel program.andP;  The Crystal version is a specification that a compilermight turn into a parallel program.andP;  In fairness, then, the two versions areincomparable, one being a program and the other, a program specification.andO;The contention that parallel programs are necessarily clunkier, more complexand less elegant than functional language program specifications is too oftenaccepted without comment, and it should not be, but the real issue lieselsewhere.andP;  Are the operations that create and control parallelism best leftin the programmer's hands, or should they be turned over to the compiler?andM;Leaving them to the compiler is attractive in some ways.andP;  It releases theprogrammer from any responsibility for the granularity of his computation.andO;The intent of projects like Crystal is for the compiler to gather up thefragments of a parallel algorithm and bundle them together in any way thatseems appropriate--into a few big bundles if the target architecture includesa few powerful processors, into many small bundles if the target machineincludes many less-powerful processors.andP;  In principle, the kinds of analysisused in the Crystal compiler would probably work for a program like theC-Linda DNA comparison also, but we haven't investigated them and don't knowfor certain.andP;  It's also true that Crystal targets special purpose SIMDarchitectures like the Connection Machine, as well as asynchronous computers;Linda is designed for asynchronous machines only.andM;We aren't sold on the compiler alternative, though, for two reasons.andP;  Oneinvolves a matter of detail related to the point discussed earlier.andP;  Theother concerns underlying concept.andM;On the general purpose parallel machines of which we are aware, it is farmore efficient to create a smaller number of processes, and put each incharge of computing an entire sub-block of the comparison matrix, than to putone process in charge of each element.andP;  Even on machines that support cheapprocesses (lightweight tasking), creating, scheduling and passing data amonga very large number of processes can be a considerable expense.andP;  The smartLinda programmer therefore starts with the program shown in Figure 3, butimmediately restructures it for efficiency by using &quot;interpretiveabstraction&quot; -- he replaces the live data structure with a passive one, andraises the processes one level in the conceptual scheme: each process fillsin many elements, rather than becoming a single element.andP;  We discuss theperformance of this kind of solution to the problem in (it shows goodspeedup).andP;  Interpretive abstraction is a fairly simple programming technique,but it's only possible if the parallelism tools are under the programmer'scontrol.andP;  In a pure functional language, it appears to be impossible.andP;  Not toworry: the functional language compiler is designed to take care of thisproblem (to carry out the equivalent of &quot;interpretive abstraction&quot;)automatically.andP;  The detail that bothers us is that this capability has yet tobe demonstrated, and achieving it automatically and generally strikes us as adifficult program.andP;  Consider one aspect of the difficulty for the DNAprogram.andP;  A reasonable approach is to divide the matrix into horizontalbands; a band is filled in by a single process; in general, one process canfill in many bands.andP;  How many bands should there be?andP;  Fewer, deeper bandsmeans loss of efficiency during startup and completion, because the delay instarting work on the second band depends on the first band's depth.andP;  Agreater number of shallower bands increases inter-band communicationoverheads.andP;  Given the Linda program, it is easy to determine a good band sizeby running some experiments.andP;  The Crystal system (and all other approaches toautomatic parallelization) takes it upon itself to determine this number apriori and automatically.andM;These issues of granularity will become less important over time.andP;  Newermachines will be capable of supporting increasingly fine-grained programs(although we don't expect that the optimal granularity issue will everdisappear entirely).andP;  But suppose we had a machine on which the finest grainprograms run well.andP;  Is it then reasonable to surrender control overparallelism and luxuriate in the pure sunlight of recursion equations?andO;Absolutely not, because in too many important cases, recursion equations havenothing to do with the programs we need to write.andM;Again, the DNA program is a good starting point.andP;  The approach we'vediscussed so far is--in our experience--interesting but wrong.andP;  The goal ofgeneticists in practice is to compare a newly-discovered sequence against alarge database of existing sequences.andP;  The problem is compute-intensive andparallelism is called for, but in our experience it is usually more efficientto run many conventional, sequential searches in parallel than to parallelizeindividual searches.andP;  One efficient approach to the problem is easy toexpress in C-Linda (Figure 4).andP;  The basic method is to create many identicalsearcher-processes, each initialied with a copy of the target sequence; amaster process hands out sequences from the database; workers executecomparisons, and return the results to the master.andP;  There are twosub-problems: (1) Sequences in the database vary greatly in length, so weneed dynamic sequence-assignment; e.g., one worker may finish 10 shortcomparisons before another finishes one long one; and (2) The database is toobig to fit in core and must be played out gradually.andP;  Both problems aresolved easily using C-Linda.andP;  Each sequence in the database is dumped as atuple into tuple space; searcher processes repeatedly grab a tuple, executethe comparison and dump a tuple holding the result back into tuple space,whence the master retrieves it.andP;  The program is simple to write and tounderstand because of the underlying physical model: a tuple is aquasi-physical thing that can be created, added to a pile, removed from apile (Figure 5).andP;  Recursion equations are in no way helpful in understandingthis simple physical process, and functional languages are inappropriate forthis kind of programming.andM;In Figure 5, a tuple space visualization tool, built by Paul Bercovitz of ourgroup, displays a series of windows onto tuple space, one for each &quot;tupleclass&quot; in the program.andP;  (The tuples within a given class have the same typesignature and have been determined by the link-time tuple analyzer tobehave--roughly speaking--in the same way.)andP;  Each tuple within a class at anygiven time is represented by a sphere in the appropriate window; users candisplay the contents of a tuple by mousing on its sphere.andP;  The figure showsfour snapshots of an executing DNA database-searcher that is similar to theprogram described in the text, executing with a master and five workerprocesses.andP;  The first snaphot (a), soon after start-up, shows the singletuple holding the target sequence (in the upper left-hand window), and thefirst of many tuples that will hold sequences from the database (in the widewindow second from the top).andP;  In snaphot (b), about 100 sequence tuples areawaiting available workers, and about 20 result tuples are waiting for themaster, who has been busy adding sequence tuples to tuple space.andP;  In snaphot(c), there are only a few sequence tuples remaining to be searched; in thelast snaphot (d), the job is done.andP;  No sequence or result tuples remain.andP;  Theworker processes were live tuples, created using eval; having completed, eachworker has become an ordinary data tuple, and the five tuples correspondingto the five finished workers appear in the upper right-hand window.andP;  Thisversion of the program uses a stream of sequence tuples rather than anunordered bag; the tuple that appears in the upper middle window is used tomaintain a pointer to the current head-of-stream.andM;The tuple space visualizer is a promising debugging tool, but it points to amore basic issue as well.andP;  A useful programming language makes it easy forthe programmer to embody a mental model directly in working code.andP;  A programthat embodies a mental model simply and directly is easier to writeinitially, and easier to understand once it exists as a working application.andO;The parallel database search described in the text is one example of aprogram that is at least as easy (we would argue far easier) to imagine interms of objects to be manipulated than in terms of equations to be solved.andO;In Linda, a mental model based on objects is easily embodied in working code.andO;Merely by displaying the state of tuple space as it evolves, the tuple spacevisualizer gives the user a strong &quot;feel&quot; for an application--how it worksand how it is progressing.andP;  (In this case, for example, how many sequenceshave been searched so far, how many results have been tabulated, whether themaster is dumping tuples into tuple space or waiting for tuples to be clearedout, and so on.)andP;  In short, if the definition of a &quot;higher level language&quot; isa language that is relatively closer to the programmer's way of thinking, itis impossible for us to accept the claim that functional languages arenecessarily higher-level than C-Linda.andM;Granted, this program is a single data point, but consider some others.andO;Neither of the examples discussed in the previous section can be solved in apure functional language.andP;  The solutions in both cases depend onnondeterminism (concretely, whichever process shows up first gets access tosome resource), and nondeterminism is impossible in functional languages.andP;  Aclass of programs we're particularly interested in now can be described asprocess lattices, i.e., heuristic programs structured as hierarchies ofconcurrent expert processes, with more general or abstract decisionprocedures appearing at higher levels.andP;  This is a promising structure for usein a problem area that (we believe) will become increasingly important: theconstruction of real time expert monitors.andP;  Nodes on the bottom rank of theprocess lattice are wired directly to external data sources, and incomingdata values filter upward through the lattice.andP;  This structure is conceivedin terms of communicating processes; recursion equations are the wrong model(or at the very least, a strongly counter-intuitive one).andP;  Whiteside andLeichter have shown that Linda programs can be made to perform well on acollection of heterogeneous machines on a local area network; if we break anapplication into modules, and run a compute-intensive back-end on acollection of parallel machines and the display-manager front-end on aworkstation, the programmer needs a way to specify inter-module (as well asintra-module) communication, and Linda gives him a way; here, once again,recursion equations would be out of place.andM;CONCLUSIONSandM;We set out to compare Linda to three leading models of parallel programming.andO;We can now summarize the arguments and draw some conclusions.andM;We have no intention of closing the book on monitors, and we're sure thatmonitor research and programming will continue for quite awhile.andP;  On adifferent note, we don't doubt that object oriented programming is a powerfuland attractive model, and we look forward to investigating a combination ofthe object model with Linda.andP;  Concurrent object oriented programming on theother hand is a phrase with a nice ring, but (in and of itself) little or nomeaning.andP;  It strikes us as more of a marketing than a technical term, usedmainly to freshen up respectable but slightly shelf-worn ideas like messagepassing and monitors.andP;  Discussion of parallelism models would be clearer andbetter-defined, we think, if this term disappeared.andM;Logic programming is obviously a powerful and attractive computing model aswell.andP;  Notwithstanding, we continue to believe that the tools in concurrentlogic languages are too policy-laden and inflexible to serve as a good basisfor most parallel programs.andP;  Applications certainly do exist that lookbeautiful in concurrent logic languages, and we tend to accept the claim thatvirtually any kind of parallel program structure can be squeezed into thisbox somehow or other (although the spectacle may not be pretty).andP;  But webelieve that, on balance, Linda is a more practical and a more elegantalternative.andM;Pure functional languages are a trickier issue.andP;  One point is immediate:these languages can't be the whole story, because in too many cases they failto provide the expressivity we need.andP;  Many (if not most) of the programs wedeal with are conceived in terms of objects to be manipulated, not equationsto be solved.andP;  But restricting our attention to those problems that do haveelegant functional solutions, like the DNA sequence-to-sequencecomparison--shouldn't we be tempted to recode these programs in functionallanguages, flip the autopilot switch, lean back, have a beer, and leave theparallelizing to them?andP;  In fairness we might be tempted, under the rightcircumstances.andP;  We'd stand to gain remarkably little in terms of concisenessor elegance over C-Linda, and nothing in terms of machine independence, sofar as asynchronous machines are concerned.andP;  (Again, Linda doesn't help inprogramming SIMD machines.andP;  We think it will be an excellent language formassively-parallel, fine-grained asynchronous machines, though.)andP;  It seems tous that, speaking in terms of the machines we deal with, the only good reasonto &quot;flip autopilot&quot; is performance.andP;  If these systems can be shown to performbetter than Linda and a competent programmer, we'll use them (or at any rate,we will gratefully appropriate their compiler technology for our Lindacompilers).andP;  Constructing a high-performance autopilot will be difficult; forour part, we'd find the search more compelling if we didn't have good,demonstrated ways to deal with these programming problems right now.andP;  But anydifficult research problem stands to yield unanticipated benefits if solvedsuccessfully, and we look forward to following these efforts.andM;On a positive note, Linda is prepared to absorb computational wisdom from anysource that can provide some.andP;  Our own preference is to program in C or Lisp,and our numerical analyst colleagues are reasonably happy with Fortran; aswe've emphasized, though, Linda can co-exist with almost any model ofcomputing.andP;  We'd like to see the Linda model flourishing in a wide variety ofmachine environments and language models.andP;  We're actively working toward thisend, and believe we're making progress.andM;We don't want to leave the impression, though, that Linda is a colorless,neutral system.andP;  The Linda model--a swarm of active tuples surrounded by acloud of passive ones--is suggestive in itself.andP;  One program now in design isa real-time &quot;tracking simulation&quot; of a subset of patients in a hospital;patients (passive tuples) are monitored and updated by a succession ofmanager and scheduler processes (active tuples).andP;  The goal is to monitoranomalies and rationalize the scheduling of tests and treatments bymaintaining a software microcosm of the external world.andP;  Another programinvolves a  biologically-accurate simulation of neuron networks; each neuronis represented by a passive tuple; there is a collection of identicalsimulation engines (active tuples), and each engine repeatedly grabs aneuron, executes one step of the simulation and blows it out the back into arecirculating pipeline.andP;  The more simulation engines, the faster the programruns.andM;The Linda operating system environment we're now building accommodatesmultiple first class tuple spaces.andP;  A tuple space is created with certainattributes--for example, some tuple spaces are persistent, and persistenttuple spaces constitute the file system.andP;  Whole tuple spaces can be treatedas single objects: they can be suspended, archived, reactivated orsnapshotted en masse.andP;  As always, a tuple space may contain active as well aspassive tuples, which suggests that an ordinary, passive file may alsocontain active processes if we choose to toss some in.andP;  Flocks of passivetuples might be stored alongside active shepherd processes--a file of mailmessages, for example, might contain a sorting-and-scanning daemon to add newmessages to the file and keep things in order.andP;  We might approach any ofthese three possibilities in contexts other than Linda, but Linda suggestedthem.andP;  Progress on these (and other less esoteric) projects continues.andM;Acknowledgments.andP;  This work is supported by National Science Foundation SBIRgrant ISI-8704025, by National Science Foundation CCR-8601920 andCCR-8657615, and by ONR N00014-86-K-0310.andP;  The authors thank the Yale Lindagroup, and particularly its senior members, Jerrold Leichter, RobertBjornson, and Venkatesh Krishnaswamy, for their vital contributions to thisresearch.andM;CR Categories and Subject Descriptors: D.1.3 [Programming Techniques]:Concurrent Programming; D.3.2 [Programming Languages]: LanguageClassifications--parallel languages; D.3.3: Language ConstructsandM;General Terms: LanguagesandM;Additional Key Words and Phrases: Concurrent object-oriented programming,monitors, concurrent logic programming, functional programming, Linda,parallelism</TEXT></DOC>