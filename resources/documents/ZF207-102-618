<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-102-618  </DOCNO><DOCID>07 102 618.andM;</DOCID><JOURNAL>Communications of the ACM  March 1989 v32 n3 p347(12)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>System structure and software maintenance performance. (ResearchContributions: management of computing)</TITLE><AUTHOR>Gibson, Virginia R.; Senn, James A.andM;</AUTHOR><SUMMARY>Research indicates that improvements in software systems may leadto a decrease in maintenance time and a decrease in the frequencyof ripple effect errors.andP;  The research investigates therelationship between system structure and maintainability.andP;  Anold, poorly structured system is improved in two sequentialstages.andP;  Experienced programmers perform a portfolio ofmaintenance tasks on the system at each of the three stages.andP;  Theresearch determines that system improvements may provide benefitseven without a complete redesign.andP;  The research indicatesprogrammers do not discern the difference in complexity, althoughthe difference is measurable.andP;  Six metrics presented in two setsmay be useful project management tools.andP;  These metrics measureboth the improvements in the system and programmer maintenanceperformance.andP;  This research, conducted under controlledconditions, provides the basis for further research in livesettings and research into programmer perceptions of complexity.andM;</SUMMARY><DESCRIPT>Topic:     Software ComplexitySoftware maintenanceProgrammersLifetimeMISAnalysisProject ManagementStudySystem Design.andO;Feature:   illustrationtable.andO;Caption:   Performance across systems and tasks; Error frequencies acrosssystems. (table)Perceived system complexity. (table)Overview of complexity metrics. (table)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>System Structure and Software Maintenance PerformanceandM;1.andP;  INTRODUCTIONandM;The software maintenance burden to industries is well documented.andP;  Studiesindicate that in business mainframe environments:andM;* Programmers spend more than half of their time on maintenance.andM;* Software maintenance consumes as much as two-thirds of the total life-cycleresources.andM;* Software applications may cost as much as 200 percent more to maintain thanto develop [31, 36].andM;Since software is subject to maintenance throughout its lifetime, while thedevelopment process takes place over a relatively short period, some of thesefindings are not surprising.andP;  Regardless of the exact figures involved, onepoint is clear: Maintenance and development compete for the scarce resourcesof money and programmer time.andP;  Any improvements in software maintenancemanagement may free resources for development activities.andM;This article presents the results of an experimental study in whichprofessional programmers performed various maintenance tasks on alternateforms of a computer program.andP;  The experiment was designed to allowpreliminary investigation of the relationship between system structure(complexity) and maintainability.andP;  The study was conducted to examine howstructural differences exhibit themselves in a system, how/whether thesedifferences influence performance, whether differences are discernible tomaintenance programmers, and how differences are reflected in existingobjective complexity metrics.andM;2.andP;  BLACKGROUND AND JUSTIFICATION FORandM;RESEARCHandM;Three general tasks are involved in modifying a system: understanding theexisting system, implementing the change, and validating the modification.andO;In fact, maintenance programmers spend most of their time trying tounderstand existing systems.andP;  According to Parikh:andM;The world of software maintenance is a world in which programmers spend halfof all their time.andP;  In this world, the main programming objective is toenhance an existing system to perform new functions or handle new data.andP;  Themajor professional challenge is to understand and conquer the complexity ofthe system as it is.andP;  [36, p. 3]andM;The issue of software complexity pervades the issue of maintainability.andO;Complexity causes two general problems in maintenance.andP;  The more complex asystem is, the more difficult it is to understand, and therefore to maintain.andO;The second problem is more insidious.andP;  Empirical research shows that complexprograms require more corrective maintenance throughout their lives.andP;  Complexprograms contain more errors, and errors are more difficult to uncover duringtesting.andP;  Consequently, they remain undetected until late in the life cycle.andO;The problem is circular--maintenance is difficult because of complexity, andbecause of complexity more maintenance is required.andM;Because these effects of complexity are known to exist, it is important topinpoint sources of complexity in programs and to assess their effects onmaintenance productivity.andP;  Most programming experts believe that structuredprogramming improves the ease with which systems can be understood andmodified [52], although empirical evidence to support this intuition isscant.andP;  In a rather harsh evaluation of the current situation, Vessey andWebber concluded:andM;Most of the developed theories have been normative, that is, they stated whatshould be done to improve the quality of programs and the programmingprocess.andP;  Unfortunately, these theories have rarely been subjected toempirical testing, and so their value remains unknown.andP;  They provide zealotswith opportunities to market a rash of seminars and courses and to flood theliterature with papers advocating the new technologies.andP;  When the theoriesare subjected to testing, what little evidence has been obtained sometimessuggests that the claimed benefits, in fact, may not exist.andM;The present study was designed to examine the effect of system structure onthe complexity and consequent maintainability of a COBOL system.andP;  Mostbusiness applications are written in COBOL system.andP;  Most businessapplications are written in COBOL, and most of the ongoing maintenance inindustry is performed on COBOL systems.andP;  Although maintainability as itapplies to COBOL systems thus has substantial practitioner interest, littleempirical research using COBOL systems is done, probably because computerscientists dislike the language.andM;Although guidelines have been developed for applying structured programmingconcepts to COBOL systems, many COBOL systems in use today are old and poorlystructured.andP;  A recent study by Peat Marwick of more than 60 million lines ofCOBOL software used by their clients found that almost 80 percent of the codewas unstructured.andP;  Thousands of old systems, developed with little thoughtgiven to maintenance, are still in use in industry.andP;  Few guidelines exist onhow, whether, or when to restructure these systems.andP;  As Elshoff described it:andM;...  most data processing installations still have large inventories ofprograms that are nearly impossible to read.andP;  Programs from this inventorymust regularly be modified or replaced.andM;It is worth it to restructure these systems?andP;  Strict adherence to structuredconstructs is awkward in COBOL.andP;  Ryge, for example, demonstrated thatstructuring in COBOL increased the number of nested IF statements andincreased the number of performed modules.andP;  Some of the gains achieved bystructuring, therefore, are offset by increases in other types of complexity.andM;It is not known how (or whether) these types of complexity influenceprogrammer performance in maintenance activities, since very little empiricalwork has been done on the effects of structuring on maintenance productivity.andO;The few studies that hve been done have not used COBOL systems, and thus havenot addressed the complexity tradeoffs noted earlier.andP;  This study providesevidence on what happens when an old, ill-structured system is &quot;improved,&quot;and addresses specifically how these improvements impact maintainability.andM;If structural differences do influence maintenance performance, it would bedesirable to be able to assess system structure at the time of purchase oracceptance of a system.andP;  In this way, managers could make decisions thatminimize lifetime costs of systems.andP;  Such structural assessments might takethe form of either comparative rankings, or relative adherence to a standard&quot;ideal.&quot;andP;  Both subjective and objective metrics have been proposed for thispurpose.andP;  Subjective metrics assume that differences in systems arediscernible to an evaluator; objective metrics assume that differences insystems can be computed using basic inherent system attributes.andP;  Each ofthese assumptions was investigated in this study.andM;The three primary objectives of this study were:andM;* To investigate how structural differences influence maintenanceperformance--more specifically, to determine whether system improvementsresult in measurable improvements in programmer performance.andM;* To determine whether system differences are discernible to programmerswhile they are performing maintenance.andM;* To determine whether system differences are measurable by existingautomatable, objective metrics.andM;3.andP;  RESEARCH METHODSandM;Maintainability is defined as the ease with which systems can be understoodand modified.andP;  The relative maintainability of a system is reflected in thetime required to implement changes to the system, the accuracy ofmodifications, programmers' confidence in the correctness of their work, andprogrammers' perceptions of the relative complexity of the system.andP;  Thesefour measures--time, accuracy, confidence, and perceptions--are the dependentvariables investigated in this study.andM;A 3 X 3 factorial design was used.andP;  Experienced professional programmersperformed three maintenance tasks and worked with threefunctionally-equivalent versions of a COBOL system.andM;3.1 ParticipantsandM;Table I summarizes the characteristics of participants, all of whom hadsubstantial data processing experience and used COBOL in their jobs.andO;Subjects averaged slightly more experience with maintenance (7 years) thanwith development (6 years).andP;  Experience with the COBOL language ranged from 1to 16 years, with a mean of 7 years.andM;3.2 The System Used in the StudyandM;The program used in this study, an ISAM file update system used by agovernment agency, is approximately 2,000 lines long.andP;  Because of theprogram's length and the number of modules, it was not possible for subjectsto grasp all details of the entire system, and ripple effects of changes werenot blatantly obvious.andP;  Thus, from a maintenance perspective, the systempresented realistic conditions.andM;The original system served as the &quot;unstructured system&quot; in the study, andalso served as the starting point for development of two new versions of thesystem.andP;  Suggestions for improving the system were solicited from students inan advanced programming class, all of whom had prior COBOL programmingexperience.andP;  Based on the most frequently recommended improvements, a secondversion of the system was developed in which long jumps in code (GO TOs) wereeliminated, and control structures were limited to sequence, selection, anditeration.andP;  The second version served as the starting point for developmentof a third system.andP;  Using the same procedures, the third system was developedby eliminating redundancies and writing procedures at the highest level ofabstraction possible.andM;Each system thus represents a perceived improvement over prior versions.andO;Using generally-accepted programming practices and structured programming asyardsticks, version 1 is the worst (most complex, least structured) system,and version 3 is the best (least complex, best structured) system.andO;Consistent with the results reported by Ryge, each subsequent &quot;improvement&quot;caused increases in nested IF-statements and in the number of performedmodules.andM;The differences in systems are summarized in Figure 1.andM;3.3 The Maintenance TasksandM;Each programmer performed three realistic (representative of actualuser-requested changes) and reasonable (feasible in less than one hour)perfective maintenance tasks.andM;The first task required the programmer to change the process of adding a newrecord to the file.andP;  When a new record is added, the system prompts the userto enter values for 27 variables.andP;  If the record is a duplicate, the user isnot informed until after values for all variables have been entered andverified by the system.andP;  Task 1 required the programmer to modify the systemto ascertain that the record is not a duplicate immediately on entry of therecord key.andM;The second task required the addition of a function to a part of the system.andO;The system allows the user to enter the command EXIT to escape any module.andO;The effect of the EXIT command is to cease current processing and move to ahigher level in the system hierarchy.andP;  This facility is built into everymodule in the update subprogram except one.andP;  Task 2 required programmers toimplement this facility where it was missing.andM;The third task required the programmer to modify the data contained in asubprogram.andP;  The system stores variable names and synonyms in a datadictionary subprogram.andP;  Variable types and sizes are not stored centrally inthe dictionary, but are dispersed throughout the system in Working-Storagesections.andP;  As the first step to centralize this information, task 3 requiredprogrammers to incorporate variable types and sizes into the Data Dictionarysubprogram.andM;Programmers implemented changes off-line.andP;  This was necessary to avoid aconfounding effect in the experiment.andP;  Since the subjects came from a varietyof backgrounds and installations, the use of a specific on-line environmentwould have provided a relative advantage to subjects familiar with theenvironment, and a substantial disadvantage to those accustomed to adifferent operating environment.andM;3.4 Experimental DesignandM;A partially confounded, 3 X 3 factorial design was used in the study.andO;Repealed measures were obtained on subjects.andP;  Each participant was exposed toeach of the three tasks and the three systems.andP;  The general experimentaldesign for one replication of the experiment is shown in Figure 2.andP;  Thesecond replication positioned blocks 4, 5, and 6 along the negative diagonalsin the design.andP;  Each block represents the specific task-system combinationsadministered to subjects within blocks.andP;  Each block contains six subjects.andO;Treatment sequence orders were counterbalanced to control for the learningeffect.andP;  Descriptions of this type of design are found in Winer [48, pp.andO;646-650] and Kirk [30, pp.andP;  336-339].andM;There are two important advantages to a design of this type.andP;  First, therepeated measures provide 108 observations from 36 subjects.andP;  And second,each subject serves as hiw own control, thus neutralizing the effect ofindividual differences in ability.andM;3.5 Variables InvestigatedandM;As noted earlier, three specific research issues were addressed by thisstudy.andP;  The first involves the effect of structural differences onperformance.andP;  Programmer performance was measured by the time taken toimplement a change, the accuracy of the modification, and confidence incorrectness of work.andM;Time was recorded to the nearest minute.andP;  After each task, programmersindicated their confidence in the correctness of their work using afive-point scale.andP;  The accuracy of modifications was assessed after theexperiment concluded using a database of test cases developed prior to thestudy.andM;The remaining two issues addressed by the study were whether systemdifferences are discernible and measurable.andP;  To determine whether systemdifferences are discernible to programmers, a post-experiment questionnairewas used.andP;  On completion of all tasks, and having worked with all threeversions of the system, programmers ranked the systems and tasks in terms ofcomplexity or difficulty.andP;  Programmer rankings were compared to actualcomplexity, i.e., relative adherence to good programming practices.andM;Finally, six existing automatable metrics were tested to see which, if any,provide rank orderings consistent with programmer performance, perceptions,and structured programming practices.andP;  Each of the metrics chosen for studywas selected either because it is relatively well-known, documented, andvalidated, or because it purports to measure a system attribute of particularinterest in this investigation.andM;3.6 Experimental ProceduresandM;At the beginning of the experimental session, the nature of the experimentand the tasks were explained.andP;  Subjects completed a two-page questionnaire ontheir programming experience and signed the informed consent agreementrequired by federal law.andP;  Each was given a source listing and documentation.andO;The experimenter then walked them through the documentation, which includedverbal descriptions of the system and components, a pictorial callinghierarchy, descriptions of variables in the ISAM database, detailed designcharts, paragraph linkages, and sample user-system interactions.andO;Participants were given time to familiarize themselves with the materials.andM;The three tasks were administered sequentially.andP;  Each participant was giventhe task and a copy of the source code listing.andP;  Modifications were madeoff-line, i.e., written to the source code listing.andP;  On completion of eachtask programmers indicated their confidence in the correctness of their workusing a five-point scale.andM;A post-experiment questionnaire was administered to assess perceptions of therelative complexity of the systems and tha tasks.andM;All participants were encouraged to take a short break between tasks.andP;  Thetotal time required, from start to finish, ranged from two to three hours.andM;4.andP;  analysis of resultsandM;Table II presents summary performance statistics across tasks and systems.andO;Time is the mean time (in minutes) taken to implement the change.andP;  Lines isthe average number of lines added to the program to implement the change.andO;Confid is programmers' average confidence in correctness, self-reported on ascale ranging from 1 (low) to 5 (high).andP;  % Errors is the percentage ofprogrammers introducing serious primary or ripple-effect errors with themodification.andM;Task 1 required changing the add module to determine whether a new record isa duplicate immediately on entry of the record key.andP;  The task wassurprisingly difficult.andP;  On average the modification took 35 minutes andeight new lines were added to the program.andP;  Because so few lines were added,it might be inferred that more time was spent thinking than writing.andP;  Despitethis, more than half (58 percent) of the implementations contained seriouserrors.andM;There is a notable discrepancy between correctness and confidence.andP;  Theseprogrammers were optimistic.andP;  Overall they believed the changes they madewould probably work.andP;  The discrepancy between correctness and confidence ismost pronounced in the original system (#1).andP;  Seventy-five percent ofsubjects made serious errors implementing task 1 t system #1; yet confidencelevels ranged from 4 to 5.andM;Task 1 appears to have been most easily implemented to themoderately-improved system (#2).andP;  In contrast to overall performance, lessthan half (42 percent) of the task 1-system #2 changes contained errors.andO;Furthermore, they were implemented relatively quickly, despite the greaternumber of lines added to system #2.andM;Task 2 required changing the high-level portion of the update system toaccept the command &quot;EXIT&quot; as an escape request.andP;  Subjects did well on task 2.andO;Overall, the modification took 26 minutes (range 9 to 59) and required addingan average of five source code lines.andP;  Subjects were justifiably confident inthe correctness of their work.andP;  Only 11 percent of the modificationscontained serious errors.andM;Task 2 appears to have been most easily implemented to the most improvedsystem (#3).andP;  This combination required the least time and inspired the mostconfidence.andP;  Only eight percent of the task 2 changes to system #3 containedserious errors.andM;Task 3 involved changing the data dictionary to return types(numeric/alphanumeric) and sizes (length of field) of variables to callingprograms.andP;  Task 3 took more time than the other two tasks.andP;  Overall, task 3was completed in 42 minutes and subjects added 36 new source code lines (notethat only about 9 lines of code represented unique logic--the remainder wasrepetitive).andP;  Thirty-nine percent of the changes contained errors.andO;Programmers were less confident in their work on task 3 than on the other twotasks.andM;Task 3 was implemented most quickly and most erroneously to system #3.andO;Because relatively few lines were required to implement the change to system#3, time may reflect the number of source code lines added, rather than easeof understanding.andM;4.1 Do Differences in System Structure Influence Performance?andM;The first question addressed by this study is how or whether differences insystem structure influence performance.andP;  The three performancemeasures--time, accuracy, and confidence--are discussed in the followingsections.andM;4.1.1 Time.andP;  The total average time required to perform the portfolio ofthree maintenance tasks was greatest for system #1, slightly less for system#2, and least for system #3.andP;  This is consistent with intuition; the&quot;improvements&quot; in the systems led to decreased maintenance time.andM;On specific individual maintenance tasks, however, results are less clear.andO;The structural improvements appear to help performance on some tasks andhinder performance on others.andP;  Since improvements in structure led toincreases in nested IFs and PERFORMed procedures, this suggests that specifictypes of complexity are important to specific tasks.andM;Improvements in system structure appear to enhance programmer performance,when performance is measured by time across a portfolio of tasks.andP;  Note thatin a &quot;live&quot; environment changes implemented inaccurately would requireadditional repair time.andP;  Our results do not reflect this additional time, noris it possible to measure it in a nonlongitudinal study.andP;  It is possible,however, to confine attention to changes implemented accurately.andP;  Table IIIshows the results of the analysis for modifications implemented correctly.andO;The conclusions are the same; in terms of the time required to performmaintenance tasks, the benefits of structural improvements are evident onlyacross a portfolio of tasks, not on individual tasks.andM;Figure 3 illustrates mean task completion times across systems.andP;  Therelationship between systems #1 and #3 is consistent across all tasks; inevery case, the original system (#1) took longer to modify than system #3.andO;System #2 introduces an interactive effect.andP;  The time required to modifysystem #2 appears to be task-dependent: task 1 was implemented most quicklyto system #2; the other two tasks were implemented most slowly to system #2.andM;Analysis of variance was used to test the significance of the interaction.andO;The results are presented in Table IV.andP;  The computed F statistic (2.71) fallsjust beyond the critical value (2.53) for alpha = 0.05 with degrees offreedom = 4,64.andP;  The marginally significant interaction indicates that thetime required to implement changes was dependent on both the system and thetask.andP;  The two factors did not operate independently in influencingmaintenance time, but exerted an interactive influence.andP;  There was no single&quot;most maintainable&quot; system.andP;  For task 1, the moderately structured improvedsystem was the best version; for tasks 2 and 3, the most improved system wasthe best.andP;  In no case was the original system the best system in terms of thetime taken to perform maintenance tasks.andM;The primary differences between the three systems were summarized in Figure1.andP;  System #2 was written to simplify system #1 by eliminating logn jumps andlimiting control to sequence, selection, and iteration.andP;  System #3 waswritten to eliminate redundancies in system #2.andP;  These perceived&quot;improvements&quot; were accomplished at the expense of nested IF and PERFORMedprocedures.andM;According to the interactive graph (Figure 3), system #3 was always betterthan system #1 in terms of maintenance time.andP;  Eliminating redundancies andwriting procedures at high levels did improve maintainability.andP;  Theintermediate step, system #2, did not always represent an improvement.andO;Apparently the increase in IF nesting was more important than the reductionin jumps for some tasks (2 and 3).andP;  For others (task 1), the opposite wastrue.andM;Not surprisingly certain system features enhance performance on somemaintenance tasks and diminish performance on others.andP;  These results provideevidence on the difficulty in predicting performance on specific individualmaintenance tasks.andP;  Unless tasks can somehow be categorized according tosystem features most important to the tasks, it will be extremely difficultto assess the costs of making specific individual changes to a system.andO;Across the portfolio of three tasks, performance consistent with theory wasobserved.andP;  Since the ultimate concern is minimizing lifetime costs,performance across a portfolio of maintenance activities may be moreimportant than performance on specific individual tasks.andM;In sum, these findings suggest that the time taken to perform maintenance isrelated to system structure and the task itself.andP;  The relationship was notobserved on specific individual maintenance tasks.andP;  Rather, across aportfolio of three tasks total average maintenance time decreased when thesystem was improved.andM;4.1.2 Accuracy.andP;  The effect of structure on accuracy was investigated byexamining error frequency patterns.andP;  Specific, errors introduced with changesare described first, followed by a discussion of results.andP;  Errors across thethree tasks differed due to the nature of the tasks.andP;  Modifications weretested and placed in one of the following categories: serious primary errors,serious ripple effect errors, minor errors, and no errors.andP;  These categorieswere defined to be mutually exclusive and exhaustive.andP;  Compile errors wereignored.andP;  Table V summarizes error frequencies across systems and tasks.andM;Serious primary errors caused the specified modification to fail.andP;  The changeitself did not work as requested.andP;  When the modification was implemented asrequested, but the changes introduced errors in unmodified system segments,it was called a serious ripple effect error.andP;  Minor errors were those whichhad logical explanation.andP;  Minor errors may have been due to the experimentalsetting.andP;  Finally, modifications with no errors were correct and introducedno ripple effect errors.andP;  To illustrate these error categories, the errorsmade on task 1 are described in Appendix A.andM;Across the portfolio of three tasks, according to primary error frequency,the original system (#1) is the best system, followed by systems #2 and #3.andO;Ripple effect error frequencies provide the opposite ranking.andP;  For theportfolio of tasks, improvements in system structure were associated withmore primary errors, but fewer ripple effect errors.andM;On a task-by-task basis, results are less clear.andP;  The patterns for task 1 areidentical to overall patterns; there was too little variability in task 2 formeaningful comparisons; and on task 3, although primary errors matchedoverall results, there were no serious ripple effect errors.andM;Figure 4 summarizes error frequencies across systems and tasks.andP;  The graph ofprimary errors exhibits remarkable consistency; the graph of ripple effecterrors does not.andP;  When primary errors are used as measures of systemmaintainability, straightforward conclusions may be drawn.andP;  For tasks 1 and3, the original system (#1) was most maintainable, followed by system #2 and#3.andP;  For task 2, all systems were equivalent.andP;  In terms of primary errorsthen, the original system (#1) is the best system and the most improvedsystem (#3) is the worst system.andM;Ripple effect errors exhibit an interactive effect.andP;  System #3 is always asgood as, or better than, the other two systems.andP;  However, the relativeperformance on system #1 and #2 is task-dependent.andM;Once again system structure appears to influence maintenance performance,when performance is measured by accuracy.andP;  This relationship is not observedon individual maintenance tasks, but rather, across a portfolio of tasks.andP;  Asthe system is improved fewer ripple effect errors are introduced withchanges, and errors tend to be confined to the area modified.andP;  This isencouraging evidence.andP;  Primary errors may be easier to detect than rippleeffect errors, since they occur where the system is modified, and themodified component is subjected to rigorous testing after modifications.andO;Ripple effect errors may occur in portions of the system physically distantfrom the modification.andP;  Since they are more difficult to anticipate, it isimportant to minimize these types of errors.andM;The results indicate that structural improvements are associated with adecreased frequency of ripple effect errors.andP;  Serious errors may still beintroduced with changeS, but they tend to occur in the modified area of theprogram or in the modification itself.andP;  One possible explanation for thereduction in ripple effect error frequency is that the structuralimprovements ease the procss of &quot;chunking&quot; or &quot;slicing&quot; for programmers [38].andO;Weiser [46] has shown that programmers use &quot;slices&quot; when debugging.andP;  Slicesare extracted statements related to one another through their actions onvariables.andP;  Programmers make sequential connections between physicallydistant, but logically related, statementS.andP;  It is possible that structuralimprovements aid programmers in extracting these slices, or chunks, of code.andM;4.1.o Confidence.andP;  On completion of each task, programmers reported theirconfidence in the correctness of their work on a five-point scale.andP;  Table VIpresents a summary of results.andP;  Most subjects were fairly confident in theaccuracy of their work, thus the confidence variable does not exhibit muchvariability.andP;  Programmers tended to be optimistic about their work even whenoptimism was unwarranted.andM;Where differences in confidence are observed, confidence appears to berelated not to system differences, but rather, to the tasks themselves.andO;Programmers were most confident about their work on task 2, regardless of thesystem to which it was implemented.andM;4.2 Are Structural Differences Discernible toandM;Programmers?andM;At the end of the experiment, each programmer ranked the systems and tasks interms of their difficulty/complexity.andP;  Tables VII and VIII summarizeperceptions across systems and tasks.andP;  Perceptions exhibit a relativelyuniform distribution.andM;There is no apparent consensus in the post-experiment rankings provided bysubjects.andP;  Ten individuals indicated that the original system was the mostcomplex system; 14 indicated that it was the least complex system.andP;  On themoderately improved system there was an equal split: eleven subjects calledsystem #2 most complex and eleven labeled it least complex.andP;  System #3 wasthe most complex system according to 15 subjects; according to 11, it was theleast complex version.andM;The apparent inconsistencies may be attributable to subjects' inability toseparate system complexity from task complexity.andP;  Tables VII and VIIIillustrate that the system on which task 2 was implemented was generallyperceived to be the least complex system.andP;  The task itself was perceived tobe relatively easy, regardless of the system to which it was implemented.andO;The inability of programmers' to separate task complexity from systemcomplexity may be a manifestation of the task X system interaction describedearlier.andM;In general, then, there was no homogeneity of perceptions of relative systmcomplexity.andP;  It appears that the complexity of the maintenance taskinfluenced (and biased) programmer perceptions of system complexity.andP;  Thestructural differences were not discernible (or important) to programmers.andM;These results offer important evidence on the efficacy of subjectively basedcomplexity metrics.andP;  Depending on the experience and prior exposure of theindividuals providing the subjective ratings, resulting measures may beinconsistent.andP;  The inspectors' perceptions may not conform to those of themaintenance programmers, which may affect the predictive ability ofsubjective metrics over the life of the system.andM;4.3 Are Structural Differences Measurable?andM;The final question addressed by this study is whether structural differencesare measurable.andP;  Objective metrics are based on inherent system attributesand can be computed objectively, unobtrusively, and algorithmically.andP;  Theresulting values are consistent and reproducible [5].andP;  Six automatableobjective complexity metrics were selected for study.andP;  Table IX summarizesthe metrics used in this study.andM;The relative rankings of system complexity provided by these metrics aresummarized in Tables X and XI.andP;  Structural improvements in the system arereflected by decreases in complexity measured by E, v(G), K, and Jumps.andO;While these metrics provide identical rank orderings, they reflect differentmagnitudes of change across systems.andP;  For example, E detects only a minisculedifference in complexity between systems #1 and #2.andP;  The improvements made tosystem #2 to create system #3 are reflected by a substantial decrease in E.andM;As reported by Ryge [40], and confirmed in this study, structuralimprovements lead to increased IF nesting and increased complexity of thePERFORMed hierarchy.andP;  MIN and C2 reflect these changes.andM;For the portfolio of tasks, Halstead's E, McCabe's v(G), Woodward's K, andGaffney's Jumps provide relative rankings consistent with relative time andthe frequency of serious ripple effect errors.andP;  E, v(G), K, and Jumps measurebulk and control flow complexity.andP;  This type of complexity appears to berelated to both the time required to perform a portfolio of maintenance tasksand the frequency of ripple effect errors introduced with modifications.andM;Benyon-Tinker's C2 provides a ranking which matches the frequency of seriousprimary errors.andP;  For the systems and tasks in this study, the morecomplicated the hierarchy of performed procedures, the more frequentlyprimary errors were introduced.andP;  Chen's MIN (depth of IF nesting) is lessstrongly associated with the frequency of primary errors.andM;These six metrics, therefore, appear to be related to performance.andP;  Themetrics provide conflicting rankings of relative complexity.andP;  Halstead's E,McCabe's v(G), Woodward's K, and Gaffney's Jumps reflect a decrease incomplexity with improvements in system structure.andP;  Chen's MIN andBenyon-Tinker's C2 reflect the offsetting increases in complexity due to IFnesting and number of modules.andP;  The conflicting rankings are notinconsistent.andP;  Rather, both sets of metrics reflect important dimensions ofsystem maintainability.andP;  These objective metrics appear to offer potential asproject management tools.andM;5.andP;  DISCUSSION AND IMPLICATIONS OFandM;RESEARCHandM;The first general purpose of this study was to examine how certain systemfeatures influence programmer performance during maintenance activities.andP;  Toaddress this issue, an ill-structured COBOL system (system #1) was improvedin two stages.andP;  System #2 was developed by eliminating long jumps in code andlimiting control flow to sequence, selection, and iteration.andP;  The thirdsystem was developed by eliminating redundancies in code and writingprocedures at higher levels of abstraction.andM;Programmers implemented a common set of maintenance tasks to the threesystems, enabling preliminary conclusions to be drawn about the impact ofsystem structure on programmer performance.andP;  Based on these conclusions, onecan speculate about the potential payoffs associated with improving oldsystems.andM;The first structural revision, eliminating jumps and adhering to threecontrol structures, produced only marginal benefits in terms of programmerperformance.andP;  The total average time required to perform the portfolio ofthree maintenance tasks was 5 percent less for system #2 than for theoriginal system (#1).andP;  This was observed only across the portfolio of alltasks.andP;  On specific individual tasks peformance was less consistent withexpectations; in fact, two of the three maintenance tasks were moretime-consuming when implemented to system #2 than to system #1.andM;Revising a system to adhere to the three structured constructs does notappear to be a very worthwhile activity until one examines the errorsintroduced with changes.andP;  Simple improvements in system structure led to asubstantial (42 percent) decrease in the frequency of ripple effect errors.andO;Errors were still introduced with modifications, but they tended to beprimary errors, rather than ripple effect errors.andP;  Since primary errors aremore easy to detect during post-maintenance testing, they are less seriousthan ripple effect errors.andM;The second set of structural improvements, eliminating redundancies andwriting code at higher levels of abstraction, enhanced programmer performanceby every measure.andP;  Improved performance was observed both on individualmaintenance tasks, and across the portfolio of task.andP;  The total average timerequired to perform the portfolio of three maintenance tasks was 13% less forsystem #3 than for system #2 (and 17% less for system #3 than for system #1).andO;Almost all errors introduced with changes to system #3 were primary errors.andO;Of the thirty-six modifications to system #3, only one introduced a rippleeffect error.andM;In sum, this study found tangible improvements in programmer performance as aresult of system structural improvements.andP;  To the extent that these resultsare generalizable, the benefits from improving ill-structured systems shouldaccrue over the life of the system.andP;  These benefits should appear in the formof reduced maintenance time and decreases in required corrective maintenance.andM;Beyond the tangible results associated with improving systems, this studyrevealed interesting phenomena related to assessing relative systemcomplexity.andP;  Programmers were inconsistent in ranking systems according tocomplexity, apparently because they were unable to separate the complexity ofthe system from the complexity of the task at hand.andP;  Objective metricsprovided rankings consistent with actual programmer performance.andP;  Thissuggests that objectively based metrics might provide more reliableinformation for managers than subjectively based measures.andM;Metrics which measure the relative ease with which systems can be modifiedwould serve at least three purposes.andP;  First, such measures may be usefulcriteria for acceptance testing--comparable to the mileage ratings used byconsumers in purchasing automobiles.andP;  Managers currently have no way ofknowing at the time of purchase or acceptance whether a system will berelatively expensive or inexpensive over its lifetime.andP;  A maintenance measuremight provide information useful in refocusing interest from initial costs,to lifetime costs, of systems.andM;Second, if metrics were used in acceptance testing, specific target valuescould be established as design goals.andP;  Deviations from target values could beflagged and investigated.andP;  And third, metrics applied before and aftermodifications are made would provide a measure of the deterioration of thesystem over time.andP;  Because changes disturb system structure, it is probablethat an actively maintained systems reaches a point at which it is mrecost-effective to redevelop the system as a new project than to furthermodify it [9].andP;  Metrics might be used to establish such a milestone.andM;The results of this study indicate that improvements in a system lead toimprovements in programmer maintenance performance.andP;  Further, structuralimprovements appear to be measurable using objective complexity metrics.andM;6.andP;  SUMMARY AND CONCLUSIONSandM;This study was designed to provide evidence on the effect of sytemstructure/complexity on system maintainability.andP;  Three specific questionswere addressed:andM;* What, if any, effect do structural differences have on maintenanceperformance?andM;* Are structural differences discernible to programmers?andP;  * Are structuraldifferences measurable?andM;The results of this study indicate that structural differences do impactperformance.andP;  Specifically, improving a system by eliminating GO TOs andeliminating redundancies through writing procedures at the highest levelpossible appears to decrease the time required to perform maintenance, and todecrease the frequency of ripple effect errors.andM;These benefits ma or may not be observed on specific individual maintenancetasks; rather, they are observed across a portfolio of tasks.andP;  Thus,rewriting systems to incorporate structural improvements may lead to benefitsover the life of an actively maintained system.andP;  There is a payoff torestructuring even without a complete redesign.andM;This study reveals that structural differences, although exerting aninfluence on performance, are not discernible to programmers.andP;  Apparentlyprogrammers were unable to separate system complexity from task complexity.andO;This leads one to question the efficacy of subjectively-based assessments ofsystem complexity.andP;  Depending upon the experience brought to bear on anevaluation, and also on the contexts in which progrmmers place themselveswhile making evaluations, diverse subjective rankings may be provided.andM;Finally, sets of automatable metrics provided relative rankings which matchedperformance.andP;  One set of metrics--Halstead's E, McCabe's v(G), Woodward's K,and Gaffney's Jumps--was related to the time required to implement theportfolio of changes and also to ripple effect error frequencies.andP;  Anotherset of metrics--Chen's MIN and Benyon-Tinker's C2--was related to thefrequency of primary errors introduced with modifications.andM;These six metrics appear to offer potential as project management tools.andO;While both sets of metrics were related to performance, E, v(G), K, and Jumpsappear to measure the more important type of complexity.andP;  These metrics wererelated to both the time required to implement changes and the frequency ofripple effect errors.andP;  Minimizing the values of E, v(G), K, and Jumps is auseful design goal, even at the expense of complexity measured by C2 and MIN.andM;The purpose of experimentation is to prove theory.andP;  The results of this studyafford some insight into software maintenance.andP;  As is often the case withempirical research, many questions have been raised through thisinvestigation.andP;  Further research is needed to determine whether therelationships observed in this tightly controlled experiment exist in livesettings.andP;  Longitudinal studies in live settings, such as those conducted byBasili et al.andP;  [6], will provide substantive data on factors influencingmaintenance performance.andP;  Further research is needed into the dimensions ofprogrammer perceptions of complexity.andP;  Investigations in these areasultimately may yield information useful in reducing the software maintenanceburden to industries.andO;</TEXT></DOC>