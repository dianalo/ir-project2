<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF108-276-670  </DOCNO><DOCID>08 276 670.andM;</DOCID><JOURNAL>IBM Systems Journal  March 1990 v29 n1 p124(17)* Full Text COPYRIGHT International Business Machines Corp. 1990.andM;</JOURNAL><TITLE>Porting DPPX from the IBM 8100 to the IBM ES/9370: installationand testing. (DPPX operating system)</TITLE><AUTHOR>Boehm, G.E.; Palmiotti, A.M.; Zingaretti, D.P.andM;</AUTHOR><SUMMARY>This paper describes the software tools, testing activities, andtesting methods that were used to port the DPPX/SP operatingsystem from its original implementation on the IBM 8100Information System to its new implementation on the IBM ES-9370Information System.andP;  (Reprinted by permission of the publisher.)andM;</SUMMARY><DESCRIPT>Product:   IBM ES/9370 (Mainframe computer) (programming).andO;Topic:     System ConversionCase StudyOperating SystemsTestingMethodsSoftware MigrationInformation Systems.andO;Feature:   illustrationtablechartgraph.andO;Caption:   The staged approach. (table)A spin. (chart)Spins and stages. (graph)andM;</DESCRIPT><TEXT>Porting DPPX from the IBM8100 to the IBM ES/9370: Installation and testingThis paper describes the software tools, testing activities, and testingmethods that were used to port the DPPX/SP operating system from its originalimplementation on the IBM 8100 Information System to its new implementationon the IBM ES/9370 Information System.andM;The porting of the distributed Processing Programming Executive SystemProduct (DPPX/SP) operating system, which was originally designed to run onthe IBM 8100 Information System, to the DPPX/370 operating system which runson IBM ES/9370 hardware, moved an operating system from one hardwarearchitecture to another.andM;To accomplish the port, four basic steps had to be completed:andM;1.andP;  A new compiler, the PL/DS2 compiler, had to be developed to generateSystem/370 Assembler instructions from PL/DS (Programming Language forDistributed Systems) source code.andM;2.andP;  the machine-dependent components of the DPPX operating system had to beredesigned and rewritten to accommodate the new hardware.andM;3.andP;  The machine-independent components of DPPX had to be recompiled with thenew compiler.andM;4.andP;  The new machine-dependent components and the recompiledmachine-independent components had to be installed and tested on the ES/9370hardware.andM;These four steps took approximately 30 months to complete, with the most timespent on installing and testing the new recompiled components.andM;Much attention was given to the testing effort of porting DPPX because of theunique problem that testing had to address: How to take the redesignedmachine-dependent components of DPPX, mix them with the recompiledcomponents, install them and test them on ES/9370 hardware, and assure thatDPPX/370 running on an ES/9370 would work with a quality rating equal to, orbetter than, DPPX/SP running on an 8100.andM;The solution was to divide the project into stages, start at the bottom, workto the top, and test it along the way.andM;Staging the installation and testing of DPPX/370andM;The layered architecture of DPPX, which is described in Reference 1, lentitself to the installation and testing of the ported code in a &quot;stagedapproach.&quot;andP;  the overall strategy of the staged approach was to divide thesystem into its basic components, identify the dependencies that existedbetween them, and implement them in sequence from the bottom up in a seriesof seven stages.andP;  The early stages contained the low-level supervisorfunction that was needed to support the higher-level functions that wee tocome in later stages.andP;  Figure 1 illustrates this staged approach.andM;A stage was the vehicle that was used to build, plan, test, and manage thecode being ported and the resources needed to do it.andP;  Each stage was assignedto a manager who was responsible for developing and executing a stage plan.andO;The stage plan showed the major system functions to be implemented and theirduration.andP;  The plan also showed, at a high level, the logical sequence inwhich the various system functions would have to become available so that thesystem could be built from the bottom up.andM;Beginning with the first stage, planning sessions were held to resolveseveral interrelated concerns.andP;  the detailed functional content of a stagewas evaluated, with consideration given to dependencies between functions ofthat stage.andP;  Module sizings (that is, lines of code) and functionalverification testing plans were reviewed with an eye on project schedues.andO;Current assumptions were examined for confilits with the latest projectplans, and any other outstanding questions applicable to the stage wereresolved as necessary.andP;  In addition, suggestions for quality and productivityimprovements were considered.andM;With the introduction of the critical supervisor and I/O functions, therewere a lot of dependencies between functions.andP;  As a result, the developmentof stages 1 through 3 occurred in a fairly sequential manner.andP;  Each functionbeing introduced into a stage had one or more dependencies on other functionspreceding it in the same stage.andP;  Thus there was little parallel development.andO;Any slippage in introducing a critical function would directly impactsubsequent functions coming in.andM;At the end of stage 3 a very important checkpoint occurred: The systemsuccessfully IPLed from the system residence direct access storage device(SYSRES DASD) and supported a user logging on and accessing a DASD data set.andO;With this level of function now available for use, new function was moreeasily introduced in stages 4 through 7.andP;  There were far fewer dependenciesbetween the new functions coming in, which allowed more parallel developmentand easier testing to occur.andM;But even though installation and testing was viewed and tracked as occurringin discrete stages, it actually occurred as a &quot;continuous integration&quot; offunction.andM;DPPX/370 spins.andP;  To introduce multiple functions in a stage, stages weredivided into spins.andM;A DPPX/370 spin combined DPPX/370 software components to make up the DPPX/370operating system.andP;  Spins, generated about every two weeks by the builddepartment, contained different levels of function.andP;  The final product,DPPX/370, was not fully realized until spin 40andgt; however, the birth ofDPPX/370 began with spin 1, which contained the foundation for the laterspins.andM;Figure 2 shows the diversity of software components contained in a spin.andO;Each spin had its own characteristics.andP;  Early spins were weighted with manyindividual modules and scaffolding to provide primitive function.andP;  Laterspins were weighted with full-function components and applications.andP;  Therewere also spins that contained only fixes to previous spins, with no newfunction being added.andP;  Figure 3 shows the relationship between spins, stages,time, and function.andM;Library structure.andP;  To introduce multiple modules, components, andapplications in a spin, code had to be segregated into different librarylevels.andM;The Project Development Library (PDL) software development tool was used tomanage the building of multiple modules into spins for testing.andP;  PDL allowsmultiple versions of the operating system parts to be stored and accessed byother developers.andM;Developers who were ready to make their code available to other developerspromoted it to their department library.andP;  This made common subroutines,modules, and control blocks that must be linked into one load module, andmessages that must be built into message data sets, accessible by everyone inthe department.andM;The highest level of the library contained the oldest and most stable versionof the code which formed the base of the operating system.andP;  Lower levelscontained changes to the modules in the base version and new code that wasadded for later spins.andP;  To create a new spin, the old levels were promoted tothe next higher library level, with the highest library level containing acollection of well-tested spins.andM;By keeping several library levels with different versions of code, developerschose the appropriate level for their use.andP;  Thus, developers who needed astable version picked a higher library level to begin their access.andP;  Adeveloper who needed the latest version of common code picked a lower levelthat was less stable but contained the latest changes.andM;Figure 4 shows the structure of the development library.andP;  The arrows show thedirection code was promoted.andP;  As code was successfully tested, it waspromoted to the more stable library levels.andM;Parallel libraries.andP;  To develop and implement in parallel, parallel librarystructures were needed to segregate the function.andM;In the early stages, because the low-level hardware-related components werebeing developed, all the testing was done by simulating the System/370hardware on a VM syste.andP;  However, separate VM systems were used fordeveloping the code and testing the code.andP;  This ensured that testing wouldnot affect the development environment and provided more flexibility in testsystem configuration.andM;To provide an efficient means to test the various spins built within a stage,executable object code from the development libraries was automaticallycopied to the PDL libraries which were set up on the test system.andM;The availability of parallel libraries ensured that testing could continue ona stable base at the same time integration testing was being done on newfunctions.andP;  VM-based testing tools allowed the developers to select thedesired level of operating system executable code to use, and performed asimulated IPL.andP;  The testers chose the appropriate level based on the functionthey were dependent on and the level's stability.andM;Once the system became stable and could be IPLed, the parallel libraries wereno longer needed.andP;  Build tools were used to load the system onto test disks,the developers added their code to be tested, and IPL was executed throughVM.andP;  The development libraries, however, continued to provide controlledsharing of the operating system code.andM;Functional verificationandM;Functional verification testing is part of the software development processused by the DPPX organization to provide quality code to its customers.andO;Traditionally, functional verification (FV) is a test, or group of tests,whose purpose is to verify that the functional operation of a module, from aninternal perspective, is correct.andP;  An FV test is performed by exercisingevery functional variation of a module with valid and invalid data to ensurethat the data are processed correctly.andM;FV testing as performed by DPPX development includes test activities that arefrequently thought of as unit test, FV test, and component test.andP;  Thus, FVtesting is applied not only to individual modules but to groups of modulesthat are combined to provide services or functions (such as commandprocessors) and whose external characteristics must be verified.andP;  Functionalverification is performed by the programmers who own the modules being portedduring a stage.andM;During the early stages, FV testing helped verify the correct operation ofthe new development compiler.andP;  The testing ensured that recompiled modulesperformed the same function on an ES/9370 that they had on an 8100.andP;  Ingeneral, however, modules that were only to be recompiled were not scheduledfor extensive FV testing.andP;  The bulk of FV testing was performed on newmodules that provided new function on the ES/9370, and on rewritten moduleswhich were changed to accommodate the new hardware.andM;Functional verification of DPPX/370 provided for the detection of severalclasses of software errors (bugs) which follow.andM;Compiler bugs and user error bugs.andP;  FV testing of DPPX/370 provided the first&quot;live&quot; test of the new compiler, which was developed in parallel withDPPX/370.andP;  It would come as no surprise, then, to see some compiler errorssurface.andP;  But compiler errors were by far the hardest bugs to detect,analyze, and correct.andP;  This difficulty was due, in part, to the lack ofexperience the DPPX organization had with the differences between 8100 andES/9370 architectures, with System/370 assembly instructions, and withrecognizing compiler bugs.andM;Compiler bugs were very important because of the far-reaching effect theycould have.andP;  Once it was recognized that compiler bugs were beingencountered, the first burst of debugging energy was put on determining the&quot;type&quot; of bug encountered.andP;  If it appeared to be a compiler bug, the bug wastransferred to a &quot;compiler debug team,&quot; that would either identify the bug asa compiler bug or a &quot;user error bug.&quot;andP;  A user error bug is a bug that wasgenerated because of the architectural differences between the 8100 and theES/9370.andP;  They were usually fixed by either declaring some variable withdifferent attributes or recoding some logic to accomplish the same task withdifferent instructions.andP;  When true compiler bugs were discovered, the entirelibrary of tested code had to be recompiled with he fixed compiler andretested.andM;Compiler bugs were most prevalent during the first three stages of theporting projectandgt; after stage 3, they were rarely encountered.andM;Software tool bugs.andP;  During the early stages of testing DPPX/370, tool bugswere almost as difficult as compiler bugs to detect and analyze.andP;  The reasonfor the difficulty was because of the close coupling between the tools tobuild, load, and execute the DPPX/370 code and the code itself.andP;  Also, a mixof skills was required to analyze tool problems.andP;  Those familiar with thetools had minimum knowledge of the internals of DPPX/370 and those familiarwith DPPX/370 had minimum knowledge of the internals of the tools.andP;  As theprogrammers became more familiar with the tools and the test bed, tool bugswere more easily detected and fixed.andM;Scaffolding bugs and DPPX/370 code bugs.andP;  Scaffolding and DPPX/370 code bugs,in contrast to compiler and tool bugs, were more readily indentifiable with aspecific function or area of DPPX/370 code, making them the easiest todetect, analyze, and fix.andP;  The majority of bugs fell into this class.andM;FV test bed.andP;  To support functional verification, simulated System/370hardware was provided by the virtual machine (VM) operating system, whichexecuted DPPX/370 code as a guest (that is, second-level) operating system.andO;Special considerations had to be made to accommodate this testing environmentsince DPPX/370 is not designed to function as a guest operating system.andM;VM was used as the FV test bed for the following reasons:andM;VM was available before the real hardware.andP;  The porting effort began eightmonths before the ES/9370 hardware was available for testing DPPX/370 code.andO;Delaying testing until ES/9370 hardware was available would have delayed theavailability of the operating system.andM;VM capacity was available in larger quantities than the real hardware.andP;  Forthe duration of the project, only seven ES/9370 computers were available to8k development programmers to port over a million lines of code.andM;In the early stages, before sufficient DPPX/370 function was available toprovide a test bed on the real hardware, 15 multisession terminals wereavailable to the programmers and testers of a stage to test their code.andO;Multisession support (one terminal supporting four different sessions at thesame time) was required because of the software tools used to test the portedcode on VM.andM;In the later stages, multisession support was provided via RLSS andVM/Virtual Telecommunications Access Method (VTAM), which allowed the DPPXprogrammers to test their code from their office terminals.andM;VM provided the capability to do parallel testing of DPPX/370 function.andP;  Themain aspect of a stage was that it identified the primitive or low-levelDPPX/370 function that had to be available to support the higher-levelfunctions and applications.andP;  But although a dependency existed between thelow- and high-level function, both levels could be tested at the same time,in parallel.andP;  Parallel testing was accomplished by using scaffolded code tosimulate function, shared VM minidisks to allow multiple system versions toexist simultaneously, and special software tools, developed by the DPPXorganization.andM;Several software tools played an important role in the porting of the DPPXoperating system.andP;  Build tools were used to: build, load, and executeprimitive versions of DPPX for testing before sufficient support wasavailable for a normal IPLandgt; create DPPX/370 system resident (SYSRES) volumesfrom specification filesandgt; and place SYSRES images onto IPLable fixed blockarchitecture disks for testing.andM;Aside from the build tools, there was a software debugging tool, described inthe next section, that became critical to the timely completion of FVTesting.andM;DPPX/370 running on VM remained the primary FV test bed for most of theporting effort.andP;  However, all code was eventually tested on the ES/9370hardware during independent component and system tests.andM;Common verification tool.andP;  When reviewing the debugging requirements forDPPX/370, several issues had to be considered.andP;  Foremost was the nature ofthe operating system itself.andP;  DPPX/370 is designed to run on a machine withthe architecture of a System/370, and a debugging tool was needed that wouldmake accessible all the features of a System/370-style operating system:general-purpose registers, control registers, real and virtual addressing,condition codes, and program status words (PSWs).andP;  Since DPPX/370 is anoperating system, the debugging tool would require different characteristicsfrom typical single-program debuggers.andM;Several approaches to debugging were considered, including the use ofdebugging facilities on the target hardware itself.andP;  This was notsatisfactory because of the lack of test hardware as explained earlier.andO;Another consideration was to build a test version of the DPPX/370 system soit could be IPLed as a guest operating system running on VM/SP.andP;  With thisapproach, any debugging facilities of CP (VM/SP's control program) would beavailable.andP;  But another drawback emerged: CP's debugging facilities are toolow level to meet our development productivity objectives.andM;To address productivity, another debugging tool, Source Level Debug (SLD),was considered.andP;  It allowed one to debug programs at the source, or programcode, level.andP;  However, there were more drawbacks since SLD fell short inthree significant areas:andM;* It supported the debugging of individual CMS programs, but DPPX/370 is anoperating system that does not run under CMS.andM;* It did not support the programming language that was being used forDPPX/370.andM;* It provided little support to debug at an assembler code level.andM;These shortcomings were unfortunate since the other features provided by SLDwould increase debugging productivity considerably.andM;In the end, it was decided to combine some functions of SLD with newdebugging functions written by our own tools department.andP;  This would provide,among other things, the necessary support for debugging non-CMS programs (ourmost critical requirement) and the ability to debug assembler code.andP;  It wouldalso provide the ability to test from one's own office terminal, eliminatingany dependence on actual ES/9370 hardware in the early stages of testing.andM;This special implementation of SLD was the tool called Common VerificationTool (CVT), an in-house tool not shipped with the DPPX/370 licensed program.andM;CVT provided debugging capabilities which were rich in function and easy touse.andP;  The primary functions provided by CVT include:andM;* Pausing at specific program locations through the use of breakpointsandM;* Interrupting program execution or wait statesandM;* Displaying or altering the contents of storage, registers, and the PSWandM;* Stepping through machine instructionsandM;* Logging the debug session and scrolling the session listingandM;* Repeating command sequencesandM;In addition to these functions, a certain amount of additional debugcapability is available by using any of several CP commands.andP;  These CPcommands can be used simultaneously with CVT, thus giving the tester greatercapability for addressing a given problem.andM;The structure of CVT requires an interface to Virtual Machine CommunicationFacility (VMCF), a component of the VM/SP operating system which permitsinformation exchange between VM userIDs.andP;  In addition, code hadd to be addedto three areas in the DPPX/370 supervisor to accommodate the needs of CVT forinitialization, program and external interrupt handling, and the programcontents of system storage.andM;Independent component testsandM;Philosophy.andP;  Independent component test (ICT) tests all the components is asystem from an external perspective.andP;  ICT looks at each component in thesystem and tries to use it or break it as a customer would.andP;  Independentmeans that the people planning and executing the tests are totally unrelatedto the component being tested.andP;  This independence gives the benefit ofsimulating a customer environment.andP;  The tester really becomes a customer andmust use customer-like documentation to learn and use the functions providedby a component.andM;ICT usually has two parts which begin after development has completed its FVtests: a regression test which verifies that the system has not regressedsince the last release and a new function test which verifies that the newfunction added to the system works according to the documentation that willbe provided to the customer.andM;Risks and concerns.andP;  Ict addressed the primary concerns, from a testingperspective, with porting code from the 8100 to the Es/9370:andM;1.andP;  Would the code generated by the new compiler perform the same function onthe ES/9370 that it did on the 8100?andM;2.andP;  Would the performance of the system increase when going from 8100architecture to ES/9370 architecture?andM;3.andP;  Could customers migrate their applications easily from 8100s to ES/9370s?andM;4.andP;  Would timing and stress-related problems show up because of thedifference between the ES/9370 and 8100 architectures?andM;Types of tests.andP;  The ICT effort, like the development effort, wasaccomplished in seven stages and the function of the ICT group actually wentbeyond a typical independent component test.andP;  The ICT group was responsiblefor performing the following tests.andM;During stages 1 through 3, there was no terminal support in the system andICT could not test the system like an external user.andP;  To avoid wasting time,and to achieve as much test coverage as possible, ICT carried out stagevalidation (SV) tests.andP;  The emphasis of the SV test was to test systemsupport for customer applications as early as possible.andM;SV tests were performed by putting hooks and stops in system code andsimulating multiple-user environments.andP;  System dumps were taken to veritycorrect operation.andM;Within each stage were multiple spins.andP;  Spin validation tests were simply a&quot;bucket&quot; or subset of test cases, that had successfully completed on previousspins.andP;  When a new spin was made available, this bucket was executed toensure that the system had not regressed.andP;  After new tests were completedsuccessfully, they were added to the test bucket.andM;As development progressed through the stages, compiler problems werediscovered and fixed.andP;  The only way to really ensure that these fixes did notcause problems with previously compiled code was to totally recompile thecode in the system to that point.andP;  Once the system was recompiled on the&quot;fixed&quot; compiler, the system was handed over to the ICT team to run compilerregression (CR) tests.andM;The CR test bucket was created by doing an analysis of the availablecomponents and determining the kind of coding techniques that were used todevelop them.andP;  The most &quot;compiler stressful&quot; components were selected and asubset of the ICT test cases was re-executed with the &quot;fixed&quot; compiler.andP;  Oncethese tests completed successfully, the &quot;fixed&quot; compiler became the &quot;only&quot;compiler and the porting effort continued.andM;As the stages progressed, more and more components became available to ICT.andO;After a component was &quot;ICTed,&quot; it became part of a multiple components instress (MCS) test.andP;  Buckets of automated tests were created by using manycomponents together in the most stressful sitution that could be created.andO;These buckets were like a stressful regression test and were used extensivelyin stage 7 after all the code was in the system.andP;  MCS buckets were used asavailability tests and many times ran over the weekends to ensure that thesystem was stable and could remain operational for extended periods of time.andO;MCS tests will continue to be valuable regression tests for future releases.andM;Planning for ICT.andP;  The &quot;test group&quot; of three senior-level DPPX programmerswas responsible for developing the basic, high-level ICT plan.andP;  Otherexperienced programmers were frequently consulted to reveiw and discuss thepreliminary high-level plans.andM;The first step in determining how to test DPPX/370 was to divide the entiresystem into logical areas of test.andP;  These areas were called environments and15 emerged.andP;  It is not necessary to discuss all 15, but some examples follow:andM;* COBOL to verify the COBOL instruction setandM;* Program prep to verify the components that a customer would need to prepareand execute a program, such as the editor, interactive map definition (IMD),format management, various compilers, and the linkage editorandM;* Problem determination to verify trace facilities, dump facilities, errorreporting, and ksummarizationandM;* Connectivity to verify all device support that was announced, such asdisplays, printers, controllers, modems, and pass-through operating systemsandM;* Communications to verify the ability of DPPX/370 to communicate with peersystems and host applicationsandM;* Migration to verify the commands, tools, and procedures that customers needto migrate their applications from DPPX/SP to DPPX/370andM;* Performance to verify that the performance of the system was improved overthe 8100andM;The components to be tested in each environment were mapped against the stagewhen the component would be available, and then a schedule of ICT start timesfor each environment was created.andP;  Based on when the environment test couldstart and an estimate of how large the environment would be, a planning phasewas projected to precede each environment test.andP;  During the planning phase,detailed test plans and test cases were written for execution in the testphase.andM;Because of tight schedules, as many redundancies as possible had to beremoved from the test plan.andP;  Tests needed to be prioritized to distinguishthose tests that needed to be executed from those that should be executed iftime and resources allowed.andP;  A test approach review (TAR) meeting was heldfor each component in the system.andP;  Each TAR meeting was attended by at leastone member of the test group, the lead developer for the component, andothers who were familiar with the component.andP;  Many times the others were fromthe National Service Division (NSD) or managementandgt; they might be people whohad previously left the area or anyone who might be considered an expert.andM;Preparation for the TAR meeting involved someone dividing the component intoits functions, subfunctions, associated commands, command operands, possibleerror conditions, and PD tools.andP;  At the TAR meeting, the preparation wasreviewed and discussed.andP;  The discussion involved:andM;* What are the risks with porting this component?andM;* How much of the code is &quot;new&quot; versus &quot;recompiled?andM;* Where were the problems in the past?andM;* What items were &quot;implicitly&quot; tested just by normal system execution?andM;* What items would be sufficiently tested by functional verification?andM;* What items needed explicit ICT tests?andM;* Who (what group) would write the explicit tests?andM;* What items would get no test at all because it was not deemed necessary?andM;* Of the items that needed explicit tests, were there any existing testcases, and, if so, where are they?andM;* What should ICT do to stress the component?andM;* Are there any available &quot;regression&quot; tests for the component?andM;* In what ICT environment should the various items be tested?andM;* How large is the ICT effort for this component?andM;* When will the component be available to ICT and how much of it will beavailable at ht time?andM;From the TAR meeting came an understanding of what work was needed to bedone, who would be responsible for the work, and when the work needed to becompleted.andP;  The basic ICT test plan was created from these meetings.andM;Executing and controlling the test effortandM;As the TAR meetings progressed, a new library structure called Test Library(TL) was developed that could hold test cases, test plans, test programs, andthe results of the TAR meetings.andP;  The TL also included automated tools, whichcould be used to create &quot;test packages&quot; for each of the 15 environments.andM;A test team and test team leader were assigned to each environment.andP;  Duringthe planning phase, the team leaders had several responsibilities: find andmerge existing test cases into the environment's test packageandgt; automate andmodify existing test cases where necessaryandgt; create test cases that wereneeded but did not existandgt; coordinate the planning and testing of anenvironment.andM;When the execution phase began, the team leader coordinated the test effortby ensuring that any corrections made to the test cases or programs duringexecution were promoted back into the test package on the TL.andP;  The teamleader also worked with the test coordinator to set up a tracking mechanismthat kept management informed of the progress of the test effort.andP;  The testteam's responsibility was to run all the test cases associated with anenvironment and ensure they completed successfully.andM;In addition to the test teams and test team leaders, the ICT group had oneICT coordinator.andP;  The test coordinator's responsibility was to understandeach of the 15 test environments and their unique requirements.andP;  Someenvironments needed special hardware, others needed special skills.andP;  Someneeded special tools and special configurations.andP;  It was the ICT coordinatorwho was the liaison among the different departments in the organization,ensuring that the special items were available when a test environment was tobegin.andM;While being actively involved with the planning and execution of each of theenvironments, the test coordinator was also responsible for trackinng andreporting the progress of the test effort.andP;  During the planning phase, thetest coordinator would meet with the team leader of an environment todetermine what areas are to be tested, who was toperform the tests, and howlong it would take.andP;  This information was tracked graphically by stage andenvironment.andM;Measuring and evaluating the results.andP;  Problem tracking and analysis report(PTAR) status meetings were held frequently with management, and the PTARswere assessed in terms of their severity and impact on the testing effort.andO;PTARs that were impeding test progress were given highest priority for beingfixed.andP;  These meetings were very successful for informing management wherethe emphasis was needed to allow the test to continue smoothly.andM;After ICT was complete, an assessment of each component was made based on thenumber of problems found and the amount of code in the component.andP;  Whenassessment revealed a weak component, recommendations for extended testingwere made to the system test group.andM;System testandM;The previous approach to DPPX system test.andP;  In the days of DPPX/SP, the goalof system test was to test the operating system by running in an environmentsimilar to a customer environment.andP;  In effect, system test was the firstcustomer.andP;  The objectives used to meet this goal were as follows:andM;1.andP;  Combine and test all components as a total system.andM;2.andP;  Test the system the way a customer would use it.andM;3.andP;  Put the entire system under stress.andM;4.andP;  Determine and approach system limits.andM;To meet these objectives customer systems were obtained that included objectcode, databases, and customer-developed command lists (CLISTs).andP;  Thesesystems were used quite extensively by the system test group in conjunctionwith &quot;in-house&quot; applications that testers wrote to test areas not covered bythe customer applications, as well as new funcitons.andP;  As new DPPX releaseswere developed, the customer applications were less effective for testing,since they only addressed old functions.andM;Since the system test group did not have the source code for these customerapplications, they could not be modified to take advantage of the newfunctions in the system.andP;  Also if an error occurred in the application, itwas vitually impossible to locate and fix the problem.andP;  This also causedproblems with DPPX problem determination, since the testers were never sureexactly what the application was doing when the system error occurred.andP;  Manysystem test environments (STEs) had to be written to test specific functionsand areas of the system.andP;  By the fourth release of DPPX/SP, system test hadbecome very component-oriented rather than system-oriented.andM;The total systems approach.andP;  Today's method, the total systems approach, isbroken up into three parts: problem determination (PD), contracted efforts,and end-user systems (EUS).andM;The problem determination part of system test had basically remained the samefrom 8100 testing to ES/9370.andP;  Tests were designed to introduce permanent andintermittent hardware and software errors to determine the capability ofDPPX/370 to recognize, properly diagnose, and report the error.andP;  Hardware bugpoints were obtained from the Endicott ES/9370 engineers.andP;  These bug pointswere the physical address of pins on cards within the ES/9370 processorwhich, when grounded, would simulate an actual card failure.andP;  These failuresincluded memory errors, adapter errors, and processor errors.andP;  When a failurewas introduced, the system error log, operator log, and host NEtView[TM]facility were checked to verify that accurate messages were logged.andO;Effectiveness testing was included as part of the PD testing.andP;  In these testcases a person who was computer literate, but not DPPX literate, was asked toperform basic tasks on the computer.andP;  Certain errors would be introduced intothe system as the subject was executing these tasks.andP;  The subject was timedto see how long it took to identify the exact cause of the problem.andP;  Only themessages in the operator and error logs, NetView, and the DPPX supportmanuals could be used for problem determination.andP;  A test was markedsuccessful when a problem was identified, and in some cases resolved, within30 minutes.andM;Certain isolated test efforts were contracted outside of IBM.andP;  Somecommunications tests were also run at IBM locations in Germany and Japan.andO;The test plans were written by the actual testers and were reviewed andapproved by system test members befor the start of testing.andP;  A system testdepartment member monitored all of these tests and reported at the weeklystaff meetings.andP;  The test locations were selected based on their knowledge ofthe functions, their interest (which stemmed from customer requirements), andtheir test bed facilities.andP;  For example, X.25 testing was performed inGermany, since our German customers have a strong requirement for X.25communications support.andM;The third part was the key to the success of this system test effort.andO;Current customer system environments were used to simulate, in our lab, thedaily activities of the customer.andP;  Using these systems allowed the testing ofmigration, usability, and equivalence.andM;The EUS test was broken up into six phases: (1) obtain a customer system, (2)learn, combine, and promote, (3) migrate, (4) execute, (5) expand, and (6)test system under stress and perform unstructured tests.andM;The first two represented the preparation for system test, whereas theremaining four were actual testing phases.andM;Preparation.andP;  System test's goal and objectives were not changed with thisnew approach, but the methods for meeting the objectives were.andP;  In order tomeet the objectives more effectively, it was necessary to find the bestcustomer systems available to use in the test.andM;The general requirements of each customer were determined before approachingthem for testing.andP;  The department members and management identified thefollowing requirements.andM;DPPX application source code.andP;  It was curcial that the group receive theapplication source code for all parts of the application that were notwritten in Cross Systems Product.andP;  All cobol had to be run through thepreprocessor and compiler, and any toehr applications would have to berewritten (for example, PL/DS and assembler applications).andM;CLISTs, panels, command facility extension (CFE) scripts, and user IDdefinitions.andP;  All parts of the system, written or modified by the customers,were necessary to run their application.andM;DPPX databases and transactions.andP;  Test data and the customer-definedtransactions were crucial to executing the complete customer application.andM;Application documentation.andP;  We felt that any information the customer couldsupply us in the form of data-logic flow diagrams and manuals would help uslearn how the application runs and how to run the application.andM;In order to duplicate the operating environment completely, it was importantto have host source code that communicates with DPPX, host databases andtransactions, and host application documentation.andP;  However, due to thecomplexity and hardware dependencies of the host applications, it wasvirtually impossible to obtain this material.andM;With the requirements known, it was decided that obtaining threee customersystems would be sufficient for the system test effort.andM;Applications were selected from three different business environments (anauto parts inventory system, an insurance claims processing system, and aplant maintenance and control system) which, when combined, would use most ofthe components of the operating system.andP;  it was also a requirement that atleast one customer application be written primarily in COBOL and at least onebe written primarily in Cross System Product.andM;Once the three customers and IBM had come to an agreement concerning theterms and conditions of the project, contracts were written up and signed,and key people from the system test department visited the customer sites tolearn the system and understand the running environment.andM;Once the customer application arrived it was immediately loaded on an 8100 toverify that all the parts were present.andP;  The department was divided intothree teams, each consiting of one senior department member and one juniormember.andP;  each team was responsible for a cutomer system, and the seniormembers were jointly responsible for creating a fourth combined system whichconsisted of all three customer applications.andM;The team members spendt approximately one month familarizing themselves withtheir customer system.andP;  They documented procedures for starting and runningthe application.andP;  These documents, along with the documentation received fromthe customer, were used to create Teleprocessing Network Simulator (TPNS)scripts that would be used for multithread and stress testing.andP;  Thisfamiliarization period was also important for planning which parts of thesystem would be migrated to the ES/9370.andM;Test Library (TL) was used for the archiving of the customer applicationparts.andP;  The tool resided on a virtual memory (VM) system.andP;  The testersscanned the customer system and transferred the customer-specific parts toTL.andP;  All parts likely to cause a problem when merged into the combined systemwere documented.andM;An installation process was developed for each system to simplify theinstallation of a customer system from TL to an 8100.andP;  This installationprocess consisted of running CLISTs that created all of the databaes, and setup environments and device definitions.andP;  installation instructions werewritten to guide the tester, and any specific hardware and softwarerequirements were identified.andM;The final phase of system test preparation was to preprocess and compile theCOBOL applications, and perform any needed rewrite of other applications.andO;The MVS COBOL II preprocessor and compiler were used in this phase.andP;  Thisproved to be the most tedious and time-consuming task.andP;  It was found that theold DPPX COBOL compiler was not as strict about adhering to the standardCOBOL programming rules.andP;  In many cases programs that compiled without errorwith the DPPX compiler.andP;  Work had to be done to modify MVS COBOL compiler.andO;Work had to be done to modify the DPPX COBOL applications.andP;  Applicationswritten in other languages such as DPPX Assembler and PL/DS also had to berewritten in either COBOL, Cross System Product, or System/370 Assembler.andP;  Intwo cases the customer sent a team of their developers to aid in therewriting of the applications.andP;  This proved mutually beneficial for obviousreasons.andP;  Once this work was completed, both the 8100 applications and therewritten ES/9370 applications were transferred to TL for archiving.andP;  We werenow ready to start system test.andM;System test activity.andP;  The first phase of system test was migration.andP;  UsingDPPX/370 Migration: Planning, [2] each team determined the best method formigrating its system.andP;  Some modifications to the team's plans had to be madeto verify that all three migration methods were used (Distributed SystemsExecutive, stand-alone DASD dump/restore, and Peer Data Transfer).andP;  Themigration tool was loaded on the 8100, and the teams used DPPX/370 Migration:Procedures [3] to execute the migration process.andP;  Timing were takenthroughout this migration phase to determine how much planning and actualmigration time was needed for these large systems.andM;The rewritten COBOL II programs were compiled on MVS and transferred down tothe ES/9370.andP;  System/370 Assembler code was assembled and transferred to theES/9370 systems.andP;  A subset of the COBOL programs was also preprocessed andcompiled on the DPXX/370 system.andM;Finally, a visual inspection of the system was performed to verify that noparts were missing, and the contents of the 810 databases were compared withthe corresponding ES/9370 databases to verify that no corruption had takenplace during the migration.andP;  Three methods were used to compare thedatabases: (1) if the databases had a record length less than 133, they couldbe printed by DPPX and compared, (2) if they were longer than 132, they weretransferred to VM and compared, or (3) if the customer application generateddetailed reports from the database contents, the 8100 and ES/9370 reportswere compared.andP;  With the system now completely migrated, it was time to beginrunning the customer applications on the ES/9370.andP;  This phase was dividedinto three basic parts.andP;  First, the application was tested live by the testteam.andP;  In some cases this uncovered run-time errors in the COBOL modules.andO;For the most part, however, the live testing proved that the migration wasextremely successful.andP;  Second, the TPNS scrip was run using a single TPNSuser to verify the network path and perform some performance tests.andP;  Theoutput of these tests was compared with that of the same run on the 8100system.andP;  Third, the number of TPNS users was increased until the system wasrunning at least 70 percent CPU utilization.andP;  At this time more performancetests were run and compared with 8100 tests.andM;Errors were introduced during the TPNS runs to determine the effect ofhardware and software (application software) errors on the DPPX/370 systemwhile running the customer applications.andP;  These errors included disk and SDLCadapter errors, stopping TPNS before completion (to simulate a communicationbreak outside the ES/9370), stopping and restarting environments, submittinglooping applications to batch processing, and powering off and on livedevices defined to the system.andM;Once the team was satisfied with how the application was running, the teamentered the next phase of system test, to expand the system beyond itscurrent capabilities.andP;  The team determined which functions, or components,were not currently being executed by the system.andP;  They then modified thesystem to incorporate these functions.andP;  Basically, these functions were thoseintroduced in Release 4 of DPPX/SP and DPPX/370 that the customer was notimplementing.andP;  All customer systems were at DPPX Release 3 level.andP;  Onecustomer system was brought to a Release 4 level upon arrival.andM;The expanded system was the rerun, starting with the customer application,and starting the new components one at a time until the entire expandedsystem was running.andP;  The objective was to push the processor to approximately90 percent utilization and continue under the stress for long periods of timewithout IPLing.andM;At the same time, the senior team members were combining the three customerapplications on a single system to run a CPU and I/O intensive stress testfor an extended length (at least 65 continuous hours).andP;  Where necessary,CLISTs and in-house developed applications were added to stress the systemfurther.andP;  Due to the size and complexity of the new system, it was impossibleto run an equivalent system on an 8100 and get performance comparisonsandgt;however, some performance tests were run.andP;  Also, time was spent tuning thecombined system to maximize execution time and reduce overhead.andM;The final phase of system test was called the &quot;unstructured test period.&quot;andO;After the test cases had been executed and all high-severity problems hadeither been fixed or a plan was in place to get them fixed, all departmentmembers converged on the combined system to rerun any tests they feltuncovered the most problems, or to run any new tests that they were not ableto use during the earlier phases of system test.andP;  This has proved in the pastto be quite beneficial shaking out some smaller problems before the systemgoes out to the customer.andM;Once system test was completed, an exit review meeting was held by bothsystem test and development managers to confirm that the test exit criteriahad been met and all outstanding problems had a plan in place to be resolved.andM;Benefits of the new approachandM;The total systems approach has proved to be beneficial in many waysandM;Working with the customer created an atmosphere of technical exchange.andO;Because of the information passed to the customer about the migrationexperiences with their system, the customer was able to start planning formigration earlier.andP;  In many cases the testers had to work with the customer'stechnical staff to resolve problems.andM;The department members gained a better understanding of how the customeractually uses the DPPX system.andP;  In general, the development and test groupsare isolated from the customerandgt; they really do not understand the degree ofcomplexity of the customer applications and how these applications interactwith the DPPX system.andP;  By gaining this understanding, they can better developand test DPPX.andM;Prior to the test effort the sytem test department members had single areasof expertise where they would spend most of their time testing.andP;  Through thisnew method, the testers learned more about the overall system than wouldotherwise have been possible in the same amount of time.andM;Since the application source code and documentation was available, it wasmuch easier to diagnose a problem and separate system problems fromapplication errors.andP;  Once it was understood in detail how the applicationsfunctioned, the time needed to obtain the proper data for development to fixa system problem was reduced significantly.andP;  If a problem needed to bereproduced, the tester was more familiar with what was going on at the timeof the error and was more like to be able to reproduce the error and takeless time to do so.andM;More components were tested by fewer people earlier in the test cycle thanwas possible using the old method.andP;  System test concentrated on a completesystem throughout the test period rather than testing individual components.andM;Fewer systems were needed to run this test, compared with the old method, andin general there was a constant high degree of machine utilization during thetest period.andP;  The system was under stress early in the test cycle.andM;The combined customer system functioned as an excellent vehicle forregression testing and fix package testing.andP;  Since the combined customersystem exercises almost every component in the DPPX/370 system, there is noneed to spend weeks sifting through old test plans to create a regressiontest.andP;  If also stressed the system in a more realistic manner by having alarge number of simulated users execute real customer applications.andM;Concern with the new approach.andP;  There were two major concerns that arose frommanagement about this new approach.andP;  There was a serious concern that thetest group may inadvertently become concerned with the customer applicationsrather than with the system itself.andP;  Management felt that it was possiblethat the testers would spend more time trying to understand what theapplication was doing rather than how the system was functioning under theapplication.andP;  To avoid this, all system test plans were inspected by peers,management, and key developers.andP;  Also, weekly status of tests completed andplanned for the following week was provided to management.andM;There was also a concern that error testing would not be performed to thedegree it was done in the past.andP;  This concern was addressed by having thetesters document in their test plans all errors they planned on introducinginto the system.andP;  Thus coverage could easily be evaluated.andP;  The problemdetermination test, which introduced a variety of hardware and softwareerrors, was also documented and inspected.andM;Directions.andP;  The total systems approach proved to be a great improvement overthe old method of system test.andP;  Many problems that were not found incomponent testing were uncovered and a number ofcustomer-application-dependent problems were found.andP;  These problems would nothave been found under the old method of testing.andP;  DPPX/370 was exposed to acustomer system environment from the start of system test.andP;  This provided uswith a stable system earlier in the development cycle.andM;We are exploring other opportunities which will involve the customer evenmore in the development cycle, including involving the customer in more testefforts and possibly development and design efforts.andM;SummaryandM;The development approach taken by the Distributed Systems Programming (DSP)organization was key to the successful port of DPPX/SP to DPPX/370 for manyreasons.andP;  Programmer productivity was optimized by staging the developmentactivities, by having a test bed before the actual hardware was available,and by providing the developers with tools to simplify debugging.andP;  A stableDPPX/370 system was attained very early in the development cycle by testingDPPX/370 at every stage rather than waiting for all development activity tocomplete.andP;  The component tests were executed by testers other than thecomponent developers.andP;  This exposed documentation and usability problems, inaddition to functional errors, which otherwise may not be have been found.andO;The use of customer application environments in system test permitted greaterand more realistic test coverage and required fewer resources than normal.andM;The DSP organization is continually investigating new ways to improve itsapproach to systems development.andM;AcknowledgmentsandM;This paper, a summary of information documented in a series of internaltechnical reports, is due in no small part to the contributions of a numberof our colleagues.andP;  Their work has provided a wealth of information fromwhich to draw.andP;  We wish to acknowledge thework of Bob Abraham, LouFitzpatrick, John Forsythe, Garth Godfrey, and Brian Goodrich.andM;NetView is a trademark of International Business Machines Corporation.andM;Cited referencesandM;[1.] R. Abraham and B. F. Goodrich, &quot;Porting DPPX from the IBM 8100 to theIBM ES/9370: Feasibility and Overview,&quot; IBM Systems Journal 29, no.andP;  1,90-105 (1990, this issue).andM;[2.] DPPX/370 Migration: Planning, GC23-0669, IBM corporationandgt; availablethrough IBM branch offices.andM;[3.] DPPX/370 Migration: Procedures, GC23-0669, IBM Corporationandgt; availablethrough IBM branch offices.andM;Gerald E. Boehm  IBM Kingston Programming Laboratory, Neighborhood Road,Kingston, New York 12401.andP;  Mr. Boehm is a staff programmer in the DistributedSystems Build, Release, and Tools department in IBM's Kingstone ProgrammingLaboratory.andP;  He joined IBM in 1973 as a junior programmer in the DistributedComputing Facility project in Poughkeepsie, new York.andP;  In 1977 he transferredto Kingston and joined the DPPX development project.andP;  He has participated indesign, code, test, and inspection activities for the DPPX linkage editor andstand-alone input/output processor.andP;  Mr. Boehm accepted a 19-month assignmentin Sweden to help develop procedures, tools, and test cases for the DPPX/APLproject.andP;  Since 1983 he has been involved with the design and implementationof automated procedures to improve various DPPX development activities.andP;  Heearned his B.S.andP;  in physics from St. Francis College, Loretto, Pennsylvania,and an M.S.andP;  in computer science from Pennsylvania State University.andP;  Hereceived training as a computer system operator while in the United StatesAir Force.andM;Arthur M. Palmiotti  IBM Kingston Programming Laboratory, Neighborhood Road,Kingston, New York 12401.andP;  Mr. Palmiotti is a project programmer in theDistributed Systems Test department in IBM's Kingston Programming Laboratory.andO;He joined IBM in 1984 as a junior associate programmer and concentrated hisefforts on testing device attachment and support.andP;  Since 1985 he has been anactive member of the DPPX I/O council which determines what devices can besupported and identifies development and test requirements.andP;  Otherresponsibilities include system test planning, stress testing, and customersystem testing.andP;  Mr. Palmiotti received his B.S.andP;  in mathematics and computerscience at the State University of New York at New Paltz in 1983.andM;David P. Zingaretti  IBM Kingston Programming Laboratory, Neighborhood Road,Kingston, New York 12401.andP;  Mr. Zingaretti is an advisory programmer in theDistributed Systems Programming Development department in IBM's KingstonProgramming Laboratory.andP;  He joined IBM in 1981 to work in DPPX communicationsnetworking.andP;  From 1981 through 1986 he developed DPPX operating systemenhancements with primary emphasis on the command facilities, environmentmanagement, DASD queue and X.25 components.andP;  From 1986 to 1988 he wasresponsible for porting various DPPX components from the 8100 InformationSystem to the 9370 Information System.andP;  Mr. Zingaretti earned his B.S.andP;  inaccounting from King's College, Wilkes-Barre, Pennsylvania, in 1975 and anM.B.A.andP;  in information systems technology from Marywood College, Scranton,Pennsylvania, in 1983.andO;</TEXT></DOC>