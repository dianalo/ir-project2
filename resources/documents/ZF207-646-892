<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO>  ZF207-646-892  </DOCNO><DOCID>07 646 892.andM;</DOCID><JOURNAL>Communications of the ACM  Sept 1989 v32 n9 p1065(8)* Full Text COPYRIGHT Assn. for Computing Machinery, Inc. 1989.andM;</JOURNAL><TITLE>Using an architectural knowledge base to generate code forparallel computers.andO;</TITLE><AUTHOR>Terrano, Anthony E.; Dunn, Stanley M.; Peters, Joseph E.andM;</AUTHOR><SUMMARY>Effective use of a parallel computer depends upon the userbreaking a program into subtasks, devising a mapping of thesubtasks onto processing elements, and adding requiredinterprocessor communication instructions.andP;  The efficiency of theresulting code depends on decisions made at each stage.andP;  Trial anderror is the only current way to solve these problems.andP;  A rapidprototyping and retargetable compiler system is presented thataddresses these programming tasks.andP;  The system employspartitioning directives to automatically decompose the probleminto sets of subtasks and generates intertask communicationinstructions.andP;  The system presents the user with a single globaladdress space.andM;</SUMMARY><DESCRIPT>Topic:     Parallel ProcessingProgrammingProgramming Management.andO;Feature:   illustrationchart.andO;Caption:   Some common stencils and their convex hulls. (chart)Global coordinate system for tiles. (chart)Five-point star stencil for k = 6. (chart)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>Using an Architectural Knowledged Base to Generate Code for ParallelComputers Generating code for existing parallel computers involves severalnontrivial operations that have no analogue in code generation foruniprocessor architectures.andP;  In particular, the work must be partitioned intoschedulable units, the resulting computational units must be assigned tospecific processors, and additional code must be generated to support theresulting interprocessor communication and synchronization.andP;  These steps mustbe performed in order; the mapping cannot be undertaken until the problem hasbeen partitioned, and the details of the interprocessor communication are notkfnown until the mapping has been completed.andP;  The options for partitioningand mapping depend on the problem being solved and the algorithm andarchitecture being used.andP;  For each new combination of problem, algorithm andarchitecture, a new partitioning and mapping must be created and evaluatedbased on the actual communication costs incurred.andM;The efficiency of the resulting program is determined by the quantity andlocality of the interprocessor communication: partitionings that result insmall amounts of distributed communication will result in shorter executiontimes than ones that require large amounts of global communication.andP;  Theactual efficiency is also strongly influenced by details of the architecture.andO;Some architectures incur large overheads to initiate each individualcommunication, so the ability to aggregate messages may have a significantimpact on the efficiency, while other architectures have large memory accesslatencies.andP;  If a particular choice of partition and map is found to be tooinefficient when the communication cost is analyzed, it is usually necessaryto repeat the code generation process from the beginning.andM;At present, the partitioning, mapping, and communication instructions areembedded in the source code for the problem.andP;  Changing any of them caninvolve substantial rewriting of the program.andP;  It is desirable to separatethe programmatic statement of an algorithm from the machine-dependentpartitioning, mapping, and communication specifications.andP;  Ideally, they wouldsimply be orthogonal and a complete program would consist of an algorithm anda partitioning strategy.andP;  Changing the partitioning strategy would notrequire any modification of the statement of the algorithm.andM;In this article, we present a compiler for distributed memory parallelcomputers which performs automatic program partitioning, mapping, andcommunication code generation.andP;  The prototype compiler is implemented inProlog, with the code generation advice incorporated as a set of Prologrules.andP;  The compiler presents the programmer with a global address spaceprogramming model.andP;  A complete prototype system is presently able to compilestandard C code employing global arrays of data into code suitable forexecution on a generic distributed memory parallel computer.andP;  Allpartitioning and mapping is performed automatically, and communicationprimitives are generated.andM;SYSTEM ARCHITECTUREandM;Until now, there has been little or no need to include expert knowledge onthe achitecture of the target machine(s).andP;  In general, optimizing compilersare built following the methodology described in [8] and generate code in twosteps.andP;  A divide and conquer control strategy is used to partition thecompilation into small program fragments, and code is generated for eachfragment independently by pattern matching [3].andP;  Some inefficiencies are thenremoved by peephole optimizations.andP;  These compilers generate good code forserial machines, implicitly using information on the architecture of themachine.andP;  The problem arises now because high performance machines areinherently parallel and often contain more than one processor or computingelement.andM;Applying the serial methodology to these new high performance architecturesresults in inefficient code generation and/or undue hardships for theprogrammer.andP;  To develop efficient software, programmers must optimizeprograms by partitioning the data, setting up the communication, andoptimizing the code for the architecture of the computing element.andP;  Forexample, Rymarczyk [5h gives guidelines for hand coding pipelined computersat the machine level.andM;Our goal is to automate these steps.andP;  Much of this information can becodified in partition directives for problem decomposition and rule basesthat describe the processing element architecture and interconnectiontopology.andP;  The code to run on these computing elements is generated with theadvice given by these directives and the knowledge of the architecture of aprocessing element and the topology of the interconnection network.andM;All compilers use processor-specific information during code generation andoptimization.andP;  However, the organization of the compiler differs from othercompilers because the architectural information is explicitly contained in amachine-specific rule base.andP;  The separation of architectural information fromthe compiler yields many advantages, such as simplifying compiler retargetingand supporting code generation by more sophisticated probelm reductioncontrol strategies.andM;Retargetable compilers, i.e., compilers that can generate code for severalprocessors, also use architectural information during code generation.andP;  Ingeneral, retargetable compilers contain more explicit information;differences between target processors must be indicated to the compiler.andO;However, retargetable compilers such as PQCC [2] or the Amsterdam CompilerKit [6] also contain implicit information, targeting machines with similararchitectures or architectures within a processor family.andM;Our design philosophy differs from these retargetable compiler designs inthat all machine-specific information is explicitly defined in a rule base.andO;This organization has many advantages:andM;1.andP;  Code generation for any processor.andP;  It is possible to generate code forany machine by simply defining a new rule based for the target processor.andO;Furthermore, retargeting requires less time and effort, since no compilersource code has to be modified.andM;2.andP;  Transportability.andP;  Code is easily transported between processors withdefined rule bases.andP;  Recompiling the source code with a different back endwill yield assembly code for a different processor.andM;3.andP;  Modifications in processor design easily updated.andP;  If the manufacturermakes modifications in the processor design, only the rules affected by themodifications need to be modified.andP;  No compiler source code has to bemodified.andM;4.andP;  Upward compatibility easily described.andP;  Upward compatible models of aprocessor family can be described simply by augmenting the existing rulebase.andP;  The rule bases for the TMS320 family of digital signal processors weredeveloped in this manner.andM;5.andP;  Differences in architectures evident.andP;  Differences in architectures arereadily discernible by examining the rule bases.andM;The rule base is consulted by the two code generators of the compiler.andP;  Thefirst code generator cpartitions data and generates code for control flow andthe second generates code for the computation.andP;  The computation codegenerator consists of eight phases: atomic assignment evaluation, automicassignment generator, data-flow analysis, expression evaluator, peepholeoptimizations on intermediate code, translation of the intermediate code intomachine-specific mnemonics, machine-specific peephole optimizations, and thepost-processor.andM;The function of the atomic assignment evaluator is to preserve high-levelparallelism at the register level.andP;  An atomic assignment evaluation isperformed on each concurrent assignment to determine if it is atomic, e.e., ahigh-level language assignment that can be translated into a single low-levelassembly instruction on the target processor.andP;  Atomic assignments can containhigh-level information on register-level parallelism that could not beotherwise described to the compiler.andP;  The atomic assignment evaluatorconsults the rule base, comparing each concurrent assignment against a listof the processor's atomic instructions.andM;If a concurrent assignment is not atomic, the rule base is consulted todetermine if it is a concurrent assignment exhibiting register-levelparallelism (CAP).andP;  A CAP is similar to an atomic assignment, except it istranslated into a sequence of low-level instructions.andP;  As in the case of theatomic assignment, the CAP contains high-level information on register-levelparallel operations.andM;A distinction is made between an atomic assignment and a CAP since it ispossible for a concurrent assignment to be either type of instruction.andP;  Ifthis is the case, the atomic assignment representation is preferred over theCAP representation of the concurrent assignment since a more efficient codesequence will be generated.andM;The rule base is consulted by comparing each concurrent assignment of thesource code to a list of atomic assignments and CAPs.andP;  Concurrent assignmentsthat do not appear in the list are passed on to the automatic assignmentgenerator, while atomic assignments and CAPs are preserved.andM;The purpose of the automatic assignment generator to break the concurrentassignment into a set of equipvalent atomic assignments, using heuristicsdescribed by Mills [4].andP;  These heuristics were originally used as designrules for generating sequential Pascal statements.andM;The output of the automatic assignment generator is three-address code [1],which is atomic for most processors.andP;  Thre-address code specifies that anarithmetic operation be performed on one or two operands, and stored in adestination register.andP;  Thus, assignments with complex expressions are brokendown into a set of assignments with simple expressions.andM;The data-flow analysis phase performs global optimization on thethree-address code.andP;  The rule base is consulted to determine if there arefeatures on the target processor that allow optimization.andP;  For example, therule base should be consulted to determine if the processor is pipelined.andP;  Ifit is pipelined, the data-flow analysis phase will perform pipelinereorganization on the code, using information from the rule base such as thenumber of pipeline stages.andM;The expression evaluator receives the code from the data-flow analysis phaseand generates assembly code with generalized mnemonics.andP;  The expressionevaluator references the rule base to determine whether the target processoris a 0-, 1-, 2-, or 3-address machine.andP;  The rule base must also be referencedto determine the available addressing modes on the target processor.andM;After peephole optimizations are made on the generalized assembly code, thegeneralized mnemonics are translated into machine-specific mnemonics.andP;  Therule base is consulted to determine how to translate the mnemonics.andM;The machine-specific peephole optimizer performs optimizations on themachine-specific assembly code.andP;  It must reference the rule base to determinewhat optimizations can be perfomed.andP;  For example, if the processor ispipelined, the rule base will supply information such as number of stages inthe pipeline, and instructions that can take advantage of pipelining.andO;Optimizations can then be performed, such as replacing branch instituionswith delayed branches.andM;The rule base is also referenced to determine what type ofreduction-in-strength strategies are efficient for the architecture.andP;  Forexample, the operation [X.sup.2] may be replaced by the operation x X x if itis a less expensive operation on the architecture.andP;  Another example might bethe reduction of the expression 4 X x into the instruction x SHR 2 (shift xright 2 positions) if it is a less expensive operation to implement on thearchitecture.andM;PROGRAMMING STRATEGYandM;As a concrete example of a partitioning strategy, we consider the iterativesolution of partial differential equations (PDEs).andP;  These algorithmstypically involve updating the current value of the solution at each gridpoint by replacing it with a simple linear combination of the values ofnearby points.andP;  This updating procedure is repeatedly applied to all of thepoints in the grid until the change in the values drops below a predeterminedthreshold.andP;  The set of neighboring points whose values are used for theupdating procedure is called a stencil: different algorithms employ differentstencils.andP;  Using these algorithms, many grid points may be updatedsimultaneously.andP;  The only constraint is that, for each point being updated,none of the other points in its stencil can be updated at the same time.andM;The best mapping of this problem onto a distributed memory architecturedepends on the number of grid points [N.sug.g] and the number of processors[N.sub.p.]  If we assume that typical problem sizes range upward fromhundreds of grid points per side, with thousands of points being more nearlyideal, then a generous lower bound for [N.sub.g] will be [10.sup.4] fortwo-dimensional problems and [10.sup.6] for three-dimensional ones.andP;  Mostdistributed memory computers have [N.sub.p] ranging up to [10.sup.4]; the largest values are still less than [10.sup.5].andP;  We make the reasonableassumption that [N.sub.g] [is greater than] [N.sub.p].andP;  Under thisassumption, the most natural mapping of the problem into the architecture canbe constructed as follows:andM;1.andP;  Embed a plane into the multiprocessor communication network.andM;2.andP;  Tile the domain of the PDE problem with [N.sub.p] identical polygons.andM;3.andP;  Assign one tile to each processor, with neighboring tiles assigned toneighboring processors under the embedding in step 1.andM;With this mapping, each processor will be solving a smaller, equivalent PDEboundary value problem, with the boundary conditions at each stage determinedby the solutions obtained thus far on the neighboring tiles.andP;  Theinterprocessor communication load is determined by the need to keep theboundary conditions current on each tile.andM;The best tiling is algorithm dependent: for different stencils, differenttiles will minimize the communication.andP;  It has been shown [7] that a simplegeometrical construction may be used to generate the optimal tile for a givenstencil: identify the points that are the farthest from the center andconnect them with line segments (Figure 1).andP;  The resulting shape will be aconvex polygon, with a specific orientation relative to the stencil and tothe coordinate axes.andP;  Such a polygon is called the convex hull of thestencil.andP;  If the polygon can be used to tile the plane, while preserving therequired orientation, it will be the optimal tile for the stencil.andM;For example, diamond tiling is optimal for the five-and nine-point crossstencils.andP;  It consists of squares oriented at 45 [degrees] with respect tothe coordinate axes.andP;  It is useful to paremeterize the tile by the length ofits diagonal k, which must be an even divisor of n.andP;  The number of pointsenclosed is [k.sup.2]/2.andP;  For the five-point cross, each of the points in theperimeter must be communicated to the adjoining tiles, as indicated in Figure2.andP;  The values of the remote points that must be exchanged to update all ofthe points in the tile are also indicated in Figure 2.andP;  The points at the topand the bottom of the tile adjoin three neighbors; the remaining points justone.andP;  Thus the total number of transfers is given by T = perimeter + 4 = 2k +2.andM;The size of the appropriate tile, as parameterized by k, is determined by thenumber of available processors and the number of grid points.andP;  Once the tilesize has been computed, an origin for a global coordinate system is chosenand the coordinates of each tile are computed (see Figure 3).andP;  Next, theprogram code loop is unrolled for values of the left-hand side of theassignment lying in a single tile.andP;  The array references on the right-handside may involve both local and nonlocal variables.andP;  The indices for thelocal variables are converted into relative coordinates with respect to theglobal coordinate system.andP;  The nonlocal variables references are convertedinto interprocessor communication instructions.andP;  The remote tile involved isidentified; its global coordinates are used to label the communicationchannel to be used.andP;  The array indices are converted into local coordinateswith respect to the remote tile, giving the local address of the variablebeing communicated.andM;ITERATIVE PARTIAL DIFFERENTIAL EQUATIONandM;SOLVERandM;The following program is an example employing the partitioning strategy givenin the previous section for solving PDEs on a distributed memoryarchitecture.andP;  We have removed the loop controlling the convergence criterionto better illustrate the automatic data partitioning and generation ofcommunication primitives.andP;  The program is #define NX 12 #define NY 12 #defineNP8 #pragma processors NP double phi[NY][NY]; #pragma tile phi[i][j] ((0 1)(1 0) (0-1) (-1 0)) int i, j; main[] { forall(iandgt;= 0; i [is less than]NX; i++:j :=0; j [is less than] NY; j++: (i+j) % ==0) phi[i][j]:= (phi[i][j+1]+phi[i+1][j] + phi[i][j-1] + phi[i-1][j])/4 forall(i := 0; i [is less than]NX; i++ : j :=0; j [is less than] NY; j++ :(i+j)% 2!= 0)phi[i][i]:=[phi[i][j+1] + phi[i+1][j] + phi[i][j-1] + phi[i-1][j])/4 }andM;The #pragma directive is the proposed ANSI preprocessor directive specifyingcompiler-dependent instructions.andP;  The tile command is a list of the pointsused in the update rule in terms of coordinates relative to each point.andP;  Theprocessor directive defines the number of processors in the array.andP;  With thecode given, the number of processors can be specified on the complier commandline, allowing the program to be compiled for an arbitrary number ofprocessors without editing the source.andM;The forall statements are unordered loops.andP;  The construction consists of twopieces: a set of control statements enclosed in parentheses and a bodyconsisting of a statement.andP;  As in a conventional loop, the foral is anoperator that applies the function defined by its body on the domainspecified by the control statements.andP;  The forall operator does not specifythe order in which the domain is enumerated.andP;  Rather, it asserts that thefunction applications are independent and can be performed in any serialorder or even simultaneously.andP;  In each application of the function, thestatements must be executed in the order specified.andM;The control arguments of the forall are set membership specifications andthese are separated by colons.andP;  Two types of specifications can be made.andP;  thefirst is a set membership function for a single index and takes the same formas a for statement, specifying a lower bound, an upper bound, and a rule forupdating the index.andP;  The second is a characteristic function of an arbitrarysubset of the Cartesian product of the indices.andP;  The domain of the loop isthe intersection of the sets of indices specified by each of the setmembership functions.andM;After scanning and parsing, the partitions and resulting communicationprimitives are generated by the control flow code generator.andP;  The controlflow code generator has five phases of execution.andP;  In the first phase, thearray points are partitioned among the processors.andP;  In phase two, the domainof each forall statement is computed.andP;  In phase three, the body of eachforall is unrolled.andP;  In phase four, the concurrent assignments andpartitioned into intraprocessor concurrent assignments.andP;  In the final phase,each global address is translated into a processor number and local address,and the necessary interprocessor communication instructions are generated.andM;We have used the syntax from Occam for the interprocessor communicationprimitives: port_a_b? array [x y] for read port_a_b ! array [x y] for writewhere _a_b are the global grid coordinates of a processor, and [xy] are thelocal array indices of the word to be transferred.andM;In Figure 4, we show the intermediate assembly language code generated by thepartitioning and control flow code generator.andP;  The assembly language code isthen translated by the computation code generator which uses the rule basedescribing the architecture of the individual processors to generate machineinstructions for the target machine.andP;  As described earlier, standard codeoptimizations are performed at this time.andP;  In particular, redundantcommunication instructions will be detected and removed with commonsubexpression combination techniques.andM;FAST FOURIER TRANSFORMandM;As a second example demonstrating the flexibility of the approach, we presentthe source and resulting intermediate code for a fast Fourier transform(FFT).andP;  The algorithm consists of two nested loops: an outer loop over theiterations of the transform and an inner loop in which the components ofintermediate transforms are computed.andP;  The outer loop is necessarily serial.andO;However, the inner loop can be directly converted into a forall loop.andO;Furthermore, the two assignments in the body of the inner loop areindependent, and may be executed as a single concurrent assignment.andP;  Theresulting parallel code is: #define N 16 #define 1l(N)4 #define NP4 #pragmaprocessors NP float phi_r[N], phi_i[N], w_r[N/2], w_i[N/2]; #pragma tilephi_r[j] (j % NP); #pragma tile phi_i[j] (j % NP); float a, b; int i, j, k;main() { a, b : = N/2, 1; for(i:=0; i [is less than] 1g(N); i++) { a, b :=a/2, b*2; forall(j :=0; j [is less than a; j++ : k := 0; k [is less than] N;k += N/b) phi_r[j+k], phi+_i[j+k], phi_r[j+k+a], phi_i[j+k+a]:= phi_r[j+k] +phi_r[j+k+a], phi_i[j+k] + phi_i[j+k+a], (phi_r[j+k] - w_r[j*b] *phi_r[j+k+a] - phi_i[j+k] + w_i[j*b] * phi_i[j+k+a]), (phi_r[j+k] - w_i[j}b]* phi_r[j+k+a] + phi_i[j+k] - w_r[j*b] * phi_i[j+k+a]); } }andM;Two partitioning strategies may be employed, depending on whether thecommunication occurs at the end of the calculation or at the beginning.andP;  Let[N.sup.p] denote the number of processing elements.andP;  In the first case, thecalculation is organized so that no communication is required until the final[log.sub.2][(N.sub.p)] iterations.andP;  To accomplish this, those elements of phiwhose indices are equivalent modul [N.sub.p] must be assigned in the samepartition.andP;  The tile directive specifies this particular rule.andP;  In the secondcase, the communication occurs over the first [log.sub.2][(N.sub.p)]iterations, and phi is partitioned into contiguous pieces of length[N/N.sub.p].andM;In Figure 5, we show the output for the case [N.sub.p] = 4 and N = 16.andP;  Asexpected, the communication occurs only in the final two iterations.andP;  As inthe preceding example, no optimizations have been performed.andP;  Conventionalcommon subexpression elimination will remove the redundant read instructions.andO;Note that the processors communicate pairwise in each iteration.andP;  Thecommunications can be grouped into a single concurrent assignment for eachiteration, possibly eliminating communication overhead latencies.andP;  Such anoptimization is straightforward to implement once the target computer hasbeen specified.andM;TARGET ARCHITECTURESandM;The intermediate language code produced by the compiler is at the level ofconventional high-level programming languages for message passing computers:all data has been assigned to individual memories; it is exchanged onlythrough explicit communication instructions; and the communication primitivessupport a completely connected virtual machine.andP;  The read and writeprimitives can be directly mapped onto the operating system calls provided onsuch computers as the Ncube/10 and the iPSC.andM;The resulting code may not be very efficient, however.andP;  These computers havefairly large communication latencies, and exchanging short messages will beslow.andP;  In general, the average message length will have to be increased bycombining shorter ones.andP;  Some combining can be carried out automatically byperforming conventional live/dead analysis on the variables in thecommunication instructions and moving the instructions into concurrentassignments.andP;  Further optimization will require global analysis, and cannotbe performed automatically at this time.andP;  However, the intermediate code doesprovide a useful starting point for further optimizations.andM;The systems presently generates machine code for several uniprocessor CPUswith multiple functional units, including the Weitek WTL 3364, the TexasInstruments TMS320 family, the NEC 7720, and the ATandamp;T DSP32.andP;  Wheneverpossible, concurrent assignments are executed in parallel.andP;  It is also thecompiler for the Coherent Parallel Computer presently under construction.andO;This system is a massively parallel, synchronous MIMD computer.andP;  The startupoverhead for an interprocessor message is only 2 cycles, and the throughputof each channel is one word per cycle.andP;  For this system, communication codewill require only peephole optimization to be efficient.andM;In the Coheren Parallel Computer, the computation units are distinct from thememory and communication system and can be customized for particularapplications.andP;  We are presently considering three implementations of themachine.andP;  The first employs commercial floating point chips and will besuitable for probelms involving linear algebra, iterative and time-evolutionPDE solvers, probabilistic relaxation labeling, low-level image processingapplications such as edge, boundary, and region detection algorithms andspectral algorithms in computational fluid dynamics.andP;  Dedicated computers forother applications can be constructed by combining the communication andmemory system with custom computational units.andP;  A programmable computer forDNA matching and sequencing can be implemented with fast string patternmatchers.andP;  And a massively parallel lattice gas simulation machine can beimplemented by using a programmable finite automation for the computationalunit.andM;CONCLUSIONandM;To make effective use of a parallel computer, the user is required to breakthe program into subtasks, devise a mapping of the subtasks onto processingelements, and finally add interprocessor communication instructions asrequired.andP;  None of these tasks are trivial, and the efficiency of theresulting code depends on decisions made at each stage.andP;  At present, it ispossible only to solve these problems by trial and error.andM;In this article, we have presented a rapid prototyping and retargetablecompiler system which addresses these programming tasks.andP;  By employingpartitioning directives, the system automatically decomposes the problem intoan appropriate set of subtasks and generates any necessary intertaskcommunication instructions.andP;  With this system, the user is presented with asingle global address space.andP;  As examples, we have shown the compilation fromC into an intermediate assembly language for a point-iterative ellipticpartial differential equation solver and for a fast Fourier transform.andM;Code generation is an optimization process.andP;  For von Neumann architectures, adivide and conquer approach is adequate, since the individual operationswhich comprise a computation are essentially independent.andP;  The inefficienciesthat result from generating a single machine instruction at a time canlargely be corrected with subsequent peephoe optimizations.andP;  For otherarchitectures however, especially those with a multitude of independentfunctional units, as in parallel or pipelined computers, the instructions aremuch more interdependent, and generating good code requires consideration ofsignificantly different execution orders.andP;  In this situation, moresophisticated optimization techniques are demanded.andP;  We have presented acompiler which uses a combination of both problem reduction and heuristicsearch to generate code.andP;  The problem reduction strategy consists ofidentifying maximal concurrent assignments rather than individual operations.andO;Heuristic search is then used to find the locally optimal code for eachconcurrent assignment and to combine the partial solutions to produceglobally (nearly) optimal code.andM;Partitioning information should be incorporated into type declarations.andP;  Ingeneral, type information is the specification of the representation of datfor a particular operation.andP;  Viewed from this perspective, the partitioningof data is a particular form of typing.andP;  When implemented as typedeclarations, partitioning rules can be extended to encompwass dynamicallyallocated variables.andP;  In addtion, complex communication operations can bespecified as a typecasting, without requiring the introduction of auxiliaryvariables or temporary storage.andP;  Furthermore, by means of a simplegeneralization of the principle of reduction in strength, the techniquesdescribed here can be employed to solve the problems of dereferencing anarbitrary pointer and performing pointer arithmetic.andP;  We are presentlyextending the system to permit the specification of partitioning informationas a generalization of a type declaration and to handle arbitrary pointeroperations.andM;REFERENCESandM;[1.] Aho, A.V., SEthi, R., and Ullman, J.D.andP;  Compilers: Principles,techniques, and tools.andP;  Addison-Wesley, Reading, MA, 1986.andM;[2.] Cattell, R.G.G., Newcomer, J.M., and Leverett, B.W.andP;  Code generation ina machine-independent compiler.andP;  ACM SIGPLAN Notices 14, 8 (Aug.andP;  1979),65-74.andM;[3.] Glanville, R.S., and Graham, S.L.andP;  A new method for compiler codegeneration.andP;  In 5th Annual ACM Symposium on Principles of ProgrammingLanguages (Tucson, Ariz., Jan. 23-25, 1978), pp.andP;  231-240.andM;[4.] Mills, H., et al.andP;  Principles of Computer Programming: A MathematicalApproach.andP;  Allyn andamp; Bacon, Newton, Mass., 1987.andM;[5.] Rymarczyk, J.W.andP;  Coding guidelines for pipelined processors.andP;  InProceedings of the Symposium on Architectural Support for ProgrammingLanguage and Operating Systems (Palo Alto, Calif., Ma.andP;  1-3, 1982), pp.andO;12-19.andM;[6.] Tanenbaum, A.S., et al.andP;  A practical tool kit for making portablecompilers.andP;  Commun.andP;  ACM 26, 9 (Sept.andP;  1983), 654-660.andM;[7.] Terrano, A.E.andP;  On the grain-size dependence of interprocessorcommunication demand.andP;  Submitted to IEEE Trans.andP;  Comput.andM;[8.]  Wulf.andP;  W., et al.andP;  The Design of an Optimizing Compiler.andP;  AmericanElsevier, New York, 1975.andM;ABOUT THE AUTHORS:andM;ANTHONY E. TERRANO holds a Ph.D.andP;  in theoretical physics from Caltech.andP;  As anassistant professor at Columbia University, he was one of the principaldesigners for the QCD Machine, a parallel supercomputer built to addressnumerical problems in theoretical physics.andP;  His research interests includeparallel computer architecture, application-specific computer design,parallel languages and algorithm and compiler design.andP;  Author's PresentAddress: Department of Electrical and Computer Engineering, DrexelUniversity, Philadelphia, PA 19104.andP;  Terrano@occlusal.rutgers.edu.andM;STANLEY M. DUNN is an assistant professor of biomedical engineering and amember of the Laboratory for Computer Science Research at Rutgers University.andO;He is also a research associate professor at the University of Medicine andDentistry of New Jersey.andP;  His research interests are in computer vision,image understanding, and parallel computing environments for vision and imageunderstanding systems.andP;  Author's Present Address: Department of BiomedicalEngineering, Rutger's University, P.O. Box 909, Piscataway, NJ 08855-0909.andO;smd@occlusal.rutgers.edu.andM;JOSEP E. PETERS is a Ph.D.andP;  candidate and research assistant in the ComputerScience Department at Rutgers University, where he earned both his B.S.andP;  andM.S.andP;  degrees in electrical engineering.andP;  His research interests includecompilers and parallel processing.andP;  Author's Present Address: Department ofComputer Science, Hill Center, Busch Campus, Rutgers University, Piscataway,NJ 08855.andP;  peters@occlusal.rutgers.edu.andO;</TEXT></DOC>