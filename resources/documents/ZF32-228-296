<?xml version='1.0' encoding='UTF-8'?>
<DOC><DOCNO> ZF32-228-296 </DOCNO><DOCID>10 371 104</DOCID><JOURNAL>Communications of the ACM  Feb 1991 v34 n2 p34(12)* Full Text COPYRIGHT Association for Computing Machinery 1991.andM;</JOURNAL><TITLE>Software safety in embedded computer systems. (Cover Story)</TITLE><AUTHOR>Leveson, Nancy G.andM;</AUTHOR><SUMMARY>System-safety engineering in software development involvesidentifying hazardous software behavior, thus reducing risks ofhazardous situations.andP;  Software safety refers to situations whererunning software operates within a system without causing risk.andO;In the absence of software reliability measurement techniques thatdemonstrate high software reliability and safety, certain softwareengineering techniques are necessary.andP;  These techniques includeidentifying hazards through analysis procedures, using hardwareand software interlocks to eliminate and control those hazards,using safety analysis during software development, and theevaluation of analysis and development procedures.andP;  This finaltechnique is used to judge the level of confidence that analysisand development provide for system-safety engineering.andM;</SUMMARY><DESCRIPT>Topic:     SafetySoftwareEmbedded SystemsSystem designRisk managementSoftware EngineeringReliability.andO;Feature:   illustrationchart.andO;Caption:   Top levels of patient monitoring system fault tree. (chart)Standard verification. (chart)Safety verification. (chart)andM;</DESCRIPT><NOTE>Only Text is presented here; see printed issues for graphics.andO;</NOTE><TEXT>In recent years, advances in computer technology have gone hand-in-hand withthe introduction of computers into new application areas.andP;  The problem ofsafety has gained importance as these applications have increasingly includedcomputer control of systems where the consequences of failure may involvedanger to human life, property, and the environment.andM;An accident or mishap is traditionally defined by engineers as an unplannedevent or series of events that leads to an unacceptable loss such as death,injury, illness, damage to or loss of equipment or property, or environmentalharm.andP;  Accidents usually involve unwanted and unexpected releases of energyor dangerous substances.andP;  By this definition, computers are relatively safedevices: They rarely explode, catch on fire, or cause physical harm.andO;However, computers can contribute substantially to accidents when theyoperate as a subsystem within a potentially dangerous system.andP;  andlsqb;1andrsqb; Examplesinclude computers that monitor and control nuclear power plants, aircraft andother means of transportation, medical devices, manufacturing processes, andaerospace and defense systems.andP;  Because computers are not unsafe whenconsidered in isolation, and any indirectly contribute to accidents, anysolution to computer or software safety problems must stem from and beevaluated and applied within the context of system safety.andM;System-safety engineers define safety in terms of hazards and risk.andP;  A hazardis a set of conditions (i.e., a state) that can lead to an accident givencertain environmental conditions.andP;  The following are examples of hazards:guard gates not lowering when trains approach traffic crossings, pressureincreasing above some threshold in a boiler, or brakes failing in a motorvehicle.andP;  Because most accidents are caused by multiple factors, safety isdefined in terms of hazards instead of accidents in order to focus on thefactors that are within the design space of the system being built.andM;For example, an air traffic control system attempts to provide minimumseparation between aircraft.andP;  If the minimum separation standards areviolated, a hazardous state exists and an accident is possible, though notinevitable.andP;  The consequences may depend upon pilot alertness and skill,visibility, mechanical failures in the aircraft, luck, etc.--factors notunder the control of the engineer designing the air traffic control system.andO;The best that can be done is to minimize the probability of the hazardousstates (i.e., to attempt to keep the aircraft separated by a safe distance).andM;If safety is defined in terms of accidents or catastrophic failures of thesystem or its components (instead of hazards), very few systems could beconsidered unsafe.andP;  Because computers are not inherently unsafe and softwarecannot directly cause accidents, defining software safety in terms ofaccidents or catastrophic software failures instead of hazards will eliminateall software from safety considerations.andP;  There is no potential failure ofthe software in an air traffic control system, for example that by itself cancause accidents; the ATC software can only contribute to hazards.andP;  However,because software can contribute to system hazards, eliminating or controllinghazardous software behavior will reduce or prevent accidents resulting fromthe hazard and in turn reduce risk.andM;Risk is defined by engineers as a function of 1) the likelihood of a hazardoccurring, 2) the likelihood that the hazard will lead to an.andP;  accident, and3) the worst possible potential loss associated with such an accident.andP;  Riskcan be reduced by decreasing any or all three of these risk factors.andM;System-safety engineering, a subdiscipline of system engineering, attempts toensure acceptable risk by applying scientific, management, and engineeringapproaches to reducing these risk factors in the system as a whole.andP;  Softwarecan potentially contribute to risk (or to reducing risk) through the effectof the software on system hazards.andP;  Therefore, software safety can be definedin terms of approaches to ensuring that software executes within the systemcontext without resulting in unacceptable risk.andP;  Some system-safety engineersprefer the use of the term &quot;software-system safety&quot; over &quot;software safety&quot; inorder to emphasize the system nature of the problem and  that solutions must,of necessity, involve an integrated, consistent, system-wide approach.andP;  Theterm software safety is used here because of its wider usage, but a systemapproach to the problem is emphasized.andM;This article proposes an approach and a research direction for softwaresafety that extends and adapts the methods used to control risk in the largersystem within which the software is embedded.andP;  The approach [12, 14] combinesstandard  software-engineering techniques with proven system-safetyengineering techniques and special software-safety techniques.andM;Some of the techniques described here are being used on real industrialsoftware projects; others have not yet progressed beyond the stage ofresearch papers.andP;  Anyone currently relying on any software-engineeringtechnique to achieve acceptable levels of safety should recognize that notechniques have yet been proven to provide high confidence.andP;  This does notnecessarily mean that safety-critical computer software cannot or should notbe built, only that the risk involved should be realistically assessed andevaluated to determine whether it is acceptable.andM;The next section provides an introduction to system-safety engineering andoutlines the connection between it and software safety.andP;  The remainder ofthis article delineates an approach to verifying software safety.andM;System Safety and Software SafetyandM;Since software safety is a part of system safety, it makes sense to start byexamining how safety is engineered into  physical  systems.andP;  System-safetyengineering involves: 1) identifying hazards, 2) assessing these hazards asto criticality and likelihood (i.e., assessing risk), and 3) designingdevices to eliminate or control hazards.andP;  Once the system is designed andbuilt, a final risk assessment is performed, based on the hazards that couldnot be completely eliminated, to determine if the system has acceptable risk.andM;Hazard IdentificationandM;In the earliest life cycle (concept formation) phases of any potentiallysafety-critical project, an initial risk assessment is made to identifysafety-critical areas and functions, to identify and evaluate hazards, and toidentify the safety design criteria to be used in the remainder of thedevelopment.andP;  This process results in a preliminary system hazard list, whichmay be updated as the project continues.andP;  An audit trail is set up for eachof the hazards, and the procedures used to eliminate or control themthroughout the project development are tracked and evaluated.andM;As the development continues, other types of hazard analysis may be used.andO;For example, subsystem hazard analyses (SSHA) may be performed as soon as thesubsystems are designed in sufficient detail.andP;  Its purpose is to identifyhazards associated with the design of the individual subsystems.andP;  Theanalysis considers how both the operation and the failure of an individualcomponent can affect the safety of the system.andP;  When a subsystem includes acomputer, a software hazard analysis may be used to identify the behavior ofthe software that could affect system safety.andM;A system hazard analysis (SHA) studies the hazards associated with theinterfaces between subsystems or with the system operating as a whole,including potential human errors.andP;  It differs from subsystem hazard analysisin that it examines how system operation or failure can affect the safety ofthe system and its subsystems while subsystem hazard analysis considers onlyhow individual component operation or failure affects the system.andP;  Both typesof analysis are iterative; they must continue as the design is updated andchanges are made.andP;  Various techniques are used to perform these analysesincluding design reviews and walkthroughs, checklists, fault tree analysis,event tree analysis, and hazard and operability studies (HAZOP).andM;Fault tree analysis involves construction of a logic diagram showing credibleevent sequences that could lead to a specified hazard.andP;  A partial fault treefor a computerized patient monitoring system is shown in Figure 1.andP;  Specificcomputer behavior that could be hazardous, i.e., the &quot;software hazards,&quot;  [2]are included in the fault tree.andM;While a fault tree traces an undesired event back to its causes, an eventtree traces a primary event forward in order to define its consequences.andP;  Thetwo trees are often used together in what is called a Cause-ConsequenceDiagram.andM;HAZOP was developed for chemical manufacturing processes.andP;  it is aqualitative procedure that involves a systematic search for hazards bygenerating questions that consider the effects of deviations in parameters.andM;Determining all the causes of a hazard is not necessary in order to deal withit.andP;  For example, there are many possible ways a ship's hull can becompromised, such as icebergs, electrolysis, explosions, and corrosion ofthrough-hull fittings.andP;  Not knowing all the causes does not prevent takingprecautions to deal with the hazard along with trying to eliminate as manycauses as possible: the ship may be designed to withstand a certain number ofruptures (regardless of how they are caused), and life boats and emergencyprocedures may be included in case that does not work.andP;  The Titanic was builtto stay afloat if 4 or fewer of the 16 underwater compartments were flooded.andO;Since previously there had never been an incident in which more than fourcompartments of a ship were damaged, this assumption was reasonable.andO;Unfortunately, the iceberg ruptured five spaces.andP;  It can be argued that theassumptions were the best possible given the state of knowledge at the time.andO;The mistake was not in attempting to make a safer ship, but in believing thatit was unsinkable and not taking normal precautions (e.g., having adequatelife boats and emergency procedures), in case it was not.andM;In practice, identifying hazards is relatively easy.andP;  Problems are morelikely to arise in risk assessment i.e., assessing the severity andlikelihood of a particular hazard or assessing the total risk involved in thesystem.andM;Hazard AssessmentandM;Early in the process of preliminary hazard analysis, an attempt is made toassess the severity and likelihood of the identified hazards.andP;  Hazards can beviewed as falling along a continuum in terms of severity.andP;  One approach toallocating resources and effort is to establish a cutoff point on thiscontinuum: Only the hazards above this point are considered in furthersystem-safety procedures.andP;  A more common approach is to use several cutoffpoints to establish categories of hazards (e.g., negligible, marginal,serious, critical) to which differing levels of time and effort are applied.andM;The second part of hazard assessment involves assessing likelihood.andP;  Thereare various ways to determine likelihood; for example, probabilities can beassigned to each event in the fault tree and the overall probability for thehazard calculated.andP;  This type of assessment is very difficult to carry outearly in the project because sufficient design information is usually notavailable.andP;  For the most part, qualitative assessments are sufficient toprovide the information needed for resource allocation during development.andM;Hazard ControlandM;Designing in safety involves using various engineering design techniques toeliminate or, if this is not possible, to control hazards.andP;  If feasible,hazards are eliminated entirely from the design.andP;  A system is intrinsicallysafe if it is incapable of generating or releasing sufficient energy orcausing harmful exposures under normal or abnormal conditions given theequipment and personnel in their most vulnerable condition [20].andM;If it is not possible or practical to completely eliminate a hazard, then thehazard must be minimized with respect to its likelihood and severity.andP;  Thebest way to achieve this is to include control devices that attempt toprevent or minimize the occurrence of the hazard.andP;  This can be accomplishedby automatic control (e.g., automatic pressure relief valves, speedgovernors, or limit-level sensing controls), lockouts, lockins, andinterlocks.andM;A lockout device prevents a hazardous event from occurring, while a lockindevice maintains safe conditions.andP;  An interlock is used to ensure that asequence of operations occurs in the correct order.andP;  Interlocks may ensurethat an event A does not occur:andM;1.andP;  inadvertently (e.g., a preliminary, intentional event B such as pushing aparticular button is required before A can occur);andM;2.andP;  while condition C exists (e.g., an access door is placed over highvoltage equipment so that when the door is opened, the circuit is broken); orandM;3.andP;  before event D (e.g., the tank will fill only if the vent valve has beenopened first).andM;If hazardous events cannot be prevented or their severity minimized throughthe design, then the next best strategy is to attempt to detect and respondto them if they occur.andP;  This  is  accomplished through fail-safe designs thatinvolve detection of hazards along with damage control, containment,isolation, and warning devices.andM;Software can be involved in controlling hazards in two ways.andP;  First,interlocks and safety controls to protect against general system hazards maybe constructed from digital computer components.andP;  An example is the tripcomputer in a nuclear power plant that initiates procedures to shutdown theplant when operating conditions are hazardous.andP;  Second, hazard controls maybe used in the design of the software itself [12] to protect against varioussoftware hazards.andP;  For example, executable assertions may be used to checkfor particular software states that could lead to hazardous outputs.andM;System Risk AssessmentandM;Decisions about the use of safety-critical systems often rest on a finalassessment of the risk involved.andP;  These numbers can be derived from (1)historical information about the reliability of the individual components andthe models that define the connection between these components or (2)historical accident data about similar systems.andM;The first of these procedures works because historical reliability figuresare available for standard parts that have been used for decades.andP;  Designerrors are usually not considered, and the failure probabilities are basedsolely on random wear-out data.andP;  The second assessment approach (i.e., theuse of historical accident data), is feasible for physical systems becausethey tend to change very little in basic design over long periods of time.andM;Neither of these basic assumptions is true for software, however.andP;  Softwareis usually specially constructed each time it is used.andP;  Even when software isreused, the interfaces between software components are extremely complex incomparison to hardware.andP;  This complexity makes it difficult to build a modelof component interaction that accurately combines the individual componentfailure probabilities into an integrated software failure probability.andO;Although such models can be constructed, their predictions are not yetreliable.andP;  For newly constructed software  components, measuring reliabilityduring testing and development remains a research topic.andP;  Although it may orwill soon be possible to derive these numbers for failure probabilities inthe mid-ranges, the very low failure probabilities required insafety-critical systems require more experience with the software than couldpossibly be obtained in a realistic amount of time [22].andM;One problem with all of these probabilistic models, both for hardware and forsoftware, is that they are usually based on easily violated assumptions aboutthe operation of the system and its components.andP;  One of the mostcomprehensive probabilistic risk analyses that has been attempted is anuclear reactor safety study called Wash-1400 andlsqb;17, 25], which attempted todemonstrate that nuclear power plants have acceptable risk.andP;  This study hasbeen criticized [19) for using elementary data that was incomplete oruncertain, and for making many unrealistic assumptions.andP;  For example,independence of failures was assumed while common mode failures were largelyignored.andP;  andlsqb;3andrsqb; Additionally, the Wash-1400 study assumed that nuclear powerplants are built to plan and are properly operated.andP;  Recent events suggestthat this may not be the case.andP;  Critics also maintain that the uncertaintiesare very large, and therefore the calculated risk numbers are not veryaccurate.andP;  Almost all such probabilistic risk assessments involve equallyunrealistic assumptions.andM;Software reliability assessment models also make many assumptions that may beunrealistic; for example, it is assumed that the software specification(against which outputs are checked during testing) is correct, that it ispossible to predict the usage environment of the software and to provide arealistic operational profile against which to execute the code and assessthe reliability, and that it is possible to anticipate and specify correctlythe appropriate behavior of the software under all possible circumstances.andO;These assumptions often do not hold for control systems, resulting inaccidents due to software errors [12, 23].andP;  As an example of such anaccident, an aircraft went out of control and crashed when a mechanicalmalfunction in a fly-by-wire aircraft set up an accelerated environment forwhich the computer was not programmed.andP;  In another, a flight-control systemwas not programmed to handle a particular attitude of the aircraft because itwas assumed that the aircraft could not attain that attitude.andP;  In yetanother, an aircraft was damaged when the computer raised the landing gear inresponse to a test pilot's command while the aircraft was standing on therunway.andP;  In each of these cases, the assumed operational profile for whichthe software had been tested was in error or the specification was in error,rendering any potential software reliability assessment inaccurate.andM;To complicate things further, the introduction of the computer itself maycause a change in the environment (which invalidates the testing profile andthus the reliability assessment) such as a change in the behavior of humanoperators stemming from complacency or inattention.andP;  Ternham [28] warns aboutchanges in pilot behavior and complacency leading to increased numbers ofsafety-critical incidents in highly computerized commercial aircraft.andO;Accidents have also occurred when operators, after having interacted with thecomputer for long periods of time, started taking shortcuts or changing theinput conditions (e.g., speed) from those assumed during testing.andM;This is not to say that probabilistic risk assessment is unimportant oruseless.andP;  But it does mean that more emphasis needs to be put on other typesof safety-enhancement procedures such as the design and evaluation of devicesand procedures to eliminate and control hazards.andP;  Other nonprobabilisticapproaches to assurance, such as formal and informal verification, may bemore appropriate given the importance of design errors in software comparedto the major emphasis put on random wear-out failures in physical systems.andM;This is true, by the way, for both hardware and software systems.andO;System-safety engineers have sometimes concentrated more on getting theproper numbers out of their models than on designing hazard control andmanagement procedures.andP;  Furthermore, with the highly complex hardware systemsnow being built, design errors are assuming more importance and cannot beignored in risk assessments.andM;The remainder of this article considers  nonprobabilistic  approaches toassurance of software safety.andP;  The following section presents basicdefinitions and outlines the goals of software safety analysis in greaterdetail.andP;  The final two sections outline some approaches to accomplishingthese goals.andM;Basic DefinitionsandM;A system is a set of components that together achieve some common purpose orobjective with a given environment.andP;  A system theoretically can be describedby a function relating inputs, outputs, and time.andP;  Each subcomponent can bethought of as a system in itself with its own functional description.andP;  Forthe most part, the systems of interest here are physical systems orprocesses.andM;A process may be self-regulating (i.e., the variables and conditions withinthe process naturally maintain the values necessary to achieve the objectiveunder normal conditions).andP;  More often, some type of control is needed.andP;  Thepurpose of a control subsystem is to order events and regulate the value ofthe variables in the system in order to optimize the achievement of thesystem goals.andP;  Software-safety issues arise when computers are embeddedwithin larger, potentially dangerous systems in order to provide partial orcomplete control.andM;The control function has two parts: 1) a description of the function to beachieved by the system and 2) a specification of the constraints on operatingconditions [17, 27].andP;  Constraints may arise from several sources includingquality considerations, equipment capacities (e.g., avoiding overload ofequipment in order to reduce maintenance), process characteristics (e.g.,andO;limiting process variables to minimize production of byproducts), and safety.andM;The safety constraints are derived through the system safety process asdefined earlier.andP;  Basically, they are the hazards rewritten as constraints(i.e., in positive rather than in negative terms).andP;  To take a simple example,excessive pressure in a boiler may be hazardous; the corresponding safetyconstraint may require that pressure remain below some threshold level.andP;  Theconstraints may have safety margins built into them in order to provide forvarious types of error and for time lags.andM;It is often possible to integrate the constraints and the basic functionalrequirements to obtain an optimized control function.andP;  The problem thenreduces to a basic problem in optimization for which mathematical solutionmethods exist [27].andP;  For the safety constraints, however, the integration isnot straightforward.andP;  Often the resolution of conflicts between safetyconstraints and desired functionality involves moral, ethical, legal,financial, and societal decisions; this is usually not a purely technical,optimization decision.andP;  Management needs to be involved because of potentialliability.andP;  Government is often involved in this decision in order to resolvepolitical and social issues.andM;Because of their importance both from a liability and a regulatorystandpoint, safety constraints are usually maintained separately and auditedboth internally and externally throughout the design, construction, andlifetime of the control system.andP;  So although safety constraints may beintegrated into the basic control function, there will often be a need toprovide at least some separate safety verification or analysis procedures.andM;Safety constraints relate to computer software in several ways.andP;  A computermay perform three basic functions in a process-control system:andM;1.andP;  data logging,andM;2.andP;  implementation of the basic control function including both directdigital control (e.g., replacement of the usual analog control devices) andhigher-level supervisory control (e.g., decisions about set points or optimalvalues of controlled variables and decisions about event sequencing), andandM;3.andP;  maintenance of safe conditions (i.e., acting as a safety interlock).andM;In the first two of these functions, the computer, at least partially, hasnon-safety-related roles.andP;  In the third, the computer is used as part of thesystem-safety control function and its only goal is to maintain safe stateswithin the plant.andM;In any particular system, computer subcomponents of the system may functionin any or all of these roles.andP;  For example, in a nuclear power plant acomputer may be used to gather the large amounts of data needed by themanagement and regulatory authorities to monitor the operation of the plant.andO;Computers may also be used to implement the basic control functions withinthe plant that result in the production of power.andP;  Finally, they may be usedas components of the safety shutdown (or trip) subsystem that moves the plantinto a safe state when hazardous conditions are detected.andM;Logging data for offline analysis has no direct safety implications althoughit may have indirect implications.andP;  The detection of the hole in the ozonelayer at the South Pole was delayed several years because the depletion wasso severe that the NASA computer analyzing the data had been suppressing it,having been programmed to assume that deviations so extreme must be errorsandlsqb;24andrsqb;.andM;The most interesting computer control functions, from a safety viewpoint, arethe second and third, i.e., when the computer acts as a controller and whenit acts as a safety interlock.andP;  These functions are treated separately in thefollowing sections.andM;Safety Analysis of control-system softwareandM;The objective of software-safety analysis for software implementing basiccontrol and supervisory functions is to ensure that the functionalityspecified and implemented is consistent with the safety constraints.andO;Ideally, this could be accomplished by verifying that 1) the softwarerequirements satisfy the safety constraints and 2) the software correctlyimplements the requirements.andP;  There are practical problems, however, inestablishing with a very high level of assurance that the software correctlyimplements the requirements.andM;Safety-critical systems often require probabilities of accidents in the rangeof 10[.sup.-8andrsqb; to 10[.sup.-12andrsqb; over a given period of time.andP;  For example, theFederal Aviation Authority rules require that any failure condition thatcould be catastrophic must be &quot;extremely improbable&quot; where extremelyimprobable is defined as 10[.sup.-9andrsqb; per hour or per flight, as appropriate,or to quote, &quot;is not expected to occur within the total life span of thewhole fleet of the model&quot; [29].andP;  Although software reliability measurementtechniques cannot measure such low levels of failure, few experts would claimthat current software-engineering techniques can guarantee this degree ofperfection in software.andM;There are some who have suggested that fault-tolerance techniques,specifically N-version programming, will provide ultra-high softwarereliability [1].andP;  Although experiments do provide some evidence that thistechnique can increase reliability (as can other software-engineeringtechniques), the resulting failure rates have been nowhere near thoserequired in safety-critical systems.andP;  In fact, they have been orders ofmagnitude lower.andP;  The most realistic formal model of N-version programmingandlsqb;5andrsqb; and some experiments [4, 10] confirm these limits on potentialreliability increase.andP;  There is no reason to believe that this techniqueprovides the level of perfection and confidence required when catastrophicconsequences can result from software failure.andM;Static analysis, including formal verification, is theoretically capable ofachieving the high confidence required.andP;  Unfortunately, formal verificationof all aspects of a specification, including timing and performance, iscurrently infeasible for all but the simplest systems.andP;  Even if this type ofverification were possible, it is not at all clear that in practice (ascontrasted to theoretically) it alone would be adequate to ensure enoughconfidence when potential hazards have extremely serious consequences.andP;  Anysuch complete formal verification of all software requirements in realsystems would be very difficult and entail a large amount of mathematicalanalysis, proofs, and documentation.andP;  High confidence that no errors wereinvolved in the process would be difficult to achieve.andP;  And the cost of sucha verification would be enormous.andP;  These problems will most likely be solvedover time, but safety-critical systems are being built right now.andM;The question is what to do until software engineering techniques areperfected.andP;  If the limitations of formal verification are examined, itappears that one factor contributing to the difficulty is that many thingsare being verified--many of which are not involved in the safety constraints.andO;Similarly, most software fault elimination and tolerance techniques attemptto eliminate or tolerate all possible faults, errors, and failures.andP;  Greaterassurance may be provided if: 1) the goals and process are more limited,i.e., a partial verification is performed to demonstrate only that thesoftware implementation is consistent with the safety constraints, but notnecessarily with the rest of the specified functionality; and 2) techniquesare used to control hazards during software execution rather than to providetotal fault tolerance.andP;  Instead of focusing on correctness, this alternativestresses safety analysis and hazard control.andM;Safety analysis may use the same type of procedures involved in a completeverification, but merely apply them to limited properties and aspects of thesoftware.andP;  It is a basic assumption of this approach that this will be lesscostly and easier to review (and thus to have confidence in) than a completeformal verification.andP;  But that does not necessarily mean that the approach isfeasible or practical.andP;  The only existing evidence of feasibility andpracticality is that the procedures are being applied, at least usingsemiformal techniques, on real software projects at reasonable costs i.e., asmall fraction of the cost of the total verification efforts that went on inparallel with the safety verification.andP;  Examples of this will be presented byW.C.andP;  Bowman, G.H.andP;  Archinoff, V.M.andP;  Raina, D.R.andP;  Tremaine, and N.G.andP;  Levesonat the Conference on Probabilistic Safety Assessment and Management (PSAM),Los Angeles, April 1991.andM;Will this type of safety analysis suffice to ensure adequate safety incomputer-controlled systems? The answer to this question depends on theformality and power of the particular analysis techniques applied, the amountof run-time protection provided to guard against errors in the software andin the analysis, and the level of assurance that is required (which in turndepends upon the level of acceptable risk).andP;  Currently, the best that can bedone is to use layers of protection&quot; so that errors in the analysis or in thesoftware that could lead to hazards are detected or handled in another way.andO;The safety verification and analysis should be backed up by usingsoftware-safety design techniques that protect against hazardous states thatmight result from undetected software faults, including those stemming fromflaws in the software requirements specification.andP;  And both of these needprotection that is external to the software.andP;  External  protection  isdescribed in more detail in the next section.andM;As with standard verification procedures, safety analysis activities shouldspan the software development process (see Figure 3).andP;  This has severaladvantages: 1) errors are caught earlier and thus are easier and less costlyto fix; 2) information from the early verification activities can be used todesign safety features into the code and to provide leverage for the finalcode verification effort; and 3) the verification effort  is  distributedthroughout the development process instead of being concentrated at the end.andO;Ideally, each step merely requires showing that newly added detail does notviolate the safety verification of the higher-level abstraction at theprevious step, i.e., that each level is consistent with the safety-relatedassumptions made in the analysis at the next-higher level.andP;  Theseverification activities may have both formal and informal aspects: staticanalysis using formal proofs  and structured  walk-throughs and dynamicanalysis involving various types of testing to provide confidence in themodels and the assumptions used in the static analysis.andM;Hierarchical verification is not a new idea; the only unique questions arewhat can be verified at each stage with respect to safety and how is theinformation from the verification used at the next development step.andM;Requirements Analysis.andP;  The first step in any safety verification procedureis to verify that the software requirements are consistent with or satisfythe safety constraints.andP;  This analysis is important not only to ensure thatthe code that satisfies these requirements will be safe, but also to identifyimportant conflicts and tradeoffs early before design decisions have beenmade.andP;  Decisions about tradeoffs between safety and reliability or othersoftware qualities, and between safety and functionality, must be made foreach project on the basis of potential risk, acceptable risk, liabilityissues, government requirements, etc.andP;  The technical staff needs to provideinformation on the tradeoffs and potential costs of eliminating or minimizinghazards so that management  and regulatory agency decisions can be made.andM;To accomplish this analysis, the requirements must be specified in a formallanguage, where a formal language is defined as one having a rigorously andunambiguously defined semantics.andP;  Without this formality, any suchverification would be suspect because of the potential ambiguities andimprecision within the specification.andP;  There are now many formal requirementsspecification languages; for many of them it would not be difficult to deriveprocedures to analyze consistency between safety constraints and basicfunctionality.andP;  Timing and failures need to be included in the modeling andanalysis.andM;Jahanian and Mok [8] have shown how to perform this analysis for timingconstraints, using a formal logic called Real Time Logic (RTL) and a model ofthe events and actions in the system.andP;  Leveson and Stolzy [16] have outlinedsimilar types of consistency analysis techniques using Time Petri-net models.andO;The latter approach differs from that of Jahanian and Mok not only in termsof the model and specification language used, but also in terms of scope:control behavior along with timing is included in the analysis; safetyconstraints can be identified from the combined software and environmentmodel, as well as shown consistent with software requirements; and controlfaults and failures are incorporated into the analysis to determine theireffects on the system and to aid in designing protection devices.andP;  Melhartandlsqb;21andrsqb; has extended this work using an extended form of statecharts to providefurther analysis of the effects of failure.andP;  Some additional safety analysiscriteria for software requirements specifications have been outlined byJaffee et al.andP;  andlsqb;7andrsqb;.andM;High-Level Design Analysis.andP;  At the high-level design stage, informationabout the software hazards and safety constraints can be used to identifysafety-critical items (processes, data, and states).andP;  The identificationprocess might involve backward flow analysis from hazardous outputs or othertypes of analysis procedures [13].andP;  Cha [2] has shown how safety invariantsfor each safety-critical module can be derived from the safety constraints.andO;This information is important in the later verification steps and also in thedesign of assertions or other execution-time protection mechanisms.andM;Once the critical items have been identified, they can be subjected tospecial treatment in the design.andP;  For example, safety will be enhanced andlater safety analysis simplified if the safety-critical code, variables, andstates are minimized, isolated and protected.andP;  Isolation may, for example, beuseful in satisfying certain types of safety constraints that involveenforcing separation such as ensuring that a safety-critical function is notinadvertently activated.andP;  Safety-critical data also needs to be protectedfrom accidental alteration.andP;  Security techniques might be used to accomplishthese goals.andP;  Rushby has shown how an idea from security, i.e., theencapsulation kernel, can be used to enforce certain types of safetyconstraints such as the isolation of critical modules [26].andM;The safety analysis at this level results in a set of design constraints orassumptions that imply the overall software-safety constraints.andP;  Thelow-level design must be shown to preserve these high-level designconstraints.andP;  The analysis is also used to tailor the high-level design inorder to reduce the necessary safety verification in the later stages ofdevelopment.andM;Low-Level Design Analysis.andP;  The amount and type of safety analysis that canbe accomplished at this stage of development depends on how much and whatkind of information is included in the low-level design specification.andP;  Notonly must it be shown that the specified behavior of the individual modulespreserves the individual module safety invariants (which were derived in thehigh-level design analysis), but it must also be demonstrated that themodules executing together preserve the high-level design constraints if thishas not already been accomplished in a previous step [2].andP;  In addition, itmay be possible to demonstrate that safety-critical variables and modules areadequately protected from errors in other parts of the software, at least atthis level of abstraction.andP;  If a formal design language has been used, thenformal analysis is possible.andP;  Again, information from this analysis can beused to design protection against hazards into the software.andM;Code Analysis.andP;  Finally, after the coding has been completed, formal andinformal verification is needed to ensure that the actual code is consistentwith the assumptions made in the low-level design analysis (e.g., that thecode preserves the module safety invariants, that the protection devices havebeen implemented correctly, and that the safety-critical functions have beenproperly isolated).andP;  In general, the goal at each of the analysislevels--high-level design, low-level design, and code--is to move theassurance of safety to the highest level of abstraction possible and then toshow that the assumptions of this analysis are preserved throughout each ofthe levels of mapping down to the code.andP;  The code-level analysis willprobably involve a combination of techniques, including testing, formalproofs, and informal verification techniques such as Software Fault TreeAnalysis [3, 15].andM;Configuration Control and Maintenance.andP;  Whenever any changes occur to thesoftware, either because of faults found during the verification proceduresor during the operational use of the software or because of functionalenhancements, a safety analysis is needed to ensure that the changes aresafe.andP;  This analysis starts at the highest level involved in the change: Itmay be necessary to start from the requirements analysis if the changeinvolves a constraint or the basic software requirements; in other cases, itmay be necessary only to redo aspects of the design and/or code analysis.andO;Some types of changes may not be allowed due to their potential decrease inthe safety of the software or because they are deemed not worth the effort ofrecertification of safety.andM;Planning for such changes can help to minimize the reanalysis that isnecessary.andP;  For example, one of the reasons to isolate safety-criticalfunctions and data is to minimize the  reanalysis  resulting  from changes inboth safety-critical and nonsafety-critical modules.andM;Ensuring Safety In interlock SoftwareandM;It may seem that if the software itself is functioning as a safety interlockwithin the larger system, the only alternative for verification of softwaresafety is to verify the complete functionality of the software (i.e., all ofthe software requirements are safety-critical since the mission of thesoftware is to maintain safety in the system within which the computer isembedded).andP;  Fortunately, this is not usually the case.andP;  Even in these typesof systems, careful design of the software and surrounding hardware may beable to limit the safety analysis process.andM;Given the current state of the art of software engineering, it is foolhardyto design process-control systems that rely solely on the proper functioningof software to maintain safety.andP;  Unfortunately, systems of this sort arebeing built.andP;  For example, the software controlling a radiation therapydevice (i.e., a linear accelerator called the Therac 25) has been involved inseveral accidents [9].andP;  These accidents might have been prevented if thestandard hardware interlocks used in the  previous  noncomputerized modelshad not been left off when the computer was introduced.andP;  After several deathsdue to software errors, a hardware interlock was finally installed on thedevice.andM;The alternative to depending solely on software correctness is to includehardware backups and interlocks to protect against software errors.andP;  Thesebackups are also applicable to the types of control systems describedearlier, but become more critical, from a safety viewpoint, when the computeracts as a safety interlock.andP;  It should be noted that using a computer to backup a computer does not provide the required level of protection againstsoftware errors.andP;  Laprie [11] has argued that software &quot;design diversity&quot;(the use of multiple software versions or N-version programming) is analogousto the use of backups in hardware such as a mechanical or pneumatic devicebacking up an analog electronic channel.andP;  However, the effectiveness ofdiversity in hardware backups depends on the devices having very differentfailure modes, i.e., being sensitive to different physical phenomena, so thatcommon-mode failures are avoided.andP;  Software backing up software does notprotect against common-mode failures (i.e., design errors) any more than ananalog channel backing up another analog channel with the same or only aslightly different design does.andP;  In the real systems that have tried usingN-version programming with which the author is familiar, the very restrictedtiming and functionality requirements have resulted in diverse software thatin reality is extremely similar in design and coding.andP;  While there areprocedures  for  performing common-mode failure analysis in hardware  devices(to  protect against latent hardware design errors triggered by common inputsor events), there is no way to ensure that software versions will fail ondifferent inputs.andM;The use of hardware protection against software errors does not eliminate theneed for software-safety verification; the protective devices themselves mayfail and thus should not be relied on solely.andP;  But hardware protection isimportant in order to augment the software-safety analysis and protectivedevices, and it provides greater confidence that risk is acceptable.andP;  Thereare basically two types of protection: passive and active.andM;Passive protection techniques involve a design in which the system fails intoa safe state.andP;  An example is the use of gravity or some physical property toensure that a vent valve opens when pressure is excessive (as opposed torequiring measurement, comparison to a safe value, and mechanical operationof the vent).andP;  Passive protective devices are very reliable, but not alwayspossible to build.andP;  When passive protection is feasible, it often limitsdesign freedom, and the necessary tradeoffs may not be acceptable.andM;Active protection devices usually involve detection and recovery from unsafestates.andP;  The problem with this approach is that errors are often difficult todetect and may be detected only after the process is in a state from whichrecovery may be unsure.andP;  It is obviously much easier to build physicaldevices that detect the lack of a software output than those that can detectan incorrect software output.andP;  In a few cases, there are hard limits on thevalues of software outputs, and devices can be built to detect when thoselimits are exceeded.andP;  The Therac 25 is an example of this--there is a limitto how much radiation is safe in certain of its operating modes.andP;  In othersituations, complete active protection with hardware is impractical.andP;  In anuclear reactor shutdown system, for example, where the computer merely putsout a signal that the plant is within safe limits or it is not within safelimits (and needs to be shutdown), any independent evaluation of thecorrectness or safety of this output would involve as much processing as thatwhich the computer had to do; duplication of this  processing would probablybe impractical in hardware (after all, that is why the computer wasintroduced in the first place).andM;Therefore, hardware protection devices will be needed but cannot solve  theentire  problem-verification of the software is required.andP;  Furthermore,because most of the requirements are safety-critical, not much is gained byjust substituting safety verification.andP;  There are ways, however, of designingthe software and hardware so that some special types of software-safetyanalysis and design techniques can be used to augment the standardverification procedures in order to provide added assurance.andM;For example, in a nuclear power plant shutdown system, it is possible to usea watchdog timer to initiate shutdown of the reactor if the software does notproduce an output within a certain amount of time.andP;  Furthermore, the softwarecan be designed to start each cycle in a tripped state (i.e., with allvariables set to trip the reactor) and during the execution of the code, thevariables are set to untripped only if the measured inputs are within safelimits.andP;  This software design protects against errors that result in nooutput within the response time limit or that result in omitting some of therequired checking of inputs (e.g., the execution somehow jumps over thechecking of certain parameters).andP;  Since the software starts in a trippedstate, verification of safety then only requires ensuring that the softwarelogic does not untrip the parameters given dangerous conditions in the plant.andM;It is important to emphasize that performing this type of safety verificationdoes not negate the need to verify the correctness of the software in someformal or informal way.andP;  It merely provides additional confidence because ituses different procedures and because it will probably be simpler and thusless error-prone than the complete verification procedures.andM;It also has the advantage of providing guidance in the use of fault-tolerancefacilities.andP;  Because the system in this design is protected against thesoftware not providing an output, error detection becomes paramount whereasrecovery is less important (and incorrect recovery efforts may actuallyincrease the danger).andP;  The software-safety verification can provide importantinformation about internal software states that could lead to hazardousoutputs and thus need to be detected through assertions, exception-handlingconditions, or other such error-detection mechanisms.andM;Software Fault Tree Analysis (SFTA) [3, 15] is an example of asafety-verification technique that can also be used to identify criticalrun-time checks.andP;  SFTA starts from unsafe outputs and traces back through thesoftware to determine if those outputs can be produced.andP;  It is closelyrelated to axiomatic program verification methods.andP;  Recently, the author wasinvolved in the application of SFTA to the software of a nuclear power planttrip computer with a design similar to that described above.andP;  During theprocess of generating the fault trees, various internal conditions weredetected that could lead to the software untripping the trip variables whenthey should stay tripped.andP;  These internal conditions, such as a particularvariable at some point having a negative value, could be shown to beunreachable according to the logic of the code.andP;  However, the checks for suchconditions were so simple (e.g., a change to the type definition or the useof a simple IF statement) that it was decided to include them in the code incase 1) the SFTA process had been flawed and the formal verificationprocedure applied to the complete functionality of the software was alsoflawed, 2) computer hardware failures or environmental factors such as atomicparticle bombardment cause these memory locations to be negative despite thecorrectness of the software, or 3) the hazardous internal state is caused byother unexpected factors that never seem to be identified until after theyoccur and sometimes never are.andP;  Note that the possible causes of thehazardous internal state do not need to be identified in order to identifythe states themselves and to build in protection.andM;ConclusionsandM;There are very good reasons for using computers in process control.andO;Attempting to dissuade people from this course because we do not yet haveperfect software engineering techniques would be hopeless.andP;  This article haspresented some ideas about what can and should be done if the decision ismade to use computers in safety-critical systems.andM;It is not suggested that the procedures described here be used in lieu ofgood software engineering practice.andP;  For example, although softwarereliability measurement techniques are currently unable to provide adequateconfidence that ultra-high reliability and safety have been achieved, theycan demonstrate that it has not been achieved and can provide importantinformation during software development and use.andP;  The same is true for allcost-effective techniques that can potentially increase the reliability andcorrectness of the process-control software without, in the process, creatingthe potential for more errors.andM;However, because of our current lack of practical and reliable measurementtechniques for such software, we cannot provide confidence of software safetysolely using probabilistic risk assessment models.' Furthermore, standardsoftware engineering techniques, including software fault tolerance andverification of correctness, cannot currently provide the high degree ofconfidence required.andP;  Our best alternative is to augment good softwareengineering practice with 1) analysis procedures to identify hazards, 2)elimination and control of these hazards through various types of hardwareand software interlocks and other protective devices using several layers ofprotection, 3) application of various types of safety analysis techniquesduring the software development to provide confidence in the safety of thesoftware and to aid in the design of hazard protection, and 4) evaluation ofthe effectiveness of the analysis and design procedures to assess the levelof confidence they merit.andM;Whether this is adequate depends upon the acceptable level of risk and howeffective the software safety measures and external protection againstsoftware errors are judged.andP;  There are few systems that are completely freeof risk.andP;  What is required for a system to be usable is that it haveacceptable risk.andP;  The level of risk that is acceptable will vary with thetype of system and the potential losses that are possible.andP;  Acceptable riskfor a military fighter aircraft or for an experimental aircraft such as theX-29 where potential loss primarily involves property will be much higherthan acceptable risk for a commercial aircraft where public safety isinvolved.andP;  In systems that currently are controlled or protected  by noncomputerized means, the decision about the introduction of computers mayinvolve a judgment as to whether the resulting risk is increased and how muchconfidence can be placed on this judgment.andM;The effectiveness and scope of applicability of the approach presented inthis article still needs to be determined.andP;  It seems obvious that it will notapply to systems that cannot tolerate any type of failure, where it is notpossible to design fail-safe procedures such as mechanical or human backup,and where the software must function perfectly to be safe.andP;  When it is notpossible to design systems to be fail-safe or somehow to protect againstsoftware-related hazards, the builders and users of these systems mustconsider whether the risk of using computers to control safety-criticalfunctions is justified.andM;FootnotesandM;1 Safety issues may also arise when software is used to design controlsystems, but these issues are outside the scope of this article.andM;2 Although theoretically there is no such thing as a software hazard-softwareitself is not dangerous and can only contribute to system hazards--forsimplicity this article refers to &quot;software-related system hazards&quot; as justsoftware hazards.andM;3 Failures are considered to be independent when no common cause can beattributed to them, (i.e., they fail in a statistically independent manner).andO;Failures are called common mode if they can be traced back to a common eventthat triggers similar or different design errors in the failing components.andM;4 This is also in the nuclear industry as defense in depth.andM;5  Whether this is also true for hardware (i.e., reliance on probabilistichardware risk assessment models is unwise) is a difficult issue and adifferent article.andM;ReferencesandM;1.andP;  Avizienis, A. and Kelly, J.P.J.andP;  Fault tolerance by design diversity:Concepts and experiments.andP;  IEEE Comput.andP;  17, 8 (Aug.andP;  1984), 67-80.andM;2.andP;  Cha, S.S.andP;  Safety verification on software design.andP;  Ph.D.andP;  dissertation,ICS Dept., University of California, Irvine, june 1990.andM;3.andP;  Cha, S.S., Leveson, N.G., and Shimeall, T.J.andP;  Verification of safety inAda programs.andP;  In Proceedings of the 10th International Conference onSoftware Engineering (Singapore, Apr. 1988), pp.andP;  377-386.andM;4.andP;  Eckhardt, D.E., Caglayan, A.K., Knight, J.C., Lee, L.D., McAllister,D.F., and Vouk, M.A.andP;  An Experimental evaluation of software redundancy as astrategy for improving reliability.andP;  Submitted  for publication.andM;5.andP;  Eckhardt, D.E.andP;  and Lee, L.D.andP;  A theoretical basis for the analysis ofmultiversion software subject to coincident errors.andP;  IEEE Trans.andP;  Softw.andO;Eng.andP;  SE-11, 12 (Dec.andP;  1985), 1511-1517.andM;6.andP;  Friedman, M. Modeling the penalty costs of software failure.andP;  Ph.D.andO;dissertation, Dept.andP;  of Information and Computer Science, University ofCalifornia, Irvine, Mar. 1986.andM;7.andP;  Jaffe, M.S., Leveson, N.G., Heimdahl, M., and Melhart, B. Softwarerequirements analysis for real-time process-control systems.andP;  IEEE Tran.andO;Softw.andP;  Eng.andP;  (Mar.andP;  1991) To be published.andM;8.andP;  Jahanian, F. and Mok, A.K.andP;  Safety analysis of timing properties inreal-time systems.andP;  IEEE Trans.andP;  Softw.andP;  Eng.andP;  SE-12, 9 (Sept.andP;  1986),890-904.andM;9.andP;  Joyce, E. Software bugs: A matter of life and liability.andP;  Datamation 33,10 (May 15, 1987), 88-92.andM;10.andP;  Knight, J.C.andP;  and Leveson, N.G.andP;  An experimental evaluation of theassumption of independence in Multiversion programming.andP;  IEEE Trans.andP;  Softw.andO;Eng.andP;  SE-12, 1 (Jan.andP;  1986), 96-109.andM;11.andP;  Lapric, J.C.andP;  The dependability approach to critical computing systems.andO;In Proceedings of the First European Conference  on  Software Engineering(Strasbourg, France, Sept. 1987).andM;12.andP;  Leveson, N.G.andP;  Software safety: Why, what, and how.andP;  ACM Comput.andP;  Surv.andO;18, 2 (june 1986), 25-69.andM;13.andP;  Leveson, N.G.andP;  Building safe software.andP;  In Aerospace SoftwareEngineering, Chris Anderson, Ed.andP;  AIAA, 1990.andM;14.andP;  Leveson, N.G.andP;  Software Safety.andP;  Addison-Wesley Reading, Mass., To beavailable fall 1990.andM;15.andP;  Leveson, N.G.andP;  and Harvey, P.R.andP;  Analyzing software safety.andP;  IEEE Trans.andO;Softw.andP;  Eng.andP;  SE-9 (Sept.andP;  1983), 569-579.andM;16.andP;  Leveson, N.G.andP;  and Stolzy, J.L.andP;  Safety analysis using petri nets.andP;  IEEETrans.andP;  Softw.andP;  Eng.andP;  SE-13 (Mar.andP;  1987), 386-397.andM;17.andP;  Levine, S. Probabilistic risk assessment: Identifying the real risks ofnuclear power.andP;  Tech.andP;  Rev.andP;  (Feb./ Mar. 1984), 41-44.andM;18.andP;  Lowe, E.I., and Hidden, A.E.andP;  Computer Control in Process Industries,Peter Peregrinus Ltd., London, 1971.andM;19.andP;  MacKenzie, J.J.andP;  Finessing the risks of nuclear power.andP;  Tech.andP;  Rev.andO;(Feb./ Mar. 1984), 34-39.andM;20.andP;  Malasky, S.W.andP;  System Safety Technology and Application, Garland STPMPress, N.Y., 1982.andM;21.andP;  Melhart, B. An interface model for software requirements.andP;  Ph.D.andO;dissertation, ICS Dept., University of California, Irvine, June 1990.andM;22.andP;  Miller, D.R.andP;  The role of statistical modeling and inference in softwarequality assurance.andP;  In Proceedings of the CSR Workshop on SoftwareCertification Gatwick, England, Sept. 1988).andM;23.andP;  Neumann, P.G.andP;  Some computer-related disasters and other egregioushorrors.andP;  ACM Softzv.andP;  Eng.andP;  Not.andP;  10, 1 (jan.andP;  1985), 6-7.andM;24.andP;  New York Times.andP;  Science Section, July 29, 1986, p. Cl.andM;25.andP;  Reactor Safety Study: An assessment of accident risks in the U.S.andO;commercial nuclear power plants.andP;  Report WASH-1400, U.S.andP;  Atomic EnergyCommission, 1975.andM;26.andP;  Rushby, J. Kernels for safety? In Proceedings of the CSR Workshop onSafety and Security (Glasgow, Scotland, Oct. 1986).andP;  Also printed in Safe andSecure Computing Systems, T. Anderson Ed., Blackwell Scientific Publications,1989, pp.andP;  210-220.andM;27.andP;  Smith, C.L.andP;  Digital Computer Process Control.andP;  International TextbookCompany, Scranton, 1972.andM;28.andP;  Ternham, K.E.andP;  Automatic complacency.andP;  Flight Crew (Winter, 1981),34-35.andM;29.andP;  Waterman, H.E.andP;  FAA's certification position on advanced avionics.andP;  AIAAAstro.andP;  Aero.andP;  (May 1978), 49-51.andM;General Terms: Embedded Systems, Program Verification, Software SafetyandM;Additional Key Words and Phrases: Fault-tolerance, process control,reliabilityandM;NANCY G. LEVESON is associate professor for the Information and ComputerScience department at UC Irvine.andP;  Her research interests include softwaresafety and reliability, real-time systems, process-control software andembedded systems.andP;  Author's Present Address: Information and Computer ScienceDept., University of California, Irvine CA 92717.andP;  leveson@ics.uci.edu.andO;</TEXT></DOC>